<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Fit, Predict, CV · PosDefManifoldML</title><meta name="title" content="Fit, Predict, CV · PosDefManifoldML"/><meta property="og:title" content="Fit, Predict, CV · PosDefManifoldML"/><meta property="twitter:title" content="Fit, Predict, CV · PosDefManifoldML"/><meta name="description" content="Documentation for PosDefManifoldML."/><meta property="og:description" content="Documentation for PosDefManifoldML."/><meta property="twitter:description" content="Documentation for PosDefManifoldML."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="PosDefManifoldML logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">PosDefManifoldML</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">PosDefManifoldML Documentation</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../MainModule/">Main Module</a></li><li><a class="tocitem" href="../tools/">Tools</a></li><li><span class="tocitem">ML Models: PD Manifold</span><ul><li><a class="tocitem" href="../mdm/">Minimum Distance to Mean</a></li></ul></li><li><span class="tocitem">ML Models: PD Tangent Space</span><ul><li><a class="tocitem" href="../enlr/">Elastic-Net Logistic Regression</a></li><li><a class="tocitem" href="../svm/">Support-Vector Machine</a></li></ul></li><li class="is-active"><a class="tocitem" href>Fit, Predict, CV</a></li><li><span class="tocitem">Statistics</span><ul><li><a class="tocitem" href="../stats_descriptive/">Descriptive</a></li><li><a class="tocitem" href="../stats_inferential/">Inferential</a></li></ul></li><li><a class="tocitem" href="../conditioners/">Conditioners &amp; Pipelines</a></li><li><a class="tocitem" href="../contribute/">How to contribute</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Fit, Predict, CV</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Fit, Predict, CV</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="..." title="View the repository"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">Repository</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="cv.jl"><a class="docs-heading-anchor" href="#cv.jl">cv.jl</a><a id="cv.jl-1"></a><a class="docs-heading-anchor-permalink" href="#cv.jl" title="Permalink"></a></h1><p>This unit implements <strong>cross-validation</strong> procedures for estimating the <strong>accuracy</strong> and <strong>balanced accuracy</strong> of machine learning models. It also reports the documentation of the <strong>fit</strong> and <strong>predict</strong> functions, as they are common to all models.</p><p><strong>Content</strong></p><table><tr><th style="text-align: left">struct</th><th style="text-align: left">description</th></tr><tr><td style="text-align: left"><a href="#PosDefManifoldML.CVres"><code>CVres</code></a></td><td style="text-align: left">Encapsulate the results of cross-validation procedures for estimating accuracy</td></tr></table><table><tr><th style="text-align: left">function</th><th style="text-align: left">description</th></tr><tr><td style="text-align: left"><a href="#StatsAPI.fit"><code>fit</code></a></td><td style="text-align: left">Fit a machine learning model with training data</td></tr><tr><td style="text-align: left"><a href="#StatsAPI.predict"><code>predict</code></a></td><td style="text-align: left">Given a fitted model, preidct labels, probabilities or scoring functions on test data</td></tr><tr><td style="text-align: left"><a href="#PosDefManifoldML.crval"><code>crval</code></a></td><td style="text-align: left">Perform a cross-validation and store accuracies, error losses, confusion matrices, the results of a statistical test and other informations</td></tr><tr><td style="text-align: left"><a href="#PosDefManifoldML.cvSetup"><code>cvSetup</code></a></td><td style="text-align: left">Generate indexes for performing cross-validtions</td></tr></table><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PosDefManifoldML.CVres" href="#PosDefManifoldML.CVres"><code>PosDefManifoldML.CVres</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">struct CVres &lt;: CVresult
    cvType      :: String
    scoring     :: Union{String, Nothing}
    modelType   :: Union{String, Nothing}
    predLabels  :: Union{Vector{Vector{Vector{I}}}, Nothing} where I&lt;:Int
    losses      :: Union{Vector{BitVector}, Nothing}
    cnfs        :: Union{Vector{Matrix{I}}, Nothing} where I&lt;:Int
    avgCnf      :: Union{Matrix{T}, Nothing} where T&lt;:Real
    accs        :: Union{Vector{T}, Nothing} where T&lt;:Real
    avgAcc      :: Union{Real, Nothing}
    stdAcc      :: Union{Real, Nothing}
    z           :: Union{Real, Nothing}
    p           :: Union{Real, Nothing}
    ms          :: Union{Int64, Nothing}
end</code></pre><p>A call to <a href="#PosDefManifoldML.crval"><code>crval</code></a> results in an instance of this structure.</p><p><strong>Fields:</strong></p><p><code>.cvTpe</code> is the type of cross-validation technique, given as a string (<em>e.g.</em>, &quot;10-fold&quot;).</p><p><code>.scoring</code> is the type of accuracy that is computed, given as a string. This is controlled when calling <a href="#PosDefManifoldML.crval"><code>crval</code></a>. Currently <em>accuracy</em> and <em>balanced accuracy</em> are supported.</p><p><code>.modelType</code> is the type of the machine learning model used for performing the cross-validation, given as a string.</p><p><code>.nTrials</code> is the total number of trials entering the cross-validation</p><p><code>.matSize</code> is the size of the input matrices (trials)</p><p><code>.predLabels</code> is an <code>f</code>-vector of <code>z</code> integer vectors holding the vectors of  predicted labels. There is one vector for each fold (<code>f</code>) and each containes as many vector as classes (<code>z</code>), in turn each one containing the predicted labels for the trials.</p><p><code>.losses</code> is an <code>f</code>-vector holding BitVector types (vectors of booleans),  each holding the binary loss for a fold. </p><p><code>.cnfs</code> is an <code>f</code>-vector of matrices holding the <em>confusion matrices</em> obtained at each fold of the cross-validation. These matrices holds  <em>frequencies</em> (counts), that is, the sum of all elements equals the number of trials used for each fold.</p><p><code>.avgCnf</code> is the <em>average confusion matrix</em> of proportions  across the folds of the cross-validation. This matrix holds <em>proportions</em>, that is, the sum of all elements equal 1.0.</p><p><code>.accs</code> is an <code>f</code>-vector of real numbers holding the <em>accuracies</em> obtained at each fold of the cross-validation.</p><p><code>.avgAcc</code> is the <em>average accuracy</em> across the folds of the cross-validation.</p><p><code>.stdAcc</code> is the <em>standard deviation of the accuracy</em> across the folds of the cross-validation.</p><p><code>.z</code> is the test-statistic fot the hypothesis that the observed average error loss is  inferior to the specified expected value.</p><p><code>.p</code> is the p-value of the above hypothesis test.</p><p><code>.ms</code> is the execution time in milliseconds, excluding the time to compute the confusion matrix and p-value. If function <a href="#PosDefManifoldML.crval"><code>crval</code></a> has not been compiled yet, the time includes the compilation time. In any case the provided estimate is subjected to high variability. To get a better estimate use the median or minimum  across several runs or call the function using the <a href="https://github.com/JuliaCI/BenchmarkTools.jl">BenchmarkTools.jl</a> package.</p><p>See <a href="#PosDefManifoldML.crval"><code>crval</code></a> for more informations</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StatsAPI.fit" href="#StatsAPI.fit"><code>StatsAPI.fit</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">function fit(model :: MDMmodel,
              𝐏Tr   :: ℍVector,
              yTr   :: IntVector;
        pipeline :: Union{Pipeline, Nothing} = nothing,
        w        :: Vector = [],
        ✓w       :: Bool  = true,
        meanInit :: Union{ℍVector, Nothing} = nothing,
        tol      :: Real  = 1e-5,
        verbose  :: Bool  = true,
        ⏩       :: Bool  = true)</code></pre><p>Fit an <a href="../mdm/#PosDefManifoldML.MDM"><code>MDM</code></a> machine learning model, with training data <code>𝐏Tr</code>, of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">ℍVector</a>, and corresponding labels <code>yTr</code>, of type <a href="../MainModule/#IntVector">IntVector</a>. Return the fitted model.</p><div class="admonition is-warning" id="Class-Labels-d4793e8f7121eaa"><header class="admonition-header">Class Labels<a class="admonition-anchor" href="#Class-Labels-d4793e8f7121eaa" title="Permalink"></a></header><div class="admonition-body"><p>Labels must be provided using the natural numbers, <em>i.e.</em>, <code>1</code> for the first class, <code>2</code> for the second class, etc.</p></div></div><p>Fitting an MDM model involves only computing a mean (barycenter) of all the matrices in each class. Those class means are computed according to the metric specified by the <a href="../mdm/#PosDefManifoldML.MDM"><code>MDM</code></a> constructor.</p><p><strong>Optional keyword arguments:</strong> </p><p>If a <code>pipeline</code>, of type <a href="../conditioners/#PosDefManifoldML.Pipeline"><code>Pipeline</code></a> is provided,  all necessary parameters of the sequence of conditioners are fitted  and all input matrices <code>𝐏Tr</code> are transformed according to the specified pipeline  before fitting the ML model. The parameters are stored in the output ML model. Note that the fitted pipeline is automatically applied by any successive call  to function <a href="#StatsAPI.predict"><code>predict</code></a> to which the output ML model is passed as argument. Note that the input matrices <code>𝐏Tr</code> are transformed; pass a copy of <code>𝐏Tr</code> if you wish to mantain the original matrices.</p><p><code>w</code> is a vector of non-negative weights associated with the matrices in <code>𝐏Tr</code>. This weights are used to compute the mean for each class. See method (3) of the <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#Statistics.mean">mean</a> function for the meaning of the arguments <code>w</code>, <code>✓w</code> and <code>⏩</code>, to which they are passed. Keep in mind that here the weights should sum up to 1 separatedly for each class, which is what is ensured by this function if <code>✓w</code> is true.</p><p><code>tol</code> is the tolerance required for those algorithms that compute the mean iteratively (they are those adopting the Fisher, logdet0 or Wasserstein metric). It defaults to 1e-5. For details on this argument see the functions that are called for computing the means (from package <em>PosDefManifold.jl</em>):</p><ul><li>Fisher metric: <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#PosDefManifold.geometricMean">gmean</a></li><li>logdet0 metric: <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#PosDefManifold.logdet0Mean">ld0mean</a></li><li>Wasserstein metric: <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#PosDefManifold.wasMean">Wasmean</a>.</li></ul><p>For those algorithm an initialization can be provided with optional keyword argument <code>meanInit</code>. If provided, this must be a vector of <code>Hermitian</code> matrices of the <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%F0%9D%95%84Vector-type-1">ℍVector</a> type and must contain as many initializations as classes, in the natural order corresponding to the class labels (see above).</p><p>If <code>verbose</code> is true (default), information is printed in the REPL.</p><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature">notation &amp; nomenclature</a>, <a href="../MainModule/#the-ℍVector-type">the ℍVector type</a></p><p><strong>See also</strong>: <a href="#StatsAPI.predict"><code>predict</code></a>, <a href="#PosDefManifoldML.crval"><code>crval</code></a></p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using PosDefManifoldML, PosDefManifold

# Generate some data
PTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.25)

# Create and fit a model:
m = fit(MDM(Fisher), PTr, yTr)

# Create and fit a model using a pre-conditioning pipeline:
p = @→ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)
m = fit(MDM(Fisher), PTr, yTr; pipeline=p)</code></pre></div></section><section><div><pre><code class="language-julia hljs">function fit(model	:: ENLRmodel,
             𝐏Tr	 :: Union{HermitianVector, Matrix{Float64}},
             yTr	:: IntVector;

    # pipeline (data transformations)
    pipeline    :: Union{Pipeline, Nothing} = nothing,

    # parameters for projection onto the tangent space
    w           :: Union{Symbol, Tuple, Vector} = Float64[],
    meanISR     :: Union{Hermitian, Nothing, UniformScaling} = nothing,
    meanInit    :: Union{Hermitian, Nothing} = nothing,
    vecRange    :: UnitRange = 𝐏Tr isa ℍVector ? (1:size(𝐏Tr[1], 2)) : (1:size(𝐏Tr, 2)),
    normalize	:: Union{Function, Tuple, Nothing} = normalize!,

    # arguments for `GLMNet.glmnet` function
    alpha           :: Real = model.alpha,
    weights         :: Vector{Float64} = ones(Float64, length(yTr)),
    intercept       :: Bool = true,
    fitType         :: Symbol = :best,
    penalty_factor  :: Vector{Float64} = ones(Float64, _getDim(𝐏Tr, vecRange)),
    constraints     :: Matrix{Float64} = [x for x in (-Inf, Inf), y in 1:_getDim(𝐏Tr, vecRange)],
    offsets         :: Union{Vector{Float64}, Nothing} = nothing,
    dfmax           :: Int = _getDim(𝐏Tr, vecRange),
    pmax            :: Int = min(dfmax*2+20, _getDim(𝐏Tr, vecRange)),
    nlambda         :: Int = 100,
    lambda_min_ratio:: Real = (length(yTr)*2 &lt; _getDim(𝐏Tr, vecRange) ? 1e-2 : 1e-4),
    lambda          :: Vector{Float64} = Float64[],
    maxit           :: Int = 1000000,
    algorithm       :: Symbol = :newtonraphson,
    checkArgs       :: Bool = true,

    # selection method
    λSelMeth        :: Symbol = :sd1,

    # arguments for `GLMNet.glmnetcv` function
    nfolds          :: Int = min(10, div(size(yTr, 1), 3)),
    folds           :: Vector{Int} =
    begin
        n, r = divrem(size(yTr, 1), nfolds)
        shuffle!([repeat(1:nfolds, outer=n); 1:r])
    end,
    parallel        :: Bool=true,

    # Generic and common parameters
    tol             :: Real = 1e-5,
    verbose         :: Bool = true,
    ⏩              :: Bool = true,
)</code></pre><p>Create and fit an <strong>2-class</strong> elastic net logistic regression (<a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a>) machine learning model, with training data <code>𝐏Tr</code>, of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">ℍVector</a>, and corresponding labels <code>yTr</code>, of type <a href="../MainModule/#IntVector">IntVector</a>. Return the fitted model(s) as an instance of the <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> structure.</p><div class="admonition is-warning" id="Class-Labels-d4793e8f7121eaa"><header class="admonition-header">Class Labels<a class="admonition-anchor" href="#Class-Labels-d4793e8f7121eaa" title="Permalink"></a></header><div class="admonition-body"><p>Labels must be provided using the natural numbers, <em>i.e.</em>, <code>1</code> for the first class, <code>2</code> for the second class, etc.</p></div></div><p>As for all ML models acting in the tangent space, fitting an ENLR model involves computing a mean (barycenter) of all the matrices in <code>𝐏Tr</code>, projecting all matrices onto the tangent space after parallel transporting them at the identity matrix and vectorizing them using the <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#PosDefManifold.vecP">vecP</a> operation. Once this is done, the ENLR is fitted.</p><p>The mean is computed according to the <code>.metric</code> field of the <code>model</code>, with optional weights <code>w</code>. The <code>.metric</code> field of the <code>model</code> is passed internally to the <a href="../tools/#PosDefManifoldML.tsMap"><code>tsMap</code></a> function. By default the metric is the Fisher metric. See the examples here below to see how to change metric. See <a href="../mdm/#mdm.jl">mdm.jl</a> or check out directly the documentation of <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/">PosDefManifold.jl</a> for the available metrics.</p><p><strong>Optional keyword arguments</strong></p><p>If a <code>pipeline</code>, of type <a href="../conditioners/#PosDefManifoldML.Pipeline"><code>Pipeline</code></a> is provided,  all necessary parameters of the sequence of conditioners are fitted  and all input matrices <code>𝐏Tr</code> are transformed according to the specified pipeline  before fitting the ML model. The parameters are stored in the output ML model. Note that the fitted pipeline is automatically applied by any successive call  to function <a href="#StatsAPI.predict"><code>predict</code></a> to which the output ML model is passed as argument. Note that the input matrices <code>𝐏Tr</code> are transformed; pass a copy of <code>𝐏Tr</code> if you wish to mantain the original matrices.</p><p>By default, uniform weights will be given to all observations for computing the mean to project the data in the tangent space. This is equivalent to passing as argument <code>w=:uniform</code> (or <code>w=:u</code>). You can also pass as argument:</p><ul><li><code>w=:balanced</code> (or simply <code>w=:b</code>). If the two classes are unbalanced, the weights should better be inversely proportional to the number of examples for each class, in such a way that each class contributes equally to the computation of the mean. This is equivalent of passing <code>w=tsWeights(yTr)</code>. See the <a href="../tools/#PosDefManifoldML.tsWeights"><code>tsWeights</code></a> function for details.</li><li><code>w=v</code>, where <code>v</code> is a user defined vector of non-negative weights for the observations, thus, <code>v</code> must contain the same number of elements as <code>yTr</code>. For example, <code>w=[1.0, 1.0, 2.0, 2.0, ...., 1.0]</code></li><li><code>w=t</code>, where <code>t</code> is a 2-tuple of real weights, one weight for each class, for example <code>w=(0.5, 1.5)</code>. This is equivalent to passing <code>w=tsWeights(yTr; classWeights=collect(0.5, 1.5))</code>, see the <a href="../tools/#PosDefManifoldML.tsWeights"><code>tsWeights</code></a> function for details.</li></ul><p>By default <code>meanISR=nothing</code> and the inverse square root (ISR) of the mean used for projecting the matrices onto the tangent space (see <a href="../tools/#PosDefManifoldML.tsMap"><code>tsMap</code></a>) is computed. An Hermitian matrix or <code>I</code> (the identity matrix) can also be passed  as argument <code>meanISR</code> and in this case this matrix will be used as the ISR of the mean. Passed or computed, it will be written in the <code>.meanISR</code> field of the  model structure created by this function. Notice that passing <code>I</code>, the matrices will be projected onto the tangent space at the identity without recentering them. This is possible if the matrices have been recentered bt a pre-conditioning pipeline (see <a href="../conditioners/#PosDefManifoldML.Pipeline"><code>Pipeline</code></a>).</p><p>If <code>meanISR</code> is not provided and the <code>.metric</code> field of the <code>model</code> is Fisher, logdet0 or Wasserstein, the tolerance of the iterative algorithm used to compute the mean is set to argument <code>tol</code> (default 1e-5). Also, in this case a particular initialization for those iterative algorithms can be provided as an <code>Hermitian</code> matrix with argument <code>meanInit</code>.</p><div class="admonition is-success" id="Euclidean-ENLR-models-544837271c9ab53d"><header class="admonition-header">Euclidean ENLR models<a class="admonition-anchor" href="#Euclidean-ENLR-models-544837271c9ab53d" title="Permalink"></a></header><div class="admonition-body"><p>ML models acting on the tangent space allows to fit a model passing as training data <code>𝐏Tr</code> directly a matrix of feature vectors, where each feature vector is a row of the matrix. In this case none of the above keyword arguments are used.  </p></div></div><p><strong>The following optional keyword arguments act on any kind of input, that is, tangent vectors and generic feature vectors</strong></p><p>If a <code>UnitRange</code> is passed with optional keyword argument <code>vecRange</code>, then if <code>𝐏Tr</code> is a vector of <code>Hermitian</code> matrices, the vectorization of those matrices once they are projected onto the tangent space concerns only the rows (or columns) given in the specified range, else if <code>𝐏Tr</code> is a matrix with feature vectors arranged in its rows, then only the columns of <code>𝐏Tr</code> given in the specified range will be used. Argument <code>vecRange</code> will be ignored if a pre-conditioning pipeline is used  and if the pipeline changes the dimension of the input matrices. In this case it will be set to its default value using the new dimension. You are not allowed to change this behavior.</p><p>With <code>normalize</code> the tangent (or feature) vectors can be normalized individually. Three functions can be passed, namely </p><ul><li><a href="../tools/#PosDefManifoldML.demean!"><code>demean!</code></a> to remove the mean,</li><li><a href="../tools/#LinearAlgebra.normalize!"><code>normalize!</code></a> to fix the norm (default),</li><li><a href="../tools/#PosDefManifoldML.standardize!"><code>standardize!</code></a> to fix the mean to zero and the standard deviation to 1.</li></ul><p>As argument <code>normalize</code> you can also pass a 2-tuple of real numbers. In this case the numbers will be the lower and upper limit to bound the vectors within these limits - see <a href="../tools/#PosDefManifoldML.rescale!"><code>rescale!</code></a>.</p><div class="admonition is-success" id="Rescaling-f64251a77b4b83aa"><header class="admonition-header">Rescaling<a class="admonition-anchor" href="#Rescaling-f64251a77b4b83aa" title="Permalink"></a></header><div class="admonition-body"><p>If you wish to rescale, use <code>(-1, 1)</code>, since tangent vectors of SPD matrices have positive and negative elements. If <code>𝐏Tr</code> is a feature matrix and the features are only positive, use <code>(0, 1)</code> instead. </p></div></div><p>If you pass <code>nothing</code> as argument <code>normalize</code>, no normalization will be carried out.</p><p>The remaining optional keyword arguments, are</p><ul><li><p>the arguments passed to the <code>GLMNet.glmnet</code> function for fitting the models. Those are always used.</p></li><li><p>the <code>λSelMeth</code> argument and the arguments passed to the <code>GLMNet.glmnetcv</code> function for finding the best lambda hyperparamater by cross-validation. Those are used only if <code>fitType</code> = <code>:path</code> or = <code>:all</code>.</p></li></ul><p><strong>Optional keyword arguments for fitting the model(s) using GLMNet.jl</strong></p><p><code>alpha</code>: the hyperparameter in <span>$[0, 1]$</span> to trade-off an elestic-net model. <em>α=0</em> requests a pure <em>ridge</em> model and <em>α=1</em> a pure <em>lasso</em> model. This defaults to 1.0, which specifies a lasso model, unless the input <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> <code>model</code> has another value in the <code>alpha</code> field, in which case this value is used. If argument <code>alpha</code> is passed here, it will overwrite the <code>alpha</code> field of the input <code>model</code>.</p><p><code>weights</code>: a vector of weights for each matrix (or feature vectors) of the same size as <code>yTr</code>. It defaults to 1 for all matrices.</p><p><code>intercept</code>: whether to fit an intercept term. The intercept is always unpenalized. Defaults to true.</p><p>If <code>fitType</code> = <code>:best</code> (default), a cross-validation procedure is run to find the best lambda hyperparameter for the given training data. This finds a single model that is written into the <code>.best</code> field of the <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> structure that will be created.</p><p>If <code>fitType</code> = <code>:path</code>, the regularization path for several values of the lambda hyperparameter is found for the given training data. This creates several models, which are written into the <code>.path</code> field of the <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> structure that will be created, none of which is optimal, in the cross-validation sense, for the given training data.</p><p>If <code>fitType</code> = <code>:all</code>, both the above fits are performed and all fields of the <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> structure that will be created will be filled in.</p><p><code>penalty_factor</code>: a vector of length <em>n(n+1)/2</em>, where <em>n</em> is the dimension of the original PD matrices on which the model is applied, of penalties for each predictor in the tangent vectors. This defaults to all ones, which weights each predictor equally. To specify that a predictor should be unpenalized, set the corresponding entry to zero.</p><p><code>constraints</code>: an <em>[n(n+1)/2]</em> x <em>2</em> matrix specifying lower bounds (first column) and upper bounds (second column) on each predictor. By default, this is [-Inf Inf] for each predictor (each element of tangent vectors).</p><p><code>offset</code>: see documentation of original GLMNet package <a href="../">🎓</a>.</p><p><code>dfmax</code>: The maximum number of predictors in the largest model.</p><p><code>pmax</code>: The maximum number of predictors in any model.</p><p><code>nlambda</code>: The number of values of <em>λ</em> along the path to consider.</p><p><code>lambda_min_ratio</code>: The smallest <em>λ</em> value to consider, as a ratio of the value of <em>λ</em> that gives the null model (<em>i.e.</em>, the model with only an intercept). If the number of observations exceeds the number of variables, this defaults to 0.0001, otherwise 0.01.</p><p><code>lambda</code>: The <em>λ</em> values to consider for fitting. By default, this is determined from <code>nlambda</code> and <code>lambda_min_ratio</code>.</p><p><code>maxit</code>: The maximum number of iterations of the cyclic coordinate descent algorithm. If convergence is not achieved, a warning is returned.</p><p><code>algorithm</code>: the algorithm used to find the regularization path. Possible values are <code>:newtonraphson</code> (default) and <code>:modifiednewtonraphson</code>.</p><p>For further informations on those arguments, refer to the resources on the GLMNet package <a href="../">🎓</a>.</p><div class="admonition is-warning" id="Possible-change-of-dimension-41ef4333fe148576"><header class="admonition-header">Possible change of dimension<a class="admonition-anchor" href="#Possible-change-of-dimension-41ef4333fe148576" title="Permalink"></a></header><div class="admonition-body"><p>The provided arguments <code>penalty_factor</code>, <code>constraints</code>, <code>dfmax</code>, <code>pmax</code> and  <code>lambda_min_ratio</code> will be ignored if a pre-conditioning <code>pipeline</code> is passed  as argument and if the pipeline	changes the dimension of the input matrices,  thus of the tangent vectors. In this case they will be set to their  default values using the new dimension. To force the use of the provided values  instead, set <code>checkArgs</code> to false (true by default). Note however that in this  case you must provide suitable values for all the abova arguments.</p></div></div><p><strong>Optional Keyword arguments for finding the best model by cv</strong></p><p><code>λSelMeth</code> = <code>:sd1</code> (default), the best model is defined as the one allowing the highest <code>cvλ.meanloss</code> within one standard deviation of the minimum, otherwise it is defined as the one allowing the minimum <code>cvλ.meanloss</code>. Note that in selecting a model, the model with only the intercept term, if it exists, is ignored. See <a href="../enlr/#PosDefManifoldML.ENLRmodel"><code>ENLRmodel</code></a> for a description of the <code>.cvλ</code> field of the model structure.</p><p>Arguments <code>nfolds</code>, <code>folds</code> and <code>parallel</code> are passed to the <code>GLMNet.glmnetcv</code> function along with the <code>⏩</code> argument. Please refer to the resources on GLMNet for details <a href="../">🎓</a>.</p><p><code>tol</code>: Is the convergence criterion for both the computation of a mean for projecting onto the tangent space (if the metric requires an iterative algorithm) and for the GLMNet fitting algorithm. Defaults to 1e-5. In order to speed up computations, you may try to set a lower <code>tol</code>; The convergence will be faster but more coarse, with a possible drop of classification accuracy, depending on the signal-to-noise ratio of the input features.</p><p>If <code>verbose</code> is true (default), information is printed in the REPL.</p><p>The <code>⏩</code> argument (true by default) is passed to the <a href="../tools/#PosDefManifoldML.tsMap"><code>tsMap</code></a> function for projecting the matrices in <code>𝐏Tr</code> onto the tangent space and to the <code>GLMNet.glmnetcv</code> function to run inner cross-validation to find the <code>best</code> model using multi-threading.</p><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature">notation &amp; nomenclature</a>, <a href="../MainModule/#the-ℍVector-type">the ℍVector type</a></p><p><strong>See also</strong>: <a href="#StatsAPI.predict"><code>predict</code></a>, <a href="#PosDefManifoldML.crval"><code>crval</code></a></p><p><strong>Tutorial</strong>: <a href="../tutorial/#Examples-using-the-ENLR-model">Examples using the ENLR model</a></p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using PosDefManifoldML, PosDefManifold

# Generate some data
PTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.1)

# Fit an ENLR lasso model and find the best model by cross-validation
m = fit(ENLR(), PTr, yTr)

# ... standardizing the tangent vectors
m = fit(ENLR(), PTr, yTr; pipeline=p, normalize=standardize!)

# ... balancing the weights for tangent space mapping
m = fit(ENLR(), PTr, yTr; w=tsWeights(yTr))

# ... using the log-Eucidean metric for tangent space projection
m = fit(ENLR(logEuclidean), PTr, yTr)

# Fit an ENLR ridge model and find the best model by cv:
m = fit(ENLR(Fisher), PTr, yTr; alpha=0)

# Fit an ENLR elastic-net model (α=0.9) and find the best model by cv:
m = fit(ENLR(Fisher), PTr, yTr; alpha=0.9)

# Fit an ENLR lasso model and its regularization path:
m = fit(ENLR(), PTr, yTr; fitType=:path)

# Fit an ENLR lasso model, its regularization path
# and the best model found by cv:
m = fit(ENLR(), PTr, yTr; fitType=:all)

# Fit using a pre-conditioning pipeline:
p = @→ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)
m = fit(ENLR(PosDefManifold.Euclidean), PTr, yTr; pipeline=p)

# Use a recentering pipeline and project the data
# onto the tangent space at the identity matrix.
# In this case the metric is irrilevant as the barycenter
# for determining the base point is not computed.
# Note that the previous call to &#39;fit&#39; has modified `PTr`,
# so we generate new data.
PTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.1)
p = @→ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)
m = fit(ENLR(), PTr, yTr; pipeline=p, meanISR=I)</code></pre></div></section><section><div><pre><code class="language-julia hljs">function fit(model     :: SVMmodel,
               𝐏Tr     :: Union{HermitianVector, Matrix{Float64}},
               yTr     :: IntVector=[];

	# pipeline (data transformations)
	pipeline    :: Union{Pipeline, Nothing} = nothing,

	# parameters for projection onto the tangent space
	w		:: Union{Symbol, Tuple, Vector} = Float64[],
	meanISR 	:: Union{Hermitian, Nothing, UniformScaling} = nothing,
	meanInit 	:: Union{Hermitian, Nothing} = nothing,
	vecRange	:: UnitRange = 𝐏Tr isa HermitianVector ? (1:size(𝐏Tr[1], 2)) : (1:size(𝐏Tr, 2)),
	normalize	:: Union{Function, Tuple, Nothing} = normalize!,

	# SVM parameters
	svmType 	:: Type = SVC,
	kernel 		:: Kernel.KERNEL = Linear,
	epsilon 	:: Float64 = 0.1,
	cost		:: Float64 = 1.0,
	gamma 		:: Float64	= 1/_getDim(𝐏Tr, vecRange),
	degree 		:: Int64	= 3,
	coef0 		:: Float64	= 0.,
	nu 			:: Float64 = 0.5,
	shrinking 	:: Bool = true,
	probability	:: Bool = false,
	weights 	:: Union{Dict{Int, Float64}, Nothing} = nothing,
	cachesize 	:: Float64	= 200.0,
	checkArgs	:: Bool = true,

	# Generic and common parameters
	tol		:: Real = 1e-5,
	verbose		:: Bool = true,
	⏩		:: Bool = true)</code></pre><p>Create and fit a <strong>1-class</strong> or <strong>2-class</strong> support vector machine (<a href="../svm/#PosDefManifoldML.SVM"><code>SVM</code></a>) machine learning model, with training data <code>𝐏Tr</code>, of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">ℍVector</a>, and corresponding labels <code>yTr</code>, of type <a href="../MainModule/#IntVector">IntVector</a>. The label vector can be omitted if the <code>svmType</code> is <code>OneClassSVM</code> (see <a href="../svm/#PosDefManifoldML.SVM"><code>SVM</code></a>). Return the fitted model as an instance of the <a href="../svm/#PosDefManifoldML.SVM"><code>SVM</code></a> structure.</p><div class="admonition is-warning" id="Class-Labels-d4793e8f7121eaa"><header class="admonition-header">Class Labels<a class="admonition-anchor" href="#Class-Labels-d4793e8f7121eaa" title="Permalink"></a></header><div class="admonition-body"><p>Labels must be provided using the natural numbers, <em>i.e.</em>, <code>1</code> for the first class, <code>2</code> for the second class, etc.</p></div></div><p>As for all ML models acting in the tangent space, fitting an SVM model involves computing a mean (barycenter) of all the matrices in <code>𝐏Tr</code>, projecting all matrices onto the tangent space after parallel transporting them at the identity matrix and vectorizing them using the <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#PosDefManifold.vecP">vecP</a> operation. Once this is done, the support-vector machine is fitted.</p><p><strong>Optional keyword arguments</strong></p><p>For the following keyword arguments see the documentation of the <a href="#StatsAPI.fit"><code>fit</code></a>  funtion for the ENLR (Elastic Net Logistic Regression) machine learning model:</p><ul><li><code>pipeline</code>, <code>transform</code> (pre-conditioning),</li><li><code>w</code>, <code>meanISR</code>, <code>meanInit</code>, <code>vecRange</code> (tangent space projection),</li></ul><div class="admonition is-success" id="Euclidean-SVM-models-80a20f75eda47a7"><header class="admonition-header">Euclidean SVM models<a class="admonition-anchor" href="#Euclidean-SVM-models-80a20f75eda47a7" title="Permalink"></a></header><div class="admonition-body"><p>ML models acting on the tangent space allows to fit a model passing as training data <code>𝐏Tr</code> directly a matrix of feature vectors, where each feature vector is a row of the matrix. In this case none of the above keyword arguments are used.</p></div></div><ul><li><code>normalize</code> (tangent or feature vectors normalization).</li></ul><p><strong>Optional keyword arguments for fitting the model(s) using LIBSVM.jl</strong></p><p><code>svmType</code> and <code>kernel</code> allow to chose among several available SVM models. See the documentation of the <a href="../svm/#PosDefManifoldML.SVM"><code>SVM</code></a> structure.</p><p><code>epsilon</code>, with default 0.1, is the epsilon in loss function of the <code>epsilonSVR</code> SVM model.</p><p><code>cost</code>, with default 1.0, is the cost parameter <em>C</em> of <code>SVC</code>, <code>epsilonSVR</code>, and <code>nuSVR</code> SVM models.</p><p><code>gamma</code>, defaulting to 1 divided by the length of the tangent (or feature) vectors, is the <em>γ</em> parameter for <code>RadialBasis</code>, <code>Polynomial</code> and <code>Sigmoid</code> kernels. The provided argument <code>gamma</code> will be ignored if a pre-conditioning <code>pipeline</code>  is passed as argument and if the pipeline changes the dimension of the input matrices,  thus of the tangent vectors. In this case it will be set to its default value using  the new dimension. To force the use of the provided <code>gamma</code> value instead,  set	<code>checkArgs</code> to false (true by default).</p><p><code>degree</code>, with default 3, is the degree for <code>Polynomial</code> kernels</p><p><code>coef0</code>, zero by default, is a parameter for the <code>Sigmoid</code> and <code>Polynomial</code> kernel.</p><p><code>nu</code>, with default 0.5, is the parameter <em>ν</em> of <code>nuSVC</code>, <code>OneClassSVM</code>, and <code>nuSVR</code> SVM models. It should be in the interval (0, 1].</p><p><code>shrinking</code>, true by default, sets whether to use the shrinking heuristics.</p><p><code>probability</code>, false by default sets whether to train a <code>SVC</code> or <code>SVR</code> model allowing probability estimates.</p><p>if a <code>Dict{Int, Float64}</code> is passed as <code>weights</code> argument, it will be used to give weights to the classes. By default it is equal to <code>nothing</code>, implying equal weights to all classes.</p><p><code>cachesize</code> for the kernel, 200.0 by defaut (in MB), can be increased for very large problems.</p><p><code>tol</code> is the convergence criterion for both the computation of a mean for projecting onto the tangent space (if the metric requires an iterative algorithm) and for the LIBSVM fitting algorithm. Defaults to 1e-5.</p><p>If <code>verbose</code> is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL. </p><p>The <code>⏩</code> argument (true by default) is passed to the <a href="../tools/#PosDefManifoldML.tsMap"><code>tsMap</code></a> function for projecting the matrices in <code>𝐏Tr</code> onto the tangent space and to the LIBSVM function that perform the fit in order to run them in multi-threaded mode.</p><p>For further information on tho LIBSVM arguments, refer to the resources on the LIBSVM package <a href="../">🎓</a>.</p><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature">notation &amp; nomenclature</a>, <a href="../MainModule/#the-ℍVector-type">the ℍVector type</a></p><p><strong>See also</strong>: <a href="#StatsAPI.predict"><code>predict</code></a>, <a href="#PosDefManifoldML.crval"><code>crval</code></a></p><p><strong>Tutorial</strong>: <a href="../tutorial/#Examples-using-SVM-models">Examples using SVM models</a></p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using PosDefManifoldML, PosDefManifold

# Generate some data
PTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.1);

# Fit a SVC SVM model and find the best model by cross-validation:
m = fit(SVM(), PTr, yTr)

# ... balancing the weights for tangent space mapping
m = fit(SVM(), PTr, yTr; w=:b)

# ... using the log-Eucidean metric for tangent space projection
m = fit(SVM(logEuclidean), PTr, yTr)

# ... using the linear kernel
m = fit(SVM(logEuclidean), PTr, yTr, kernel=Linear)

# or

m = fit(SVM(logEuclidean; kernel=Linear), PTr, yTr)

# ... using the Nu-Support Vector Classification
m = fit(SVM(logEuclidean), PTr, yTr, kernel=Linear, svmtype=NuSVC)

# or

m = fit(SVM(logEuclidean; kernel=Linear, svmtype=NuSVC), PTr, yTr)

# N.B. all other keyword arguments must be passed to the fit function
# and not to the SVM constructor.

# Fit a SVC SVM model using a pre-conditioning pipeline:
p = @→ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)
m = fit(SVM(PosDefManifold.Euclidean), PTr, yTr; pipeline=p)

# Use a recentering pipeline and project the data
# onto the tangent space at the identity matrix.
# In this case the metric is irrilevant as the barycenter
# for determining the base point is not computed.
# Note that the previous call to &#39;fit&#39; has modified `PTr`,
# so we generate new data.
PTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.1)
p = @→ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)
m = fit(SVM(), PTr, yTr; pipeline=p, meanISR=I)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StatsAPI.predict" href="#StatsAPI.predict"><code>StatsAPI.predict</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">function predict(model  :: MDMmodel,
                 𝐏Te    :: ℍVector,
                 what   :: Symbol = :labels;
        pipeline    :: Union{Pipeline, Nothing} = nothing,
        verbose     :: Bool = true,
        ⏩          :: Bool = true)</code></pre><p>Given an <a href="../mdm/#PosDefManifoldML.MDM"><code>MDM</code></a> <code>model</code> trained (fitted) on <em>z</em> classes and a testing set of <em>k</em> positive definite matrices <code>𝐏Te</code> of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">ℍVector</a>:</p><p>if <code>what</code> is <code>:labels</code> or <code>:l</code> (default), return the predicted <strong>class labels</strong> for each matrix in <code>𝐏Te</code>, as an <a href="../MainModule/#IntVector">IntVector</a>. For MDM models, the predicted class &#39;label&#39; of an unlabeled matrix is the serial number of the class whose mean is the closest to the matrix (minimum distance to mean). The labels are &#39;1&#39; for class 1, &#39;2&#39; for class 2, etc;</p><p>if <code>what</code> is <code>:probabilities</code> or <code>:p</code>, return the predicted <strong>probabilities</strong> for each matrix in <code>𝐏Te</code> to belong to all classes, as a <em>k</em>-vector of <em>z</em> vectors holding reals in <span>$[0, 1]$</span>. The &#39;probabilities&#39; are obtained passing to a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> the squared distances of each unlabeled matrix to all class means with inverted sign;</p><p>if <code>what</code> is <code>:f</code> or <code>:functions</code>, return the <strong>output function</strong> of the model as a <em>k</em>-vector of <em>z</em> vectors holding reals. The function of each element in <code>𝐏Te</code> is the ratio of the  squared distance from each class to the (scalar) geometric mean of the  squared distances from all classes.</p><p>If <code>verbose</code> is true (default), information is printed in the REPL.</p><p>It f <code>⏩</code> is true (default), the computation of distances is multi-threaded.</p><p>Note that if the field <code>pipeline</code> of the provided <code>model</code> is not <code>nothing</code>, implying that a pre-conditioning pipeline has been fitted, the pipeline is applied to the data before to carry out the prediction. If you wish to <strong>adapt</strong> the pipeline to the testing data,  just fit the pipeline to the testing data overwriting the model pipeline. This is useful in a cross-session and cross-subject setting.</p><div class="admonition is-warning" id="Adapting-the-Pipeline-90cdb23bb2ad0c26"><header class="admonition-header">Adapting the Pipeline<a class="admonition-anchor" href="#Adapting-the-Pipeline-90cdb23bb2ad0c26" title="Permalink"></a></header><div class="admonition-body"><p>Be careful when adapting a pipeline; if a <a href="../conditioners/#PosDefManifoldML.Recenter"><code>Recenter</code></a> conditioner is included in the pipeline and dimensionality reduction was sought (parameter <code>eVar</code> different  from <code>nothing</code>), then <code>eVar</code> must be set to an integer so that the dimension of the training ad testing data is the same after adaptation. See the example here below.</p></div></div><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature">notation &amp; nomenclature</a>, <a href="../MainModule/#the-ℍVector-type">the ℍVector type</a></p><p><strong>See also</strong>: <a href="#StatsAPI.fit"><code>fit</code></a>, <a href="#PosDefManifoldML.crval"><code>crval</code></a>, <a href="../stats_descriptive/#PosDefManifoldML.predictErr"><code>predictErr</code></a></p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using PosDefManifoldML, PosDefManifold

# Generate some data
PTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80)

# Craete and fit an MDM model
m = fit(MDM(Fisher), PTr, yTr)

# Predict labels
yPred = predict(m, PTe, :l)

# Prediction error
predErr = predictErr(yTe, yPred)

# Predict probabilities
predict(m, PTe, :p)

# Output functions
predict(m, PTe, :f)

# Using and adapting a pipeline

# get some random data and labels as an example
PTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80)

# For adaptation, we need to set `eVar` to an integer or to `nothing`.
# We will use the dimension determined on training data.
# Note that the adaptation does not work well if the class proportions
# of the training data is different from the class proportions of the test data.
p = @→ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)

# Fit the model using the pre-conditioning pipeline
m = fit(MDM(), PTr, yTr; pipeline = p)

# Define the same pipeline with fixed dimensionality reduction parameter
p = @→ Recenter(; eVar=dim(m.pipeline)) Compress Shrink(Fisher; radius=0.02)

# Fit the pipeline to testing data (adapt):
predict(m, PTe, :l; pipeline=p) 

# Suppose we want to adapt recentering, but not shrinking, which also has a 
# learnable parameter. We would then use this pipeline instead:
p = deepcopy(m.pipeline)
p[1].eVar = dim(m.pipeline)</code></pre></div></section><section><div><pre><code class="language-julia hljs">function predict(model   :: ENLRmodel,
		𝐏Te         :: Union{ℍVector, Matrix{Float64}},
		what        :: Symbol = :labels,
		fitType     :: Symbol = :best,
		onWhich     :: Int    = Int(fitType==:best);
    pipeline    :: Union{Pipeline, Nothing} = nothing,
    meanISR     :: Union{ℍ, Nothing, UniformScaling} = nothing,
    verbose     :: Bool = true,
    ⏩          :: Bool = true)</code></pre><p>Given an <a href="../enlr/#PosDefManifoldML.ENLR"><code>ENLR</code></a> <code>model</code> trained (fitted) on 2 classes and a testing set of <em>k</em> positive definite matrices <code>𝐏Te</code> of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">ℍVector</a>,</p><p>if <code>what</code> is <code>:labels</code> or <code>:l</code> (default), return the predicted <strong>class labels</strong> for each matrix in <code>𝐏Te</code>, as an <a href="../MainModule/#IntVector">IntVector</a>. Those labels are &#39;1&#39; for class 1 and &#39;2&#39; for class 2;</p><p>if <code>what</code> is <code>:probabilities</code> or <code>:p</code>, return the predicted <strong>probabilities</strong> for each matrix in <code>𝐏Te</code> to belong to each classe, as a <em>k</em>-vector of <em>z</em> vectors holding reals in <em>[0, 1]</em> (probabilities). The &#39;probabilities&#39; are obtained passing to a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> the output of the ENLR model and zero;</p><p>if <code>what</code> is <code>:f</code> or <code>:functions</code>, return the <strong>output function</strong> of the model, which is the raw output of the ENLR model.</p><p>If <code>fitType</code> = <code>:best</code> (default), the best model that has been found by cross-validation is used for prediction.</p><p>If <code>fitType</code> = <code>:path</code>,</p><ul><li>if <code>onWhich</code> is a valid serial number for a model in the <code>model.path</code>,</li></ul><p>then this model is used for prediction,</p><ul><li>if <code>onWhich</code> is zero, all models in the <code>model.path</code> will be used for predictions, thus the output will be multiplied by the number of models in <code>model.path</code>.</li></ul><p>Argument <code>onWhich</code> has no effect if <code>fitType</code> = <code>:best</code>.</p><div class="admonition is-info" id="Nota-Bene-71daa2093a5fc7c9"><header class="admonition-header">Nota Bene<a class="admonition-anchor" href="#Nota-Bene-71daa2093a5fc7c9" title="Permalink"></a></header><div class="admonition-body"><p>By default, the <a href="#StatsAPI.fit"><code>fit</code></a> function fits only the <code>best</code> model. If you want to use the <code>fitType</code> = <code>:path</code> option you need to invoke the fit function with optional keyword argument <code>fitType</code>=<code>:path</code> or <code>fitType</code>=<code>:all</code>. See the <a href="#StatsAPI.fit"><code>fit</code></a> function for details.</p></div></div><p>Optional keyword argument <code>meanISR</code> can be used to specify the principal inverse square root (ISR) of a new mean to be used as base point for projecting the matrices in testing set <code>𝐏Te</code> onto the tangent space. By default <code>meanISR</code> is equal to nothing, implying that the base point will be the mean used to fit the model. This corresponds to the classical &#39;training-test&#39; mode of operation.</p><p>Passing with argument <code>meanISR</code> a new mean ISR allows the <em>adaptation</em> first described in Barachant et <em>al.</em> (2013)<a href="../">🎓</a>. Typically <code>meanISR</code> is the ISR of the mean of the matrices in <code>𝐏Te</code> or of a subset of them. Notice that this actually performs <em>transfer learning</em> by parallel transporting both the training and test data to the identity matrix as defined in Zanini et <em>al.</em> (2018) and later taken up in Rodrigues et <em>al.</em> (2019)<a href="../">🎓</a>.   You can aslo pass <code>meanISR=I</code>, in which case the base point is taken as the identity matrix. This is possible if the set <code>𝐏Te</code> is centered to the identity, for instance, if a recentering pre-conditioner is included in a pipeline and the pipeline  is adapted as well (see the example below).</p><p>If <code>verbose</code> is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.</p><p>If ⏩ = true (default) and <code>𝐏Te</code> is an ℍVector type, the projection onto the tangent space is multi-threaded.</p><p>Note that if the field <code>pipeline</code> of the provided <code>model</code> is not <code>nothing</code>, implying that a pre-conditioning pipeline has been fitted during the fitting of the model, the pipeline is applied to the data before to carry out the prediction. If you wish to <strong>adapt</strong> the pipeline to the testing data,  just pass the same pipeline as argument <code>pipeline</code> in this function.</p><div class="admonition is-warning" id="Adapting-the-Pipeline-90cdb23bb2ad0c26"><header class="admonition-header">Adapting the Pipeline<a class="admonition-anchor" href="#Adapting-the-Pipeline-90cdb23bb2ad0c26" title="Permalink"></a></header><div class="admonition-body"><p>Be careful when adapting a pipeline; if a <a href="../conditioners/#PosDefManifoldML.Recenter"><code>Recenter</code></a> conditioner is included in the pipeline and dimensionality reduction was sought (parameter <code>eVar</code> different  from <code>nothing</code>), then <code>eVar</code> must be set to an integer so that the dimension of the training ad testing data is the same after adaptation. See the example here below.</p></div></div><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature">notation &amp; nomenclature</a>, <a href="../MainModule/#the-ℍVector-type">the ℍVector type</a></p><p><strong>See also</strong>: <a href="#StatsAPI.fit"><code>fit</code></a>, <a href="#PosDefManifoldML.crval"><code>crval</code></a>, <a href="../stats_descriptive/#PosDefManifoldML.predictErr"><code>predictErr</code></a></p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using PosDefManifoldML, PosDefManifold

# Generate some data
PTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80)

# Fit an ENLR lasso model and find the best model by cv
m = fit(ENLR(Fisher), PTr, yTr)

# Predict labels from the best model
yPred = predict(m, PTe, :l)

# Prediction error
predErr = predictErr(yTe, yPred)

# Predict probabilities from the best model
predict(m, PTe, :p)

# Output functions from the best model
predict(m, PTe, :f)

# Fit a regularization path for an ENLR lasso model
m = fit(ENLR(Fisher), PTr, yTr; fitType=:path)

# Predict labels using a specific model
yPred = predict(m, PTe, :l, :path, 10)

# Predict labels for all models
yPred = predict(m, PTe, :l, :path, 0)

# Prediction error for all models
predErr = [predictErr(yTe, yPred[:, i]) for i=1:size(yPred, 2)]

# Predict probabilities from a specific model
predict(m, PTe, :p, :path, 12)

# Predict probabilities from all models
predict(m, PTe, :p, :path, 0)

# Output functions from specific model
predict(m, PTe, :f, :path, 3)

# Output functions for all models
predict(m, PTe, :f, :path, 0)

## Adapting the base point
PTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80)
m = fit(ENLR(Fisher), PTr, yTr)
predict(m, PTe, :l; meanISR=invsqrt(mean(Fisher, PTe)))

# Also using and adapting a pre-conditioning pipeline
# For adaptation, we need to set `eVar` to an integer or to `nothing`.
# We will use the dimension determined on training data.
# Note that the adaptation does not work well if the class proportions
# of the training data is different from the class proportions of the test data.
p = @→ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)

# Fit the model using the pre-conditioning pipeline
m = fit(ENLR(), PTr, yTr; pipeline = p)

# Define the same pipeline with fixed dimensionality reduction parameter
p = @→ Recenter(; eVar=dim(m.pipeline)) Compress Shrink(Fisher; radius=0.02)

# Fit the pipeline to testing data (adapt) and use the identity matrix as base point:
predict(m, PTe, :l; pipeline=p, meanISR=I) 

# Suppose we want to adapt recentering, but not shrinking, which also has a 
# learnable parameter. We would then use this pipeline instead:
p = deepcopy(m.pipeline)
p[1].eVar = dim(m.pipeline)</code></pre></div></section><section><div><pre><code class="language-julia hljs">function predict(model :: SVMmodel,
			𝐏Te	:: Union{ℍVector, Matrix{Float64}},
			what	:: Symbol = :labels;
		meanISR	:: Union{ℍ, Nothing, UniformScaling} = nothing,
		pipeline:: Union{Pipeline, Nothing} = nothing,
		verbose	:: Bool = true,
		⏩	:: Bool = true)</code></pre><p>Compute predictions given an <a href="../svm/#PosDefManifoldML.SVM"><code>SVM</code></a> <code>model</code> trained (fitted) on 2 classes and a testing set of <em>k</em> positive definite matrices <code>𝐏Te</code> of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">ℍVector</a>.</p><p>For the meaning of arguments <code>what</code>, <code>meanISR</code>, <code>pipeline</code> and <code>verbose</code>, see the documentation of the <a href="#StatsAPI.predict"><code>predict</code></a> function for the ENLR model.</p><p>If ⏩ = true (default) and <code>𝐏Te</code> is an ℍVector type, the projection onto the tangent space will be multi-threaded. Also, the prediction of the LIBSVM.jl prediction function will be multi-threaded.</p><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature">notation &amp; nomenclature</a>, <a href="../MainModule/#the-ℍVector-type">the ℍVector type</a></p><p><strong>See also</strong>: <a href="#StatsAPI.fit"><code>fit</code></a>, <a href="#PosDefManifoldML.crval"><code>crval</code></a>, <a href="../stats_descriptive/#PosDefManifoldML.predictErr"><code>predictErr</code></a></p><p><strong>Examples</strong> </p><p>see the examples for the <a href="#StatsAPI.predict"><code>predict</code></a> function for the ENLR model; the syntax is identical, only the model used there has to be changed with a <code>SVMmodel</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PosDefManifoldML.crval" href="#PosDefManifoldML.crval"><code>PosDefManifoldML.crval</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">function crval(model    :: MLmodel,
               𝐏        :: ℍVector,
               y        :: IntVector;
        pipeline    :: Union{Pipeline, Nothing} = nothing,
        nFolds      :: Int     = min(10, length(y)÷3),
        shuffle     :: Bool    = false,
        scoring     :: Symbol  = :b,
        hypTest     :: Union{Symbol, Nothing} = :Bayle,
        verbose     :: Bool    = true,
        outModels   :: Bool    = false,
        ⏩           :: Bool    = true,
        fitArgs...)</code></pre><p>Stratified cross-validation accuracy for a machine learning <code>model</code> given an ℍVector <span>$𝐏$</span> holding <em>k</em> Hermitian matrices and an <a href="../MainModule/#IntVector">IntVector</a> <code>y</code> holding the <em>k</em> labels for these matrices. Return a <a href="#PosDefManifoldML.CVres"><code>CVres</code></a> structure.</p><p>For each fold, a machine learning model is fitted on training data and labels are predicted on testing data. Summary classification performance statistics are stored in the output structure.</p><p><strong>Optional keyword arguments</strong></p><p>If a <code>pipeline</code>, of type <a href="../conditioners/#PosDefManifoldML.Pipeline"><code>Pipeline</code></a> is provided,  the pipeline is fitted on training data and applied for predicting the testing data.</p><p><code>nFolds</code> by default is set to the minimum between 10 and the number of observation ÷ 3 (integer division).</p><p>If <code>scoring</code>=:b (default) the <strong>balanced accuracy</strong> is computed. Any other value will make the function returning the regular <strong>accuracy</strong>. Balanced accuracy is to be preferred for unbalanced classes. For balanced classes the balanced accuracy reduces to the regular accuracy, therefore there is no point in using regular accuracy if not to avoid a few unnecessary computations when the class are balanced.</p><div class="admonition is-info" id="Error-loss-c750f8ea043ea10b"><header class="admonition-header">Error loss<a class="admonition-anchor" href="#Error-loss-c750f8ea043ea10b" title="Permalink"></a></header><div class="admonition-body"><p>Note that this function computes the error loss for each fold (see <a href="#PosDefManifoldML.CVres"><code>CVres</code></a>). The average error loss is the complement of accuracy,  not of balanced accuracy. If the classes are balanced and you use <code>scoring</code>=:a (accuracy),  the average error loss within each fold is equal to 1 minus the average accuracy,  which is also computed by this function. However, this is not true if the classes are unbalanced and you use <code>scoring</code>=:b (default). In this case the returned error loss and accuracy may appear incoherent.</p></div></div><p><code>hypTest</code> can be <code>nothing</code> or a symbol specifying the kind of statistical test to be carried out. At the moment, only <code>:Bayle</code> is a possible symbol and this test is performed by default. Bayle&#39;s procedure tests whether the average observed binary error loss is inferior to what is to be  expected by the hypothesis of random chance, which is set to <span>$1-\frac{1}{z}$</span>, where <span>$z$</span> is the number of classes (see <a href="../stats_inferential/#PosDefManifoldML.testCV"><code>testCV</code></a>).</p><p>For the meaning of the <code>shuffle</code> argument (false by default), see function <a href="#PosDefManifoldML.cvSetup"><code>cvSetup</code></a>, to which this argument is passed internally.</p><p>For the meaning of the <code>seed</code> argument (1234 by default), see function <a href="#PosDefManifoldML.cvSetup"><code>cvSetup</code></a>, to which this argument is passed internally.</p><p>If <code>verbose</code> is true (default), information is printed in the REPL.</p><p>If <code>outModels</code> is true, return a 2-tuple holding a <a href="#PosDefManifoldML.CVres"><code>CVres</code></a> structure and a <code>nFolds</code>-vector of the model fitted for each fold, otherwise (default), return only a <a href="#PosDefManifoldML.CVres"><code>CVres</code></a> structure.</p><p>If <code>⏩</code> the computations are multi-threaded across folds. It is true by default. Set it to false if there are problems in running this function and for debugging.</p><div class="admonition is-info" id="Multi-threading-662b10381c5f5d19"><header class="admonition-header">Multi-threading<a class="admonition-anchor" href="#Multi-threading-662b10381c5f5d19" title="Permalink"></a></header><div class="admonition-body"><p>If you run the cross-validation with independent threads per fold setting <code>⏩=true</code>(default), the <a href="../conditioners/#StatsAPI.fit!"><code>fit!</code></a> and <a href="#StatsAPI.predict"><code>predict</code></a> function that will be called within each fold will we run in single-threaded mode. Vice versa, if you pass <code>⏩=false</code>, these two functions will be run in multi-threaded mode. This is done to avoid overshooting the number of threads to be activated.</p></div></div><p><code>fitArgs</code> are optional keyword arguments that are passed to the <a href="#StatsAPI.fit"><code>fit</code></a> function called for each fold of the cross-validation. For each machine learning model, all optional keyword arguments of their fit method are elegible to be passed here, however, the arguments listed in the following table for each model should not be passed. Note that if they are passed, they will be disabled:</p><table><tr><th style="text-align: center">MDM/MDMF</th><th style="text-align: center">ENLR</th><th style="text-align: center">SVM</th></tr><tr><td style="text-align: center"><code>verbose</code></td><td style="text-align: center"><code>verbose</code></td><td style="text-align: center"><code>verbose</code></td></tr><tr><td style="text-align: center"><code>⏩</code></td><td style="text-align: center"><code>⏩</code></td><td style="text-align: center"><code>⏩</code></td></tr><tr><td style="text-align: center"><code>meanInit</code></td><td style="text-align: center"><code>meanInit</code></td><td style="text-align: center"><code>meanInit</code></td></tr><tr><td style="text-align: center"><code>meanISR</code></td><td style="text-align: center"><code>fitType</code></td><td style="text-align: center"></td></tr><tr><td style="text-align: center"></td><td style="text-align: center"><code>offsets</code></td><td style="text-align: center"></td></tr><tr><td style="text-align: center"></td><td style="text-align: center"><code>lambda</code></td><td style="text-align: center"></td></tr><tr><td style="text-align: center"></td><td style="text-align: center"><code>folds</code></td><td style="text-align: center"></td></tr></table><p>If you pass the <code>meanISR</code> argument, this must be nothing (default)  or I (the identity matrix). If you pass <code>meanISR=I</code> for a tangent space model, parallel transport of the points to the identity before projecting the points onto the tangent space will not be carried out. This can be used if a recentering conditioner is passed in the <code>pipeline</code> (see the <a href="#StatsAPI.fit"><code>fit</code></a> method for the ENLR and SVM model).</p><p>Also, if you pass a <code>w</code> argument (weights for barycenter estimations), do not pass a vector of weights, just pass a symbol, <em>e.g.</em>, <code>w=:b</code> for balancing weights.</p><p><strong>See</strong>: <a href="../MainModule/#notation-and-nomenclature">notation &amp; nomenclature</a>, <a href="../MainModule/#the-ℍVector-type">the ℍVector type</a></p><p><strong>See also</strong>: <a href="#StatsAPI.fit"><code>fit</code></a>, <a href="#StatsAPI.predict"><code>predict</code></a></p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using PosDefManifoldML, PosDefManifold

# Generate some data
P, _dummyP, y, _dummyy = gen2ClassData(10, 60, 80, 30, 40, 0.2)

# Perform 10-fold cross-validation using the minimum distance to mean classifier
# adopting the Fisher-Rao (affine-invariant) metric (default)
cv = crval(MDM(), P, y)

# Adopting the log-Euclidean metric
cv = crval(MDM(logEuclidean), P, y)

# Apply a pre-conditioning pipeline to adopt the pseudo affine-invariant metric
p = @→ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)
cv = crval(MDM(Euclidean), P, y; pipeline = p)

# Apply a pre-conditioning pipeline and project the data 
# onto the tangent space at I without recentering the matrices.
# Note that this makes sense only for tangent space ML models.
p = @→ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)
cv = crval(ENLR(), P, y; pipeline = p, meanISR=I)

# Perform 10-fold cross-validation using the lasso logistic regression classifier
cv = crval(ENLR(Fisher), P, y)

# ...using the support-vector machine classifier
cv = crval(SVM(Fisher), P, y)

# ...with a Polynomial kernel of order 3 (default)
cv = crval(SVM(Fisher), P, y; kernel=kernel.Polynomial)

# Perform 8-fold cross-validation instead
cv = crval(SVM(Fisher), P, y; nFolds=8)

# ...balance the weights for tangent space projection
cv = crval(ENLR(Fisher), P, y; nFolds=8, w=:b)

# perform another cross-validation shuffling the folds
cv = crval(ENLR(Fisher), P, y; shuffle=true, nFolds=8, w=:b)
</code></pre></div></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PosDefManifoldML.cvSetup" href="#PosDefManifoldML.cvSetup"><code>PosDefManifoldML.cvSetup</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">function cvSetup(y            :: Vector{Int64},  
                 nCV          :: Int64;           
                 shuffle      :: Bool = false,
                 seed         :: Int = 1234)</code></pre><p>Given a vector of labels <code>y</code> and a parameter <code>nCV</code>, this function generates indices for nCV-fold cross-validation sets, organized by class.</p><p>The function performs a stratified cross-validation by maintaining the same class distribution across all folds. This ensures that each fold contains approximately the same proportion of samples from each class as in the complete dataset.</p><p>Each element is used exactly once as a test sample across all folds, ensuring that the entire dataset is covered.</p><p>The <code>shuffle</code> parameter controls whether the indices within each class are randomized. When <code>shuffle</code> is false (default), the original sequence of indices is preserved, ensuring  consistent results across multiple executions.</p><p>When <code>shuffle</code>, is true the indices within each class are randomly permuted before creating  the cross-validation folds.  Randomization is controlled by the <code>seed</code> parameter (default: 1234).  Using the same <code>seed</code> value generates identical cross-validation sets. Using different <code>seed</code> values produce different random partitions.</p><p>This combination of <code>shuffle</code> and <code>seed</code> parameters allows you to generate reproducible random  splits for consistent experimentation, create different random partitions to assess the robustness  of your results and maintain exact reproducibility of your cross-validation experiments.</p><p>This function is used in <a href="#PosDefManifoldML.crval"><code>crval</code></a>. It constitutes the fundamental basis to implement customized cross-validation procedures.</p><p>Return the 2-tuple (indTr, indTe) where:</p><ul><li>indTr is an array of arrays where indTr[i][f] contains the training indices for class i in fold f</li><li>indTe is an array of arrays where indTe[i][f] contains the test indices for class i in fold f</li></ul><p>Each array is organized by class and then by fold, ensuring stratified sampling across the cross-validation sets.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using PosDefManifoldML, PosDefManifold

y = [1,1,1,1,2,2,2,2,2,2]

cvSetup(y, 2)
# returns:
# Training Arrays:
#   Class 1: Array{Int64}[[3, 4], [1, 2]]
#   Class 2: Array{Int64}[[4, 5, 6], [1, 2, 3]]
# Testing Arrays:
#   Class 1: Array{Int64}[[1, 2], [3, 4]]
#   Class 2: Array{Int64}[[1, 2, 3], [4, 5, 6]]

cvSetup(y, 2; shuffle=true, seed=1)
# returns:
# Training Arrays:
#   Class 1: Array{Int64}[[1, 4], [2, 3]]
#   Class 2: Array{Int64}[[1, 3, 4], [2, 5, 6]]
# Testing Arrays:
#   Class 1: Array{Int64}[[2, 3], [1, 4]]
#   Class 2: Array{Int64}[[2, 5, 6], [1, 3, 4]]

cvSetup(y, 3)
# returns:
# Training Arrays:
#   Class 1: Array{Int64}[[2, 3], [1, 3, 4], [1, 2, 4]]
#   Class 2: Array{Int64}[[3, 4, 5, 6], [1, 2, 5, 6], [1, 2, 3, 4]]
# Testing Arrays:
#   Class 1: Array{Int64}[[1, 4], [2], [3]]
#   Class 2: Array{Int64}[[1, 2], [3, 4], [5, 6]]</code></pre></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../svm/">« Support-Vector Machine</a><a class="docs-footer-nextpage" href="../stats_descriptive/">Descriptive »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Tuesday 17 June 2025 14:04">Tuesday 17 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
