var documenterSearchIndex = {"docs":
[{"location":"MainModule/#MainModule","page":"Main Module","title":"MainModule","text":"","category":"section"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"This is the main unit containing the PosDefManifoldML module.","category":"page"},{"location":"MainModule/#dependencies","page":"Main Module","title":"dependencies","text":"","category":"section"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"standard Julia packages external packages\nLinearAlgebra PosDefManifold\nStatistics GLMNet\nRandom Distributions\nDates LIBSVM\nStatsBase PermutationTests\nDiagonalizations Folds\nSerialization DataFrames","category":"page"},{"location":"MainModule/#types","page":"Main Module","title":"types","text":"","category":"section"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"The following types are used.","category":"page"},{"location":"MainModule/#MLmodel","page":"Main Module","title":"MLmodel","text":"","category":"section"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"This type is created (a struct in Julia) to hold a ML model. The abstract type for all machine learning models is","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"abstract type MLmodel end","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"The abstract type for all machine learning models acting on the positive definite (PD) manifold (for example, see MDM) is","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"abstract type PDmodel<:MLmodel end","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"The abstract type for all machine learning models acting on the tangent space (for example, see ENLR) is","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"abstract type TSmodel<:MLmodel end","category":"page"},{"location":"MainModule/#Conditioner","page":"Main Module","title":"Conditioner","text":"","category":"section"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"Conditioners (also called pre-conditioners) are data transformations for the manifold of positive-definite matrices. A sequence of conditioners forms a pipeline (see Pipeline). Their role of a transforming pipeline is to improve the classification performance and/or to reduce the computational complexity.","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"A type is created (a struct in Julia) to specify a conditioner.  The abstract type for all conditioners. is","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"abstract type Conditioner end # all SPD matrix transformations","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"These here below are the types for the conditioners currently implemented:","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"    abstract type Tikhonov      <: Conditioner end # Tikhonov regularization\n    abstract type Recentering   <: Conditioner end # Recentering with or w/o dim reduction\n    abstract type Compressing   <: Conditioner end # Compressing (global scaling)\n    abstract type Equalizing    <: Conditioner end # Equalizing (individual scaling)\n    abstract type Shrinking     <: Conditioner end # Geodesic Shrinking","category":"page"},{"location":"MainModule/#IntVector","page":"Main Module","title":"IntVector","text":"","category":"section"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"In all concerned functions, class labels are given as a vector of integers, of type","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"IntVector=Vector{Int}","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"Class labels must be natural numbers in 1z, where z is the number of classes.","category":"page"},{"location":"MainModule/#Tips-and-Tricks","page":"Main Module","title":"Tips & Tricks","text":"","category":"section"},{"location":"MainModule/#working-with-metrics","page":"Main Module","title":"working with metrics","text":"","category":"section"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"In order to work with metrics for the manifold of positive definite matrices, make sure you install the PosDefManifold package.","category":"page"},{"location":"MainModule/#the-â„Vector-type","page":"Main Module","title":"the â„Vector type","text":"","category":"section"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"Check this documentation on typecasting matrices.","category":"page"},{"location":"MainModule/#notation-and-nomenclature","page":"Main Module","title":"notation & nomenclature","text":"","category":"section"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"Throughout the code of this package the following notation is followed:","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"scalars and vectors are denoted using lower-case letters, e.g., y,\nmatrices using upper case letters, e.g., X\nsets (vectors) of matrices using bold upper-case letters, e.g., ð—.","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"The following nomenclature is used consistently:","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"ðTr: a training set of positive definite matrices\nðTe: a testing set of positive definite matrices\nð, ð: a generic set of positive definite matrices.\nw: a weights vector of non-negative real numbers\nyTr: a training set class labels vector of positive integer numbers (1, 2,...)\nyTe: a testing set class labels vector of positive integer numbers\ny: a generic class labels vector of positive integer numbers.\nz: number of classes of a ML model\nk: number of matrices in a set","category":"page"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"In the examples, bold upper-case letters are replaced by upper case letters in order to allow reading in the REPL.","category":"page"},{"location":"MainModule/#acronyms","page":"Main Module","title":"acronyms","text":"","category":"section"},{"location":"MainModule/","page":"Main Module","title":"Main Module","text":"MDM: minimum distance to mean\nENLR: Elastic-Net Logistic Regression\nSVM: Support-Vector Machine\ncv: cross-validation","category":"page"},{"location":"stats_inferential/#stats_inferential.jl","page":"Inferential","title":"stats_inferential.jl","text":"","category":"section"},{"location":"stats_inferential/","page":"Inferential","title":"Inferential","text":"This unit implements inferential statistics (hypothesis tests) for cross-validation analysis.","category":"page"},{"location":"stats_inferential/#Content","page":"Inferential","title":"Content","text":"","category":"section"},{"location":"stats_inferential/","page":"Inferential","title":"Inferential","text":"function description\ntestCV Test the average error loss observed in a cross-validation against an expected (chance) level\ntestCV Test the average error loss observed in two cross-validation obtained by two different models and/or processing pipelines on the same data","category":"page"},{"location":"stats_inferential/","page":"Inferential","title":"Inferential","text":"See also stats_descriptive.jl","category":"page"},{"location":"stats_inferential/#PosDefManifoldML.testCV","page":"Inferential","title":"PosDefManifoldML.testCV","text":"(1) function testCV(cv:CVresult, \n\t\t\texpected::Union{Float64, Symbol} = :randomchance)\n\n(2) function testCV(ð‚::Vector{Vector{Int}}; \n\t\t\texpected::Union{Float64, Symbol} = :randomchance)\n\n\n(1) Given a CVres stucture (which is of type CVresult) obtained calling the crval method,  performs Bayles's test (Bayles et al., 2020ðŸŽ“)  on the difference in distribution of the average binary error losses obtained in the  cross-validation as compared to a given expected average error loss.  This can be used to test whether the performance obtained in the cross-validation is superior to a specified chance level.\n\n(2) As method (1) but giving as first argument a vector of f confusion matrices,  The format of the confusion matrices (Vector{Vector{Int}}) is the same  used to store the confusion matrices in a CVres stucture.\n\nThe test is always directional. Denoting Î¼ the average loss and E the expected average, the test if performed according to null hypethesis H_0 Î¼1 = E against alternative Î¼  E; if the test is significant, the error loss is statistically inferior to the specified expected level, which means that the  observed accuracy is statistically superior to the expected accuracy.\n\nBy default, expected is set to 1-frac1z, where z is the number of classes. You can pass another value, which must be a real number.\n\nReturn the 3-tuple (z, p, ase) holding, the z test-statistic (a standard deviate),  the p-value and the asymptotic standard error.\n\nwarning: Class Inbalance\nThe error lost reflects the accuracy, not the balance accuracy. If the class are inbalanced, the test will be driven by the majority classes. See also the documentation on crval.\n\nExamples:\n\nusing PosDefManifoldML\n\n# Generate a set of random data\nP, _dummyP, y, _dummyy = gen2ClassData(10, 60, 80, 30, 40, 0.1)\n\n# Perform a cross-validation\ncv = crval(MDM(Fisher), P, y; scoring=:a)\n\n# The `z` and `p` values are printed out as they are computed automatically\n# and stored in the created structure. To compute it yourself:\n\nz, p, ase = testCV(cv)\n\n\n\n\n\n\n(1) function testCV(cv1::CVresult, cv2::CVresult; \n               \tdirection = PermutationTests.Both())\n\n(2) function testCV(ðžâ‚::Vector{BitVector}, ðžâ‚‚::Vector{BitVector}; \n               \tdirection = PermutationTests.Both())\n\n\n(1) Given two CVres stuctures cv1 and cv2, which can be obtaoined  calling the crval method, performs Bayles's test (Bayles et al., 2020ðŸŽ“) on the difference in distribution  of the average binary error losses obtained in cross-validations cv1 and cv2.  This can be used to compare the performance obtained by different machine learning models and/or different pre-processing, processing or pre-conditoning pipelines on the same data. \n\nwarning: Be careful\nWhen you compare two models and/or pipelines with cross-validation,  do not use keyword argument shuffle=true in crval, as in this case  the actual data composing the folds would not be identical. Also, is you use the seed keyword argument, make sure the same seed is employed for the two cross-validations.\n\n(2) The function also accepts as argument the error losses directly,  denoted ðžâ‚ and ðžâ‚‚ in method (2) here above.  In this case ðžâ‚ and ðžâ‚‚ are vectors holding the  vectors of error losses obtained at each fold.  The format is the same used to store the error losses in  a CVres stucture.\n\nDenoting Î¼_1 and Î¼_2 the average loss for cv1 and cv2 (or ðžâ‚ and ðžâ‚‚),  respectively, the direction keyword argument determines the test directionality:\n\nH_1 Î¼_1  Î¼_2 (use direction=PermutationTests.Right())\nH_1 Î¼_1  Î¼_2 (use direction=PermutationTests.Left())\nH_1 Î¼_1  Î¼_2 (use direction=PermutationTests.Both(), the default)\n\nIn al cases H_0 Î¼_1 = Î¼_2.\n\ntip: Test direction in terms of accuracy\nNote that if the test is significant with direction=PermutationTests.Right(), the  error loss of cv1(or ðžâ‚) is statistically superior to the error loss of cv2(or ðžâ‚‚), which means that the accuracy in cv1(or ðžâ‚) is statistically inferior to the accuracy  in cv2(or ðžâ‚‚).\n\nReturn the 3-tuple (z, p, ase) holding, the z test-statistic (a standard deviate),  the p-value and the asymptotic standard error.\n\nwarning: Class Inbalance\nThe error lost reflects the accuracy, not the balance accuracy. If the class are inbalanced, the test will be driven by the majority classes. See also the documentation on crval.\n\nExamples:\n\nusing PosDefManifoldML\n\n## Test the performance of the same model and pipeline on different data:\n\n# Generate two sets of random data with the same distribution\nP1, _dummyP, y1, _dummyy = gen2ClassData(10, 60, 80, 30, 40, 0.01)\nP2, _dummyP, y2, _dummyy = gen2ClassData(10, 60, 80, 30, 40, 0.01)\n\n# Perform a cross-validation on the two sets of data\ncv1 = crval(MDM(Fisher), P1, y1; scoring=:a)\ncv2 = crval(MDM(Fisher), P2, y2; scoring=:a)\n\nz, p, ase = testCV(cv1, cv2) # two-sided test\n\n# This is equivalent to\nz, p, ase = testCV(cv1.losses, cv2.losses)\n\n## Test the performance of, for example, different metrics on the same data:\n\n# Generate two sets of random data with the same distribution\nP, _dummyP, y, _dummyy = gen2ClassData(10, 40, 40, 30, 40, 0.15)\n\ncv1 = crval(MDM(logEuclidean), P, y; scoring=:a)\ncv2 = crval(MDM(Wasserstein), P, y; scoring=:a)\n\nz, p, ase = testCV(cv1, cv2) # two-sided test\n\n\n\n\n\n\n","category":"function"},{"location":"contribute/#How-to-Contribute","page":"How to contribute","title":"How to Contribute","text":"","category":"section"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"You can easily contribute a new ML model to PosDefManifoldML.jl package following these steps:","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"Let's say you want to contribute an ML model named ABC.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"If the model act on the manifold of positive definite matrices (PSD), use as template the mdm.jl unit. If it acts on the tangent space, use as template the svm.jl unit (you can also check the enlr.jl unit).","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"Save the template unit as unit abc.jl in the same directory where the template file is.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"Implementing your ABC model entails the following five steps:","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"1) Declare an abstract type for the model","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"if your model acts on the manifold of PSD matrices, this will be","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"abstract type ABCmodel<:PDmodel end","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"if your model act on the tangent space, this will be","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"abstract type ABCmodel<:TSmodel end","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"2) Declare the struct to hold the model and its default creator","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"This will look like:","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"mutable struct ABC <: ABCmodel\n\t\tmetric        :: Metric\n\t\tdefaultkwarg1 :: Type_of_defaultkwarg1\n\t\tdefaultkwarg2 :: Type_of_defaultkwarg2\n\t\t...\n\t\tvoidkwarg1\n\t\tvoidkwarg2\n\t\t...\n\t\tfunction ABC(metric :: Metric=Fisher;\n\t\t\t\tdefaultkwarg1 :: Type_of_defaultkwarg1 = default_value,\n\t\t\t\tdefaultkwarg2 :: Type_of_defaultkwarg2 = default_value,\n\t\t\t\t...\n\t\t\t\tvoidkwarg1 = nothing,\n\t\t\t\tvoidkwarg2 = nothing,\n\t\t\t\t...)\n\t\t\tnew(metric, defaultkwarg1, defaultkwarg1,...,\n\t\t\t\tvoidkwarg1, voidkwarg2,...)\n\t\tend\nend","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"In the above example defaultkwarg are essential parameters that should be set by default upon creation. Use as few as those as needed to obtain a working ML model when the user does not pass any argument.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"The voidkwarg arguments are arguments that you wish be accessible to the user in the structure once the model has been fitted. Include here as few of them as possible.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"3) write the fit function","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"This function will fit the model. Its default behavior should fit the model and tune hyperparameters in order to find the best model by cross-validation if the ABC model has hyperparameters.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"Your fit function declaration will look like:","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"function fit(   model :: ABCmodel,\n\t\t\t    ðTr :: â„Vector,\n\t\t\t\t# if the model acts on the tangent space use:\n\t\t\t\t# ðTr :: Union{â„Vector, Matrix{Float64}}\n\t\t\t\tyTr :: Vector;\n\t\t\t\tverbose :: Bool = true\n\t\t\t\tkwarg1  :: type-of-kwarg1 = default-value,\n\t\t\t\tkwarg2  :: type-of-kwarg2 = default-value,\n\t\t\t\t...)\nend","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"Here you can use as many kwarg arguments as you wish. Currently, all ML models have a verbose argument. Your fit function should starts with:","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"âŒš=now() # time in milliseconds\nâ„³=deepcopy(model) # output model","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"and ends with","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"verbose && println(defaultFont, \"Done in \", now()-âŒš,\".\")\nreturn â„³","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"In between these two blocks you will fit the model and write into â„³ the voidkwarg arguments you have declared in the ABC struct (see above).","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"Keep in mind that if the model acts in the tangent space you will need to project the data therein. For doing so you can use the _getTSvec_fit! internal function (declared in unit tools.jl), as it is done for the ENLR and SVM models. This entails using some standard arguments for tangent space projection, which should be given as options to the user, as done for these models. See how this is done in the fit function of the ENLR and SVM models (unit enlr.jl and svm.jl, respectively).","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"Once you have finished, a call such as","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"m1 = fit(ABC(), PTr, yTr)","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"for some data PTr(a matrix or â„Vector type) and labels yTr (see fit) should fit the model in such a way that it is ready to allow a call to the predict function and return the model.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"4) write the predict function","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"Your predict function declaration will look like:","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"function predict(model   :: ABCmodel,\n\t\t\t\tðTe :: Union{â„Vector, Matrix{Float64}},\n\t\t\t\t# if the model acts on the tangent space use:\n\t\t\t\t# ðTr :: Union{â„Vector, Matrix{Float64}}\n\t\t\t\twhat :: Symbol = :labels;\n\t\t\t\tkwarg1 :: type_of_kwarg1 = default_value\n\t\t\t\tkwarg2 :: type_of_kwarg2 = default_value\n\t\t\t\t...\n\t\t\t\tverbose  :: Bool = true)","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"where in general here you will not need kwarg arguments.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"Your predict function should starts with:","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"âŒš=now() # time in milliseconds","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"and ends with","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"verbose && println(defaultFont, \"Done in \", now()-âŒš,\".\")\nverbose && println(titleFont, \"\\nPredicted \",_what2Str(what),\":\", defaultFont)\nreturn ðŸƒ","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"where in between these two blocks variable ðŸƒ has been filled with the prediction.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"To use this template code, in the declaration of the _what2Str function (declared in unit tools.jl), add a line to allow returning the full name of your model as a string.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"As for the fit function, if the model acts in the tangent space you will need to project the data therein. For doing so you can use here the _getTSvec_Predict! internal function (declared in unit tools.jl), as it is done for the ENLR and SVM models. This entails using some standard arguments for tangent space projection, which should be given as options to the user, as done for these models. Note that _getTSvec_Predict! is similar, but not the same as _getTSvec_fit! function.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"All ML models implemented so far allow three types of prediction, depending on the symbol passed by the user with argument what. See the documentation of the predict function for the ENLR model to see what these three types of predictions are and the code of the function in unit enlr.jl for an example on how to compute them. Note that the returned type of the predict function, the variable ðŸƒ, depends on what is predicted.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"5) Allow the crval function to support your model properly","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"If you have been following these guidelines so far, the crval function (declared in unit cv.jl) will be able to perform k-fold cross-validation on data using your ABC model.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"This function allows the user to pass an arbitrary number of optional keyword arguments, so the user will be able to pass here the kwarg arguments you have declared in your fit function for the ABC model and those will be passed to the fit function when fitting the model at each fold.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"This will be done automatically, however it is necessary to prevent the user from passing here optional keyword arguments that should not be used in a k-fold cross-validation setting (if in your fit function you have declared such arguments).","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"Also, it is necessary here to overwrite into the ABC model that is passed by the user as argument to the crval function the defaultkwarg1 fields of the ABC struct you have declared, if the user can request different values for those fields using optional keyword arguments passed to the crval function. For example, consider the SVM model; suppose the user pass to the crval function a default SVM model. For such a model the kernel is a linear kernel. Suppose the user has passed to the crval function argument kernel=Polynomial. If you don't overwrite the kernel into the model, the linear kernel will be used instead of the desired polynomial kernel.","category":"page"},{"location":"contribute/","page":"How to contribute","title":"How to contribute","text":"You can do so easily using the _rmArgs and _getArgValue internal functions (declared in unit tools.jl), as done for the SVM and ENLR model (see the code of the crval function).","category":"page"},{"location":"stats_descriptive/#stats_descriptive.jl","page":"Descriptive","title":"stats_descriptive.jl","text":"","category":"section"},{"location":"stats_descriptive/","page":"Descriptive","title":"Descriptive","text":"This unit implements descriptive statistics for cross-validation analysis.","category":"page"},{"location":"stats_descriptive/#Content","page":"Descriptive","title":"Content","text":"","category":"section"},{"location":"stats_descriptive/","page":"Descriptive","title":"Descriptive","text":"function description\nconfusionMat Confusion matrix given true and predicted labels\npredictAcc Prediction accuracy given true and predicted labels or a confusion matrix\npredictErr Prediction error given true and predicted labels or a confusion matrix\nbinaryloss Binary error loss","category":"page"},{"location":"stats_descriptive/","page":"Descriptive","title":"Descriptive","text":"See also stats_inferential.jl","category":"page"},{"location":"stats_descriptive/#PosDefManifoldML.confusionMat","page":"Descriptive","title":"PosDefManifoldML.confusionMat","text":"function confusionMat(yTrue::IntVector, yPred::IntVector)\n\nReturn the confusion matrix expressing frequencies (counts), given integer vectors of true label yTrue and predicted labels yPred.\n\nThe length of yTrue and yPred must be equal. Furthermore, the yTrue vector must comprise all natural numbers in between 1 and z, where z is the number of classes.\n\nThe confusion matrix will have size z. It is computed starting from a matrix filled everywhere with zeros and adding, for each label, 1 at entry [i, j] of the matrix, where i is the true label and j the predicted label. Thus, the first row will report the true labels for class 1,  the second row the true labels for class 2, etc.\n\nThe returned matrix is a matrix of integers.\n\nSee predict, predictAcc, predictErr\n\nExamples\n\nusing PosDefManifoldML\nconfusionMat([1, 1, 1, 2, 2], [1, 1, 1, 1, 2])\n# return: [3 0; 1 1]\n\n\n\n\n\n","category":"function"},{"location":"stats_descriptive/#PosDefManifoldML.predictAcc","page":"Descriptive","title":"PosDefManifoldML.predictAcc","text":"(1)\nfunction predictAcc(yTrue::IntVector, yPred::IntVector;\n\t\tscoring\t:: Symbol = :b,\n\t\tdigits\t:: Int=3)\n\n(2)\nfunction predictAcc(CM::Union{Matrix{R}, Matrix{S}};\n\t\tscoring\t:: Symbol = :b,\n\t\tdigits\t:: Int=3) where {R<:Real, S<:Int}\n\nReturn the prediction accuracy as a proportion, that is, âˆˆ0 1, given\n\n(1) the integer vectors of true labels yTrue and of predicted labels yPred, or\n(2) a confusion matrix.\n\nThe confusion matrix may hold integers, in which case it is interpreted as expressing frequencies (counts) or may hold real numbers, in which case it is interpreted as expressing proportions.\n\nIf scoring=:b (default) the balanced accuracy is computed. Any other value will make the function returning the regular accuracy. Balanced accuracy is to be preferred for unbalanced classes. For balanced classes the balanced accuracy reduces to the regular accuracy, therefore there is no point in using regular accuracy if not to avoid a few unnecessary computations when the class are balanced.\n\nThe error is rounded to the number of optional keyword argument digits, 3 by default.\n\nMaths\n\nThe regular accuracy is given by sum of the diagonal elements of the confusion matrix expressing proportions.\n\nFor the balanced accuracy, the diagonal elements of the confusion matrix are divided by the respective row sums and their mean is taken.\n\nSee predict, predictErr, confusionMat\n\nExamples\n\nusing PosDefManifoldML\npredictAcc([1, 1, 1, 2, 2], [1, 1, 1, 1, 2]; scoring=:a)\n# regular accuracy, return: 0.8\npredictAcc([1, 1, 1, 2, 2], [1, 1, 1, 1, 2])\n# balanced accuracy, return: 0.75\n\n\n\n\n\n","category":"function"},{"location":"stats_descriptive/#PosDefManifoldML.predictErr","page":"Descriptive","title":"PosDefManifoldML.predictErr","text":"(1)\nfunction predictErr(yTrue::IntVector, yPred::IntVector;\n\t\tscoring\t:: Symbol = :b,\n\t\tdigits\t:: Int=3)\n(2)\nfunction predictErr(CM::Union{Matrix{R}, Matrix{S}};\n\t\tscoring\t:: Symbol = :b,\n\t\tdigits\t:: Int=3) where {R<:Real, S<:Int}\n\nReturn the complement of the predicted accuracy, that is, 1.0 minus the result of predictAcc, given\n\n(1) the integer vectors of true labels yTrue and of predicted labels yPred, or\n(2) a confusion matrix.\n\nSee predictAcc, confusionMat\n\n\n\n\n\n","category":"function"},{"location":"stats_descriptive/#PosDefManifoldML.binaryloss","page":"Descriptive","title":"PosDefManifoldML.binaryloss","text":"function binaryloss(yTrue::IntVector, yPred::IntVector)\n\nBinary error loss given a vector of true labels yTrue and a vector of predicted labels yPred. These two vectors must have the same size. The error loss is 1 if the corresponding labels are different, zero otherwise. Return a BitVector, that is, a vector of booleans.\n\nSee predict.\n\nExamples\n\nusing PosDefManifoldML, Random\ndummy1, dummy2, yTr, yPr=gen2ClassData(2, 10, 10, 10, 10, 0.1);\nshuffle!(yPr)\n[yTr yPr binaryloss(yTr, yPr)]\n\n\n\n\n\n","category":"function"},{"location":"enlr/#enlr.jl","page":"Elastic-Net Logistic Regression","title":"enlr.jl","text":"","category":"section"},{"location":"enlr/","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"This unit implements the elastic net logistic regression (ENLR) machine learning model on the tangent space for symmetric positive definite (SDP) matrices, i.e., real PD matrices. This model features two hyperparameters: a user-defined alpha hyperparameter, in range 0 1, where Î±=0 allows a pure Ridge LR model and Î±=1 a pure lasso LR model and the lambda (regularization) hyperparameter. When the model is fitted, we can request to find the optimal lambda hyperparameter for the given training data using cross-validation and/or to find the regularization path.","category":"page"},{"location":"enlr/","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"The lasso model (default) has enjoyed popularity in the field of brain-computer interaces due to the winning score obtained in six international data classification competitions.","category":"page"},{"location":"enlr/","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"The ENLR model is implemented using the Julia GLMNet.jl package. See ðŸŽ“ for resources on GLMNet.jl and learn how to use purposefully this model.","category":"page"},{"location":"enlr/","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"The fit, predict and crval functions for the ENLR models are reported in the cv.jl unit, since those are homogeneous across all machine learning models. Here it is reported the ENLRmodel abstract type and the ENLR structure.","category":"page"},{"location":"enlr/#PosDefManifoldML.ENLRmodel","page":"Elastic-Net Logistic Regression","title":"PosDefManifoldML.ENLRmodel","text":"abstract type ENLRmodel<:TSmodel end\n\nAbstract type for Elastic Net Logistic Rgression (ENLR) machine learning models. See MLmodel.\n\n\n\n\n\n","category":"type"},{"location":"enlr/#PosDefManifoldML.ENLR","page":"Elastic-Net Logistic Regression","title":"PosDefManifoldML.ENLR","text":"mutable struct ENLR <: ENLRmodel\n    metric      :: Metric = Fisher;\n    alpha       :: Real = 1.0\n    pipeline    :: Pipeline\n    normalize\t:: Union{Function, Tuple, Nothing}\n    intercept   :: Bool\n    meanISR     :: Union{Hermitian, Nothing, UniformScaling}\n    vecRange    :: UnitRange\n    featDim     :: Int\n    # GLMNet Models\n    path        :: GLMNet.GLMNetPath\n    cvÎ»         :: GLMNet.GLMNetCrossValidation\n    best        :: GLMNet.GLMNetPath\nend\n\nENLR machine learning models are incapsulated in this mutable structure. Fields:\n\n.metric, of type Metric, is the metric that will be adopted to compute the mean used as base-point for tangent space projection. By default the Fisher metric is adopted. See mdm.jl for the available metrics. If the data used to train the model are not positive definite matrices, but Euclidean feature vectors, the .metric field has no use. In order to use metrics you need to install the PosDefManifold package.\n\n.alpha is the hyperparameter in 0 1 trading-off the elestic-net model. Î±=0 requests a pure ridge model and Î±=1 a pure lasso model. By default, Î±=1 is specified (lasso model). This argument is usually passed as parameter to the fit function, defaulting therein to Î±=1 too. See the examples here below.\n\nAll other fields do not correspond to arguments passed upon creation of the model by the default creator. Instead, they are filled later when a model is created by the fit function:\n\nThe field pipeline, of type Pipeline, holds an optional sequence of data pre-conditioners to be applied to the data.  The pipeline is learnt when a ML model is fitted - see fit -  and stored in the model. If the pipeline is fitted, it is  automatically applied to the data at each call of the predict function.    \n\nFor the content of fields normalize, intercept, meanISR and vecRange, please see the documentation of the fit function for the ENLR model.\n\nif the data used to train the model are positive definite matrices, .featDim is the length of the vectorized tangent vectors. This is given by n(n+1)Ã·2 (integer division), where n is the dimension of the original PD matrices on which the model is applied once they are mapped onto the tangent space. If feature vectors are used to train the model, .featDim is the length of these vectors. If for fitting the model you have provided an optional keyword argument vecRange, .featDim will be reduced accordingly.\n\n.path is an instance of the following GLMNetPath structure of the GLMNet.jl package. It holds the regularization path that is created when the fit function is invoked with optional keyword parameter fitType = :path or = :all:\n\nstruct GLMNetPath{F<:Distribution}\n    family::F                        # Binomial()\n    a0::Vector{Float64}              # intercept values for each solution\n    betas::CompressedPredictorMatrix # coefficient values for each solution\n    null_dev::Float64                # Null deviance of the model\n    dev_ratio::Vector{Float64}       # R^2 values for each solution\n    lambda::Vector{Float64}          # lambda values for each solution\n    npasses::Int                     # actual number of passes over the\n                                     # data for all lamda values\nend\n\n.cvÎ» is an instance of the following GLMNetCrossValidation structure of the GLMNet.jl package. It holds information about the cross-validation used for estimating the optimal lambda hyperparameter by the fit function when this is invoked with optional keyword parameter fitType = :best (default) or = :all:\n\nstruct GLMNetCrossValidation\n    path::GLMNetPath            # the cv path\n    nfolds::Int                 # the number of folds for the cv\n    lambda::Vector{Float64}     # lambda values for each solution\n    meanloss::Vector{Float64}   # mean loss for each solution\n    stdloss::Vector{Float64}    # standard deviation of the mean losses\nend\n\n.best is an instance of the GLMNetPath structure (see above). It holds the model with the optimal lambda parameter found by cross-validation that is created by default when the fit function is invoked.\n\nExamples:\n\n# Note: creating models with the default creator is possible,\n# but not useful in general.\n\nusing PosDefManifoldML, PosDefManifold\n\n# Create an empty lasso model\nm = ENLR(Fisher)\n\n# Since the Fisher metric is the default metric,\n# this is equivalent to\nm = ENLR()\n\n# Create an empty ridge model using the logEuclidean metric\nm = ENLR(logEuclidean; alpha=0)\n\n# Empty models can be passed as first argument of the `fit` function\n# to fit a model. For instance, this will fit a ridge model of the same\n# kind of `m` and put the fitted model in `m1`:\nm1 = fit(m, PTr, yTr)\n\n# In general you don't need this machinery for fitting a model,\n# since you can specify a model by creating one on the fly:\nm2 = fit(ENLR(logEuclidean; alpha=0), PTr, yTr)\n\n# which is equivalent to\nm2 = fit(ENLR(logEuclidean), PTr, yTr; alpha=0)\n\n\n\n\n\n\n","category":"type"},{"location":"svm/#svm.jl","page":"Support-Vector Machine","title":"svm.jl","text":"","category":"section"},{"location":"svm/","page":"Support-Vector Machine","title":"Support-Vector Machine","text":"This unit implements several Suport-Vector Machine (SVM) machine learning models on the tangent space for symmetric positive definite (SDP) matrices, i.e., real PD matrices. Several models can be obtained with different combinations of the svmType and the kernel arguments when the model is fitted. Optimal hyperparameters for the given training data are found using cross-validation.","category":"page"},{"location":"svm/","page":"Support-Vector Machine","title":"Support-Vector Machine","text":"All SVM models are implemented using the Julia LIBSVM.jl package. See ðŸŽ“ for resources on the original LIBSVM C library and learn how to use purposefully these models.","category":"page"},{"location":"svm/","page":"Support-Vector Machine","title":"Support-Vector Machine","text":"The fit, predict and crval functions for the SVM models are reported in the cv.jl unit, since those are homogeneous across all machine learning models. Here it is reported the SVMmodel abstract type and the SVM structure.","category":"page"},{"location":"svm/#PosDefManifoldML.SVMmodel","page":"Support-Vector Machine","title":"PosDefManifoldML.SVMmodel","text":"abstract type SVMmodel<:TSmodel end\n\nAbstract type for Support-Vector Machine (SVM) learning models. See MLmodel.\n\n\n\n\n\n","category":"type"},{"location":"svm/#PosDefManifoldML.SVM","page":"Support-Vector Machine","title":"PosDefManifoldML.SVM","text":"mutable struct SVM <: SVMmodel\n\tmetric\t\t:: Metric\n\tsvmType\t\t:: Type\n\tkernel\t\t:: LIBSVM.Kernel.KERNEL\n\tpipeline \t:: Pipeline\n\tnormalize\t:: Union{Function, Tuple, Nothing}\n\tmeanISR\t\t:: Union{HermitianVector, Nothing, UniformScaling}\n\tvecRange\t:: UnitRange\n\tfeatDim\t\t:: Int\n\tsvmModel #store the training model from the SVM library\n\nSVM machine learning models are incapsulated in this mutable structure. Fields:\n\n.metric, of type Metric, is the metric that will be adopted to compute the mean used as base-point for tangent space projection. By default the Fisher metric is adopted. See mdm.jl for the available metrics. If the data used to train the model are not positive definite matrices, but Euclidean feature vectors, the .metric field has no use.  In order to use metrics you need to install the PosDefManifold package.\n\n.svmType, a generic Type of SVM models used in LIBSVM. Available types are:\n\nSVC: C-Support Vector Classification. The fit time complexity is more  than quadratic with the number of observations. The multiclass support is handled  according to a one-vs-one scheme,\nNuSVC: Nu-Support Vector Classification. Similar to SVC but uses a  parameter to control the number of support vectors,\nOneClassSVM: Unsupervised outlier detection. Estimate the support of a high-dimensional distribution,\nEpsilonSVR: Epsilon-Support Vector Regression,\nNuSVR: Nu-Support Vector Regression.\n\nThe default is SVC, unless labels are not provided while fitting the model, in which case it defaults to OneClassSVM.\n\n.kernel, a LIBSVM.Kernel type. Available kernels are declared as constants in the main module. They are:\n\nLinear \t\t(default)\nRadialBasis \nPolynomial\nSigmoid\nPrecomputed (not supported).\n\nAll other fields do not correspond to arguments passed upon creation of the model by the default creator. Instead, they are filled later when a model is created by the fit function:\n\nFor the content of fields .vecRange and .normalize, please see the documentation of the fit function for the ENLR model.\n\nFor the content of the .meanISR, .featDim and .pipeline fields please see the documentation of the ENLR structure.\n\n.svmModel holds the model structure created by LIBSVM when the model is fitted (declared here).\n\nExamples:\n\n# Note: creating models with the default creator is possible,\n# but not useful in general.\n\nusing PosDefManifoldML, PosDefManifold\n\n# Create an empty SVM model\nm = SVM(Fisher)\n\n# Since the Fisher metric is the default metric,\n# this is equivalent to\nm = SVM()\n\n# Create an empty SVM model using the logEuclidean metric\nm = SVM(logEuclidean)\n\n# Generate some data\nPTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.1);\n\n# Empty models can be passed as first argument of the `fit` function\n# to fit a model. For instance, this will fit an SVM model of the same\n# kind of `m` and put the fitted model in `m1`:\nm1 = fit(m, PTr, yTr)\n\n# In general you don't need this machinery for fitting a model,\n# since you can specify a model by creating one on the fly:\nm2 = fit(SVM(logEuclidean), PTr, yTr; kernel=Linear)\n\n# which is equivalent to\nm2 = fit(m, PTr, yTr; kernel=Linear)\n\n# note that, albeit model `m` has been created as an SVM model\n# with the default kernel (RadialBasis),\n# you have passed `m` and overwritten the `kernel` type.\n# You can also overwrite the `svmType`.\n# The metric, instead, cannot be overwritten.\n\n\n\n\n\n","category":"type"},{"location":"tools/#tools.jl","page":"Tools","title":"tools.jl","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"This unit implements tools that are useful for building Riemannian and Euclidean machine learning classifiers. For basic usage of the package  you won't need these functions.","category":"page"},{"location":"tools/#Content","page":"Tools","title":"Content","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"function description\ntsMap project data on a tangent space to apply Euclidean ML models therein\ntsWeights generate weights for tagent space mapping\ngen2ClassData generate 2-class positive definite matrix data for testing Riemannian ML models\nrescale! rescale the rows or columns of a real matrix to be in range [a, b]\ndemean! remove the mean of the rows or columns of a real matrix\nnormalize! normalize the rows or columns of a real matrix\nstandardize! standardize the rows or columns of a real matrix\nsaveas save a whatever object to a file\nload load a whatever object from a file","category":"page"},{"location":"tools/#PosDefManifoldML.tsMap","page":"Tools","title":"PosDefManifoldML.tsMap","text":"function tsMap(\tmetric :: Metric, ð :: â„Vector;\n    w           :: Vector = Float64[],\n    âœ“w         :: Bool = true,\n    â©          :: Bool = true,\n    meanISR     :: Union{â„, Nothing, UniformScaling}  = nothing,\n    meanInit    :: Union{â„, Nothing}  = nothing,\n    tol         :: Real = 0.,\n    transpose   :: Bool = true,\n    vecRange    :: UnitRange = 1:size(ð[1], 1))\n\nThe tangent space mapping of positive definite matrices P_i, i=1...k with mean G, once those points have been parallel transported to the identity matrix, is given by:\n\nS_i=textrmlog(G^-12 P_i G^-12).\n\nGiven a vector of k matrices ð flagged by julia as Hermitian, return a matrix X with such tangent vectors of the matrices in ð vectorized as per the vecP operation.\n\nThe mean G of the matrices in ð is found according to the specified metric, of type Metric. A natural choice is the Fisher metric. If the metric is Fisher, logdet0 or Wasserstein the mean is found with an iterative algorithm with tolerance given by optional keyword argument tol. By default tol is set by the function mean. For those iterative algorithms a particular initialization can be provided as an Hermitian matrix by optional keyword argument meanInit.\n\nA set of k optional non-negative weights w can be provided for computing a weighted mean G, for any metrics. If w is non-empty and optional keyword argument âœ“w is true (default), the weights are normalized so as to sum up to 1, otherwise they are used as they are passed and should be already normalized. This option is provided to allow calling this function repeatedly without normalizing the same weights vector each time.\n\nIf an Hermitian matrix is provided as optional keyword argument meanISR, then the mean G is not computed, intead this matrix is used directly in the formula as the inverse square root (ISR) G^-12. If meanISR is provided, arguments tol and meanInit have no effect whatsoever.\n\nIf meanISR=I is used, then the tangent space mapping is obtained  at the identity matrix as\n\nS_i=textrmlog(P_i).\n\nThis corresponds to the tangent space mapping adopting the log-Euclidean metric. It is also useful when the data has been already recentered, for example by means of a Recenter pre-conditioner. If meanISR=I is used, arguments  w, âœ“w, meanInit, and tol are ignored.\n\nIf meanISR is not provided, return the 2-tuple (X G^-12), otherwise return only matrix X.\n\nIf an UnitRange is provided with the optional keyword argument vecRange, the vectorization concerns only the columns (or rows) of the matrices ð specified by the range.\n\nIf optional keyword argument transpose is true (default), X holds the k vectorized tangent vectors in its rows, otherwise they are arranged in its columns. The dimension of the rows in the former case and of the columns is the latter case is n(n+1)Ã·2 (integer division), where n is the size of the matrices in ð, unless a vecRange spanning a subset of the columns or rows of the matrices in ð has been provided, in which case the dimension will be smaller. (see vecP ).\n\nif optional keyword argument â© if true (default), the computation of the mean and the projection on the tangent space are multi-threaded. Multi-threading is automatically disabled if the number of threads Julia is instructed to use is <2 or <2k.\n\nExamples:\n\nusing PosDefManifoldML\n\n# generate four random symmetric positive definite 3x3 matrices\nPset = randP(3, 4)\n\n# project and vectorize in the tangent space\nX, Gâ»Â½ = tsMap(Fisher, Pset)\n\n# X is a 4x6 matrix, where 6 is the size of the\n# vectorized tangent vectors (n=3, n*(n+1)/2=6)\n\n# If repeated calls have to be done, faster computations are obtained\n# providing the inverse square root of the matrices in Pset, e.g.,\nX1 = tsMap(Fisher, â„Vector(Pset[1:2]); meanISR = Gâ»Â½)\nX2 = tsMap(Fisher, â„Vector(Pset[3:4]); meanISR = Gâ»Â½)\n\nSee: the â„Vector type.\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.tsWeights","page":"Tools","title":"PosDefManifoldML.tsWeights","text":"function tsWeights(y::Vector{Int}; classWeights=[])\n\nGiven an IntVector of labels y, return a vector of weights summing up to 1 such that the overall weight is the same for all classes (balancing). This is useful for machine learning models in the tangent space with unbalanced classes for computing the mean, that is, the base point to map PD matrices onto the tangent space. For this mapping, giving equal weights to all observations actually overweights the larger classes and downweight the smaller classes.\n\nClass labels for n classes must be the first n natural numbers, that is, 1 for class 1, 2 for class 2, etc. The labels in y can be provided in any order.\n\nif a vector of n weights is specified as optional keyword argument classWeights, the overall weights for each class will be first balanced (see here above), then weighted by the classWeights. This allow user-defined control of weighting independently from the number of observations in each class. The weights in classWeights can be any integer or real non-negative numbers. The returned weight vector will nonetheless sum up to 1.\n\nWhen you invoke the fit function for tangent space models you don't actually need this function, as you can invoke it implicitly passing symbol :balanced (or just :b) or a tuple with the class weights as optional keyword argument w.\n\nExamples\n\n# generate some data; the classes are unbalanced\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1)\n\n# Fit an ENLR lasso model and find the best model by cross-validation\n# balancing the weights for tangent space mapping\nm=fit(ENLR(), PTr, yTr; w=tsWeights(yTr))\n\n# A simpler syntax is\nm=fit(ENLR(), PTr, yTr; w=:balanced)\n\n# to balance the weights and then give overall weight 0.5 to class 1\n# and 1.5 to class 2:\nm=fit(ENLR(), PTr, yTr; w=(0.5, 1.5))\n\n# which is equivalent to\nm=fit(ENLR(), PTr, yTr; w=tsWeights(yTr; classWeights=(0.5, 1.5)))\n\n\nThis is how it works:\n\nusing PosDefManifoldML\n\n# Suppose these are the labels:\n\ny=[1, 1, 1, 1, 2, 2]\n\n# We want the four observations of class 1 to count as much\n# as the two observations of class 2.\n\ntsWeights(y)\n\n# 6-element Array{Float64,1}:\n# 0.125\n# 0.125\n# 0.125\n# 0.125\n# 0.25\n# 0.25\n\n# i.e., 0.125*4 = 1.25*2\n# and all weights sum up to 1\n\n# Now, suppose we want to give to class 2 a weight\n# four times bigger as compared to class 1:\n\ntsWeights(y, classWeights=[1, 4])\n\n# 6-element Array{Float64,1}:\n# 0.05\n# 0.05\n# 0.05\n# 0.05\n# 0.4\n# 0.4\n\n# and, again, all weights sum up to 1.\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.gen2ClassData","page":"Tools","title":"PosDefManifoldML.gen2ClassData","text":"function gen2ClassData(n        ::  Int,\n                       k1train  ::  Int,\n                       k2train  ::  Int,\n                       k1test   ::  Int,\n                       k2test   ::  Int,\n                       separation :: Real = 0.1)\n\nGenerate for two classes (1 and 2) a random training set holding k1train+k2train and a random test set holding k1test+k2test symmetric positive definite matrices. All matrices have size nxn.\n\nThe training and test sets can be used to train and test any MLmodel.\n\nseparation is a coefficient determining how well the two classs are separable; the higher it is, the more separable the two classes are. It must be in 0 1 and typically a value of 0.5 already determines complete separation.\n\nReturn a 4-tuple with\n\nan â„Vector holding the k1train+k2train matrices in the training set,\nan â„Vector holding the k1test+k2test matrices in the test set,\na vector holding the k1train+k2train labels (integers) corresponding to the matrices of the training set,\na vector holding the k1test+k2test labels corresponding to the matrices of the test set (1 for class 1 and 2 for class 2).\n\nExamples\n\nusing PosDefManifoldML\n\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.25)\n\n# PTr=training set: 30 matrices for class 1 and 40 matrices for class 2\n# PTe=testing set: 60 matrices for class 1 and 80 matrices for class 2\n# all matrices are 10x10\n# yTr=a vector of 70 labels for the training set\n# yTe=a vector of 140 labels for the testing set\n\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.rescale!","page":"Tools","title":"PosDefManifoldML.rescale!","text":"function rescale!(X::Matrix{T},\tbounds::Tuple=(-1, 1);\n\t\tdims::Int=1) where T<:Real\n\nRescale the columns or the rows of real matrix X to be in range [a, b], where a and b are the first and seconf elements of tuple bounds.\n\nBy default it applies to the columns. Use dims=2 for rescaling the rows.\n\nThis function is used for normalizing tangent space (feature) vectors. Typically, you won't need it. When fitting a model with fit  or performing a cross-validation with crval, you can simply pass bounds to the argument rescale of these functions.\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.demean!","page":"Tools","title":"PosDefManifoldML.demean!","text":"function demean!(X::Matrix{T}; dims::Int=1) where T<:Real \n\nRemove the mean from the columns or from the rows of real matrix X.\n\nBy default it applies to the columns. Use dims=2 for demeaning the rows.\n\nThis function is used for normalizing tangent space (feature) vectors. Typically, you won't need it. When fitting a model with fit  or performing a cross-validation with crval, you can simply pass this function as the normalize argument.\n\n\n\n\n\n","category":"function"},{"location":"tools/#LinearAlgebra.normalize!","page":"Tools","title":"LinearAlgebra.normalize!","text":"function normalize!(X::Matrix{T}; dims::Int=1) where T<:Real \n\nNormalize the columns or the rows of real matrix X so that their 2-norm divided by their number of elements is 1.0. This way the value of each element of matrix X gravitates  around 1.0, regardless its size.\n\nBy default it applies to the columns. Use dims=2 for normlizing the rows.\n\nThis function is used for normalizing tangent space (feature) vectors. Typically, you won't need it. When fitting a model with fit  or performing a cross-validation with crval, you can simply pass this function as the normalize argument.\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.standardize!","page":"Tools","title":"PosDefManifoldML.standardize!","text":"function standardize!(X::Matrix{T}; dims::Int=1) where T<:Real \n\nStandardize the columns or the rows of real matrix X so that they have zero mean and unit (uncorrected) standard deviation.\n\nBy default it applies to the columns. Use dims=2 for standardizing the rows.\n\nThis function is used for normalizing tangent space (feature) vectors. Typically, you won't need it. When fitting a model with fit  or performing a cross-validation with crval, you can simply pass this function as the normalize argument.\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.saveas","page":"Tools","title":"PosDefManifoldML.saveas","text":"    function saveas(object, filename::String)\n\nSave the object to a file, which full path is given as filemane. It can be used, for instance, to save CVres structures and Pipeline tuples.\n\nSee: load\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold, Serialization\n\n## Save and then load a cross-validation structure\n\nP, _dummyP, y, _dummyy = gen2ClassData(10, 40, 40, 30, 40, 0.15)\n\ncv = crval(MDM(logEuclidean), P, y; scoring=:a)\n\nfilename=joinpath(@__DIR__, \"mycv.jls\")\nsaveas(cv, filename)\n\nmycv = load(filename)\n\n# retrive the p-value of the cross-validation\nmycv.p\n\n## Save and then load a pipeline\n\n# Generate some 'training' and `test` data\nPTr=randP(3, 20) # 20 random 3x3 Hermitian matrices\nPTe=randP(3, 5) # 5 random 3x3 Hermitian matrices\n\n# fit a pipeline and transform the training data\np = fit!(PTr, @pipeline Recenter(; eVar=0.99) Compress)\n\n# save the fitted pipeline\nfilename=joinpath(@__DIR__, \"pipeline.jls\")\nsaveas(p, filename) \n\nmypipeline = load(filename)\n\n# transform the testing data using the loaded pipeline\ntransform!(PTe, mypipeline)\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.load","page":"Tools","title":"PosDefManifoldML.load","text":"    function load(filename::String)\n\nLoad and retur an object stored to a file,  which full path is given as filemane.\n\nIt can be used, for instance, to load CVres structures and Pipeline tuples.\n\nFor pipelines, there is no check that it matches the dimension of the data  to which it will be applied. This is the user's responsibility.\n\nSee saveas\n\n\n\n\n\n","category":"function"},{"location":"mdm/#mdm.jl","page":"Minimum Distance to Mean","title":"mdm.jl","text":"","category":"section"},{"location":"mdm/","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"This unit implements the Riemannian MDM (Minimum Distance to Mean) classifier for the manifold of positive definite (PD) matrices, both real (symmetric PD) or complex (Hermitian PD) matrices. The MDM is a simple, yet efficient, deterministic and paramater-free classifier acting directly on the manifold of positive definite matrices (Barachat el al., 2012; Congedo et al., 2017a ðŸŽ“): given a number of PD matrices representing class means, the MDM classify an unknown datum (also a PD matrix) as belonging to the class whose mean is the closest to the datum. The process is illustrated in the upper part of this figure.","category":"page"},{"location":"mdm/","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"The MDM classifier involves only the concepts of a distance function for two PD matrices and a mean (center of mass or barycenter) for a number of them. Those are defined for any given metric, a Metric enumerated type declared in PosDefManifold.","category":"page"},{"location":"mdm/","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"Currently supported metrics are:","category":"page"},{"location":"mdm/","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"metric (distance) mean estimation known also as\nEuclidean arithmetic Arithmetic\ninvEuclidean harmonic \nChoEuclidean Cholesky Euclidean \nlogEuclidean log-Euclidean \nlogCholesky log-Cholesky \nFisher Fisher Cartan, Karcher, Pusz-Woronowicz, Affine-Invariant, ...\nlogdet0 logDet S, Î±, Bhattacharyya, Jensen, ...\nJeffrey Jeffrey symmetrized Kullback-Leibler\nWasserstein Wasserstein Bures, Hellinger, optimal transport, ...","category":"page"},{"location":"mdm/","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"Do not use the Von Neumann metric, which is also supported in PosDefManifold, since it does not allow a definition of mean. See here for details on the metrics. In order to use these metrics you need to install the PosDefManifold package.","category":"page"},{"location":"mdm/","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"The fit, predict and crval functions for the MDM model are reported in the cv.jl unit, since those are homogeneous across all machine learning models. Here it is reported the MDMmodel abstract type, the MDM structure and the following functions, which typically you will not need to access directly, but are nonetheless provided to facilitate low-level operations with MDM classifiers:","category":"page"},{"location":"mdm/","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"function description\nbarycenter compute the barycenter of positive definite matrices for fitting the MDM model\ndistances compute the distances of a matrix set to a set of means","category":"page"},{"location":"mdm/#PosDefManifoldML.MDMmodel","page":"Minimum Distance to Mean","title":"PosDefManifoldML.MDMmodel","text":"Abstract type for MDM (Minimum Distance to Mean) machine learning models\n\n\n\n\n\n","category":"type"},{"location":"mdm/#PosDefManifoldML.MDM","page":"Minimum Distance to Mean","title":"PosDefManifoldML.MDM","text":"mutable struct MDM <: MDMmodel\n    metric  :: Metric = Fisher;\n    pipeline :: Pipeline\n    featDim :: Int\n    means   :: â„Vector\n    imeans  :: â„Vector\nend\n\nMDM machine learning models are incapsulated in this mutable structure. MDM models have four fields: .metric, .pipeline, .featDim and .means.\n\nThe field metric, of type Metric, is to be specified by the user. It is the metric that will be adopted to compute the class means and the distances to the mean.\n\nAll other fields do not correspond to arguments passed upon creation of the model by the default creator. Instead, they are filled later when a model is created by the fit function:\n\nThe field pipeline, of type Pipeline, holds an optional sequence of data pre-conditioners to be applied to the data.  The pipeline is learnt when a ML model is fitted - see fit -  and stored in the model. If the pipeline is fitted, it is  automatically applied to the data at each call of the predict function.\n\nThe field featDim is the dimension of the manifold in which the model acts. This is given by n(n+1)/2, where n is the dimension of the PD matrices. This field is not to be specified by the user, instead, it is computed when the MDM model is fit using the fit function and is accessible only thereafter.\n\nThe field means is an â„Vector holding the class means, i.e., one mean for each class. This field is not to be specified by the user, instead, the means are computed when the MDM model is fitted using the fit function and are accessible only thereafter.\n\nThe field imeans is an â„Vector holding the inverse of the matrices in means. This also is not to be specified by the user, is computed when the model is fitted and is accessible only thereafter. It is used to optimize the computation of distances if the model is fitted using the Fisher metric (default).\n\nExamples:\n\nusing PosDefManifoldML, PosDefManifold\n\n# Create an empty model\nm = MDM(Fisher)\n\n# Since the Fisher metric is the default metric,\n# this is equivalent to\nm = MDM()\n\nNote that in general you need to invoke these constructors only when an empty MDM model is needed as an argument to a function, otherwise you can more simply create and fit an MDM model using the fit function.\n\n\n\n\n\n","category":"type"},{"location":"mdm/#PosDefManifoldML.barycenter","page":"Minimum Distance to Mean","title":"PosDefManifoldML.barycenter","text":"function barycenter(metric :: Metric, ð:: â„Vector;\n              w        :: Vector = [],\n              âœ“w       :: Bool    = true,\n              meanInit :: Union{â„, Nothing} = nothing,\n              tol      :: Real   = 0.,\n              â©      :: Bool    = true)\n\nTypically, you will not need this function as it is called by the fit function.\n\nGiven a metric of type Metric, an â„Vector of Hermitian matrices ð and an optional non-negative real weights vector w, return the (weighted) mean of the matrices in ð. This is used to fit MDM models.\n\nThis function calls the appropriate mean functions of package PostDefManifold, depending on the chosen metric, and check that, if the mean is found by an iterative algorithm, then the iterative algorithm converges.\n\nSee method (3) of the mean function for the meaning of the optional keyword arguments w, âœ“w, meanInit, tol and â©, to which they are passed.\n\nThe returned mean is flagged by Julia as an Hermitian matrix (see LinearAlgebra).\n\n\n\n\n\n","category":"function"},{"location":"mdm/#PosDefManifoldML.distances","page":"Minimum Distance to Mean","title":"PosDefManifoldML.distances","text":"function distances(metric :: Metric,\n                      means  :: â„Vector,\n                      ð      :: â„Vector;\n                imeans  :: Union{â„Vector, Nothing} = nothing,\n                scale   :: Bool = false,\n                â©      :: Bool = true)\n\nTypically, you will not need this function as it is called by the predict function.\n\nGiven an â„Vector ð holding k Hermitian matrices and an â„Vector means holding z matrix means, return the square of the distance of each matrix in ð to the means in means.\n\nThe squared distance is computed according to the chosen metric, of type Metric. See metrics for details on the supported distance functions.\n\nThe computation of distances is optimized for the Fisher metric if an â„Vector holding the inverse of the means in means is passed as optional keyword argument imeans. For other metrics this argument is ignored.\n\nIf scale is true (default), the distances are divided by the size of the matrices in ð. This can be useful to compare distances computed on manifolds with different dimensions. It has no effect here, but is used as it is good practice.\n\nIf â© is true, the distances are computed using multi-threading, unless the number of threads Julia is instructed to use is <2 or <3k.\n\nThe result is a zÃ—k matrix of squared distances.\n\n\n\n\n\n","category":"function"},{"location":"cv/#cv.jl","page":"Fit, Predict, CV","title":"cv.jl","text":"","category":"section"},{"location":"cv/","page":"Fit, Predict, CV","title":"Fit, Predict, CV","text":"This unit implements cross-validation procedures for estimating the accuracy and balanced accuracy of machine learning models. It also reports the documentation of the fit and predict functions, as they are common to all models.","category":"page"},{"location":"cv/","page":"Fit, Predict, CV","title":"Fit, Predict, CV","text":"Content","category":"page"},{"location":"cv/","page":"Fit, Predict, CV","title":"Fit, Predict, CV","text":"struct description\nCVres encapsulate the results of cross-validation procedures for estimating accuracy","category":"page"},{"location":"cv/","page":"Fit, Predict, CV","title":"Fit, Predict, CV","text":"function description\nfit fit a machine learning model with training data\npredict given a fitted model, predict labels, probabilities or scoring functions on test data\ncrval perform a cross-validation and store accuracies, error losses, confusion matrices, the results of a statistical test and other informations\ncvSetup generate indexes for performing cross-validtions","category":"page"},{"location":"cv/#PosDefManifoldML.CVres","page":"Fit, Predict, CV","title":"PosDefManifoldML.CVres","text":"struct CVres <: CVresult\n    cvType      :: String\n    scoring     :: Union{String, Nothing}\n    modelType   :: Union{String, Nothing}\n    predLabels  :: Union{Vector{Vector{Vector{I}}}, Nothing} where I<:Int\n    losses      :: Union{Vector{BitVector}, Nothing}\n    cnfs        :: Union{Vector{Matrix{I}}, Nothing} where I<:Int\n    avgCnf      :: Union{Matrix{T}, Nothing} where T<:Real\n    accs        :: Union{Vector{T}, Nothing} where T<:Real\n    avgAcc      :: Union{Real, Nothing}\n    stdAcc      :: Union{Real, Nothing}\n    z           :: Union{Real, Nothing}\n    p           :: Union{Real, Nothing}\n    ms          :: Union{Int64, Nothing}\nend\n\nA call to crval results in an instance of this structure.\n\nFields:\n\n.cvTpe is the type of cross-validation technique, given as a string (e.g., \"10-fold\").\n\n.scoring is the type of accuracy that is computed, given as a string. This is controlled when calling crval. Currently, accuracy and balanced accuracy are supported.\n\n.modelType is the type of the machine learning model used for performing the cross-validation, given as a string.\n\n.nTrials is the total number of trials entering the cross-validation.\n\n.matSize is the size of the input matrices (trials).\n\n.predLabels is an f-vector of z integer vectors holding the vectors of  predicted labels. There is one vector for each fold (f) and each containes as many vector as classes (z), in turn each one containing the predicted labels for the trials.\n\n.losses is an f-vector holding BitVector types (vectors of booleans),  each holding the binary loss for a fold. \n\n.cnfs is an f-vector of matrices holding the confusion matrices obtained at each fold of the cross-validation. These matrices holds  frequencies (counts), that is, the sum of all elements equals the number of trials used for each fold.\n\n.avgCnf is the average confusion matrix of proportions  across the folds of the cross-validation. This matrix holds proportions, that is, the sum of all elements equal 1.0.\n\n.accs is an f-vector of real numbers holding the accuracies obtained at each fold of the cross-validation.\n\n.avgAcc is the average accuracy across the folds of the cross-validation.\n\n.stdAcc is the standard deviation of the accuracy across the folds of the cross-validation.\n\n.z is the test-statistic fot the hypothesis that the observed average error loss is  inferior to the specified expected value.\n\n.p is the p-value of the above hypothesis test.\n\n.ms is the execution time in milliseconds, excluding the time to compute the confusion matrix and p-value. If function crval has not been compiled yet, the time includes the compilation time. In any case the provided estimate is subjected to high variability. To get a better estimate use the median or minimum  across several runs or call the function using the BenchmarkTools.jl package.\n\nSee crval for more informations.\n\n\n\n\n\n","category":"type"},{"location":"cv/#StatsAPI.fit","page":"Fit, Predict, CV","title":"StatsAPI.fit","text":"function fit(model :: MDMmodel,\n              ðTr   :: â„Vector,\n              yTr   :: IntVector;\n        pipeline :: Union{Pipeline, Nothing} = nothing,\n        w        :: Vector = [],\n        âœ“w       :: Bool  = true,\n        meanInit :: Union{â„Vector, Nothing} = nothing,\n        tol      :: Real  = 1e-5,\n        verbose  :: Bool  = true,\n        â©       :: Bool  = true)\n\nFit an MDM machine learning model, with training data ðTr, of type â„Vector, and corresponding labels yTr, of type IntVector. Return the fitted model.\n\nwarning: Class Labels\nLabels must be provided using the natural numbers, i.e., 1 for the first class, 2 for the second class, etc.\n\nFitting an MDM model involves only computing a mean (barycenter) of all the matrices in each class. Those class means are computed according to the metric specified by the MDM constructor.\n\nOptional keyword arguments: \n\nIf a pipeline, of type Pipeline is provided,  all necessary parameters of the sequence of conditioners are fitted  and all input matrices ðTr are transformed according to the specified pipeline  before fitting the ML model. The parameters are stored in the output ML model. Note that the fitted pipeline is automatically applied by any successive call  to function predict to which the output ML model is passed as argument. Note that the input matrices ðTr are transformed; pass a copy of ðTr if you wish to mantain the original matrices.\n\nw is a vector of non-negative weights associated with the matrices in ðTr. This weights are used to compute the mean for each class. See method (3) of the mean function for the meaning of the arguments w, âœ“w and â©, to which they are passed. Keep in mind that here the weights should sum up to 1 separatedly for each class, which is what is ensured by this function if âœ“w is true.\n\ntol is the tolerance required for those algorithms that compute the mean iteratively (they are those adopting the Fisher, logdet0 or Wasserstein metric). It defaults to 1e-5. For details on this argument see the functions that are called for computing the means (from package PosDefManifold.jl):\n\nFisher metric: gmean\nlogdet0 metric: ld0mean\nWasserstein metric: Wasmean.\n\nFor those algorithm an initialization can be provided with optional keyword argument meanInit. If provided, this must be a vector of Hermitian matrices of the â„Vector type and must contain as many initializations as classes, in the natural order corresponding to the class labels (see above).\n\nIf verbose is true (default), information is printed in the REPL.\n\nSee: notation & nomenclature, the â„Vector type\n\nSee also: predict, crval\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# Generate some data\nPTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.25)\n\n# Create and fit a model:\nm = fit(MDM(Fisher), PTr, yTr)\n\n# Create and fit a model using a pre-conditioning pipeline:\np = @â†’ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)\nm = fit(MDM(Fisher), PTr, yTr; pipeline=p)\n\n\n\n\n\nfunction fit(model\t:: ENLRmodel,\n             ðTr\t :: Union{HermitianVector, Matrix{Float64}},\n             yTr\t:: IntVector;\n\n    # pipeline (data transformations)\n    pipeline    :: Union{Pipeline, Nothing} = nothing,\n\n    # parameters for projection onto the tangent space\n    w           :: Union{Symbol, Tuple, Vector} = Float64[],\n    meanISR     :: Union{Hermitian, Nothing, UniformScaling} = nothing,\n    meanInit    :: Union{Hermitian, Nothing} = nothing,\n    vecRange    :: UnitRange = ðTr isa â„Vector ? (1:size(ðTr[1], 2)) : (1:size(ðTr, 2)),\n    normalize\t:: Union{Function, Tuple, Nothing} = normalize!,\n\n    # arguments for `GLMNet.glmnet` function\n    alpha           :: Real = model.alpha,\n    weights         :: Vector{Float64} = ones(Float64, length(yTr)),\n    intercept       :: Bool = true,\n    fitType         :: Symbol = :best,\n    penalty_factor  :: Vector{Float64} = ones(Float64, _getDim(ðTr, vecRange)),\n    constraints     :: Matrix{Float64} = [x for x in (-Inf, Inf), y in 1:_getDim(ðTr, vecRange)],\n    offsets         :: Union{Vector{Float64}, Nothing} = nothing,\n    dfmax           :: Int = _getDim(ðTr, vecRange),\n    pmax            :: Int = min(dfmax*2+20, _getDim(ðTr, vecRange)),\n    nlambda         :: Int = 100,\n    lambda_min_ratio:: Real = (length(yTr)*2 < _getDim(ðTr, vecRange) ? 1e-2 : 1e-4),\n    lambda          :: Vector{Float64} = Float64[],\n    maxit           :: Int = 1000000,\n    algorithm       :: Symbol = :newtonraphson,\n    checkArgs       :: Bool = true,\n\n    # selection method\n    Î»SelMeth        :: Symbol = :sd1,\n\n    # arguments for `GLMNet.glmnetcv` function\n    nfolds          :: Int = min(10, div(size(yTr, 1), 3)),\n    folds           :: Vector{Int} =\n    begin\n        n, r = divrem(size(yTr, 1), nfolds)\n        shuffle!([repeat(1:nfolds, outer=n); 1:r])\n    end,\n    parallel        :: Bool=true,\n\n    # Generic and common parameters\n    tol             :: Real = 1e-5,\n    verbose         :: Bool = true,\n    â©              :: Bool = true,\n)\n\nCreate and fit an 2-class elastic net logistic regression (ENLR) machine learning model, with training data ðTr, of type â„Vector, and corresponding labels yTr, of type IntVector. Return the fitted model(s) as an instance of the ENLR structure.\n\nwarning: Class Labels\nLabels must be provided using the natural numbers, i.e., 1 for the first class, 2 for the second class, etc.\n\nAs for all ML models acting in the tangent space, fitting an ENLR model involves computing a mean (barycenter) of all the matrices in ðTr, projecting all matrices onto the tangent space after parallel transporting them at the identity matrix and vectorizing them using the vecP operation. Once this is done, the ENLR is fitted.\n\nThe mean is computed according to the .metric field of the model, with optional weights w. The .metric field of the model is passed internally to the tsMap function. By default the metric is the Fisher metric. See the examples here below to see how to change metric. See mdm.jl or check out directly the documentation of PosDefManifold.jl for the available metrics.\n\nOptional keyword arguments\n\nIf a pipeline, of type Pipeline is provided,  all necessary parameters of the sequence of conditioners are fitted  and all input matrices ðTr are transformed according to the specified pipeline  before fitting the ML model. The parameters are stored in the output ML model. Note that the fitted pipeline is automatically applied by any successive call  to function predict to which the output ML model is passed as argument. Note that the input matrices ðTr are transformed; pass a copy of ðTr if you wish to mantain the original matrices.\n\nBy default, uniform weights will be given to all observations for computing the mean to project the data in the tangent space. This is equivalent to passing as argument w=:uniform (or w=:u). You can also pass as argument:\n\nw=:balanced (or simply w=:b). If the two classes are unbalanced, the weights should better be inversely proportional to the number of examples for each class, in such a way that each class contributes equally to the computation of the mean. This is equivalent of passing w=tsWeights(yTr). See the tsWeights function for details.\nw=v, where v is a user defined vector of non-negative weights for the observations, thus, v must contain the same number of elements as yTr. For example, w=[1.0, 1.0, 2.0, 2.0, ...., 1.0]\nw=t, where t is a 2-tuple of real weights, one weight for each class, for example w=(0.5, 1.5). This is equivalent to passing w=tsWeights(yTr; classWeights=collect(0.5, 1.5)), see the tsWeights function for details.\n\nBy default meanISR=nothing and the inverse square root (ISR) of the mean used for projecting the matrices onto the tangent space (see tsMap) is computed. An Hermitian matrix or I (the identity matrix) can also be passed  as argument meanISR and in this case this matrix will be used as the ISR of the mean. Passed or computed, it will be written in the .meanISR field of the  model structure created by this function. Notice that passing I, the matrices will be projected onto the tangent space at the identity without recentering them. This is possible if the matrices have been recentered by a pre-conditioning pipeline (see Pipeline).\n\nIf meanISR is not provided and the .metric field of the model is Fisher, logdet0 or Wasserstein, the tolerance of the iterative algorithm used to compute the mean is set to argument tol (default 1e-5). Also, in this case a particular initialization for those iterative algorithms can be provided as an Hermitian matrix with argument meanInit.\n\ntip: Euclidean ENLR models\nML models acting on the tangent space allows to fit a model passing as training data ðTr directly a matrix of feature vectors, where each feature vector is a row of the matrix. In this case none of the above keyword arguments are used.  \n\nThe following optional keyword arguments act on any kind of input, that is, tangent vectors and generic feature vectors\n\nIf a UnitRange is passed with optional keyword argument vecRange, then if ðTr is a vector of Hermitian matrices, the vectorization of those matrices once they are projected onto the tangent space concerns only the rows (or columns) given in the specified range, else if ðTr is a matrix with feature vectors arranged in its rows, then only the columns of ðTr given in the specified range will be used. Argument vecRange will be ignored if a pre-conditioning pipeline is used  and if the pipeline changes the dimension of the input matrices. In this case it will be set to its default value using the new dimension. You are not allowed to change this behavior.\n\nWith normalize the tangent (or feature) vectors can be normalized individually. Three functions can be passed, namely \n\ndemean! to remove the mean,\nnormalize! to fix the norm (default),\nstandardize! to fix the mean to zero and the standard deviation to 1.\n\nAs argument normalize you can also pass a 2-tuple of real numbers. In this case the numbers will be the lower and upper limit to bound the vectors within these limits - see rescale!.\n\ntip: Rescaling\nIf you wish to rescale, use (-1, 1), since tangent vectors of SPD matrices have positive and negative elements. If ðTr is a feature matrix and the features are only positive, use (0, 1) instead. \n\nIf you pass nothing as argument normalize, no normalization will be carried out.\n\nThe remaining optional keyword arguments, are\n\nthe arguments passed to the GLMNet.glmnet function for fitting the models. Those are always used.\nthe Î»SelMeth argument and the arguments passed to the GLMNet.glmnetcv function for finding the best lambda hyperparamater by cross-validation. Those are used only if fitType = :path or = :all.\n\nOptional keyword arguments for fitting the model(s) using GLMNet.jl\n\nalpha: the hyperparameter in 0 1 to trade-off an elestic-net model. Î±=0 requests a pure ridge model and Î±=1 a pure lasso model. This defaults to 1.0, which specifies a lasso model, unless the input ENLR model has another value in the alpha field, in which case this value is used. If argument alpha is passed here, it will overwrite the alpha field of the input model.\n\nweights: a vector of weights for each matrix (or feature vectors) of the same size as yTr. It defaults to 1 for all matrices.\n\nintercept: whether to fit an intercept term. The intercept is always unpenalized. Defaults to true.\n\nIf fitType = :best (default), a cross-validation procedure is run to find the best lambda hyperparameter for the given training data. This finds a single model that is written into the .best field of the ENLR structure that will be created.\n\nIf fitType = :path, the regularization path for several values of the lambda hyperparameter is found for the given training data. This creates several models, which are written into the .path field of the ENLR structure that will be created, none of which is optimal, in the cross-validation sense, for the given training data.\n\nIf fitType = :all, both the above fits are performed and all fields of the ENLR structure that will be created will be filled in.\n\npenalty_factor: a vector of length n(n+1)/2, where n is the dimension of the original PD matrices on which the model is applied, of penalties for each predictor in the tangent vectors. This defaults to all ones, which weights each predictor equally. To specify that a predictor should be unpenalized, set the corresponding entry to zero.\n\nconstraints: an [n(n+1)/2] x 2 matrix specifying lower bounds (first column) and upper bounds (second column) on each predictor. By default, this is [-Inf Inf] for each predictor (each element of tangent vectors).\n\noffset: see documentation of original GLMNet package ðŸŽ“.\n\ndfmax: The maximum number of predictors in the largest model.\n\npmax: The maximum number of predictors in any model.\n\nnlambda: The number of values of Î» along the path to consider.\n\nlambda_min_ratio: The smallest Î» value to consider, as a ratio of the value of Î» that gives the null model (i.e., the model with only an intercept). If the number of observations exceeds the number of variables, this defaults to 0.0001, otherwise 0.01.\n\nlambda: The Î» values to consider for fitting. By default, this is determined from nlambda and lambda_min_ratio.\n\nmaxit: The maximum number of iterations of the cyclic coordinate descent algorithm. If convergence is not achieved, a warning is returned.\n\nalgorithm: the algorithm used to find the regularization path. Possible values are :newtonraphson (default) and :modifiednewtonraphson.\n\nFor further informations on those arguments, refer to the resources on the GLMNet package ðŸŽ“.\n\nwarning: Possible change of dimension\nThe provided arguments penalty_factor, constraints, dfmax, pmax and  lambda_min_ratio will be ignored if a pre-conditioning pipeline is passed  as argument and if the pipeline\tchanges the dimension of the input matrices,  thus of the tangent vectors. In this case they will be set to their  default values using the new dimension. To force the use of the provided values  instead, set checkArgs to false (true by default). Note however that in this  case you must provide suitable values for all the abova arguments.\n\nOptional Keyword arguments for finding the best model by cv\n\nÎ»SelMeth = :sd1 (default), the best model is defined as the one allowing the highest cvÎ».meanloss within one standard deviation of the minimum, otherwise it is defined as the one allowing the minimum cvÎ».meanloss. Note that in selecting a model, the model with only the intercept term, if it exists, is ignored. See ENLRmodel for a description of the .cvÎ» field of the model structure.\n\nArguments nfolds, folds and parallel are passed to the GLMNet.glmnetcv function along with the â© argument. Please refer to the resources on GLMNet for details ðŸŽ“.\n\ntol: Is the convergence criterion for both the computation of a mean for projecting onto the tangent space (if the metric requires an iterative algorithm) and for the GLMNet fitting algorithm. Defaults to 1e-5. In order to speed up computations, you may try to set a lower tol; The convergence will be faster but more coarse, with a possible drop of classification accuracy, depending on the signal-to-noise ratio of the input features.\n\nIf verbose is true (default), information is printed in the REPL.\n\nThe â© argument (true by default) is passed to the tsMap function for projecting the matrices in ðTr onto the tangent space and to the GLMNet.glmnetcv function to run inner cross-validation to find the best model using multi-threading.\n\nSee: notation & nomenclature, the â„Vector type\n\nSee also: predict, crval\n\nTutorial: Examples using the ENLR model\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# Generate some data\nPTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.1)\n\n# Fit an ENLR lasso model and find the best model by cross-validation\nm = fit(ENLR(), PTr, yTr)\n\n# ... standardizing the tangent vectors\nm = fit(ENLR(), PTr, yTr; pipeline=p, normalize=standardize!)\n\n# ... balancing the weights for tangent space mapping\nm = fit(ENLR(), PTr, yTr; w=tsWeights(yTr))\n\n# ... using the log-Eucidean metric for tangent space projection\nm = fit(ENLR(logEuclidean), PTr, yTr)\n\n# Fit an ENLR ridge model and find the best model by cv:\nm = fit(ENLR(Fisher), PTr, yTr; alpha=0)\n\n# Fit an ENLR elastic-net model (Î±=0.9) and find the best model by cv:\nm = fit(ENLR(Fisher), PTr, yTr; alpha=0.9)\n\n# Fit an ENLR lasso model and its regularization path:\nm = fit(ENLR(), PTr, yTr; fitType=:path)\n\n# Fit an ENLR lasso model, its regularization path\n# and the best model found by cv:\nm = fit(ENLR(), PTr, yTr; fitType=:all)\n\n# Fit using a pre-conditioning pipeline:\np = @â†’ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)\nm = fit(ENLR(PosDefManifold.Euclidean), PTr, yTr; pipeline=p)\n\n# Use a recentering pipeline and project the data\n# onto the tangent space at the identity matrix.\n# In this case the metric is irrilevant as the barycenter\n# for determining the base point is not computed.\n# Note that the previous call to 'fit' has modified `PTr`,\n# so we generate new data.\nPTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.1)\np = @â†’ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)\nm = fit(ENLR(), PTr, yTr; pipeline=p, meanISR=I)\n\n\n\n\n\nfunction fit(model     :: SVMmodel,\n               ðTr     :: Union{HermitianVector, Matrix{Float64}},\n               yTr     :: IntVector=[];\n\n\t# pipeline (data transformations)\n\tpipeline    :: Union{Pipeline, Nothing} = nothing,\n\n\t# parameters for projection onto the tangent space\n\tw\t\t:: Union{Symbol, Tuple, Vector} = Float64[],\n\tmeanISR \t:: Union{Hermitian, Nothing, UniformScaling} = nothing,\n\tmeanInit \t:: Union{Hermitian, Nothing} = nothing,\n\tvecRange\t:: UnitRange = ðTr isa HermitianVector ? (1:size(ðTr[1], 2)) : (1:size(ðTr, 2)),\n\tnormalize\t:: Union{Function, Tuple, Nothing} = normalize!,\n\n\t# SVM parameters\n\tsvmType \t:: Type = SVC,\n\tkernel \t\t:: LIBSVM.Kernel.KERNEL = Linear,\n\tepsilon \t:: Float64 = 0.1,\n\tcost\t\t:: Float64 = 1.0,\n\tgamma \t\t:: Float64\t= 1/_getDim(ðTr, vecRange),\n\tdegree \t\t:: Int64\t= 3,\n\tcoef0 \t\t:: Float64\t= 0.,\n\tnu \t\t\t:: Float64 = 0.5,\n\tshrinking \t:: Bool = true,\n\tprobability\t:: Bool = false,\n\tweights \t:: Union{Dict{Int, Float64}, Nothing} = nothing,\n\tcachesize \t:: Float64\t= 200.0,\n\tcheckArgs\t:: Bool = true,\n\n\t# Generic and common parameters\n\ttol\t\t:: Real = 1e-5,\n\tverbose\t\t:: Bool = true,\n\tâ©\t\t:: Bool = true)\n\nCreate and fit a 1-class or 2-class support vector machine (SVM) machine learning model, with training data ðTr, of type â„Vector, and corresponding labels yTr, of type IntVector. The label vector can be omitted if the svmType is OneClassSVM (see SVM). Return the fitted model as an instance of the SVM structure.\n\nwarning: Class Labels\nLabels must be provided using the natural numbers, i.e., 1 for the first class, 2 for the second class, etc.\n\nAs for all ML models acting in the tangent space, fitting an SVM model involves computing a mean (barycenter) of all the matrices in ðTr, projecting all matrices onto the tangent space after parallel transporting them at the identity matrix and vectorizing them using the vecP operation. Once this is done, the support-vector machine is fitted.\n\nOptional keyword arguments\n\nFor the following keyword arguments see the documentation of the fit  funtion for the ENLR (Elastic Net Logistic Regression) machine learning model:\n\npipeline, transform (pre-conditioning),\nw, meanISR, meanInit, vecRange (tangent space projection),\n\ntip: Euclidean SVM models\nML models acting on the tangent space allows to fit a model passing as training data ðTr directly a matrix of feature vectors, where each feature vector is a row of the matrix. In this case none of the above keyword arguments are used.\n\nnormalize (tangent or feature vectors normalization).\n\nOptional keyword arguments for fitting the model(s) using LIBSVM.jl\n\nsvmType and kernel allow to chose among several available SVM models. See the documentation of the SVM structure.\n\nepsilon, with default 0.1, is the epsilon in loss function of the epsilonSVR SVM model.\n\ncost, with default 1.0, is the cost parameter C of SVC, epsilonSVR, and nuSVR SVM models.\n\ngamma, defaulting to 1 divided by the length of the tangent (or feature) vectors, is the Î³ parameter for RadialBasis, Polynomial and Sigmoid kernels. The provided argument gamma will be ignored if a pre-conditioning pipeline  is passed as argument and if the pipeline changes the dimension of the input matrices,  thus of the tangent vectors. In this case it will be set to its default value using  the new dimension. To force the use of the provided gamma value instead,  set\tcheckArgs to false (true by default).\n\ndegree, with default 3, is the degree for Polynomial kernels\n\ncoef0, zero by default, is a parameter for the Sigmoid and Polynomial kernel.\n\nnu, with default 0.5, is the parameter Î½ of nuSVC, OneClassSVM, and nuSVR SVM models. It should be in the interval (0, 1].\n\nshrinking, true by default, sets whether to use the shrinking heuristics.\n\nprobability, false by default sets whether to train a SVC or SVR model allowing probability estimates.\n\nif a Dict{Int, Float64} is passed as weights argument, it will be used to give weights to the classes. By default it is equal to nothing, implying equal weights to all classes.\n\ncachesize for the kernel, 200.0 by defaut (in MB), can be increased for very large problems.\n\ntol is the convergence criterion for both the computation of a mean for projecting onto the tangent space (if the metric requires an iterative algorithm) and for the LIBSVM fitting algorithm. Defaults to 1e-5.\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL. \n\nThe â© argument (true by default) is passed to the tsMap function for projecting the matrices in ðTr onto the tangent space and to the LIBSVM function that perform the fit in order to run them in multi-threaded mode.\n\nFor further information on tho LIBSVM arguments, refer to the resources on the LIBSVM package ðŸŽ“.\n\nSee: notation & nomenclature, the â„Vector type\n\nSee also: predict, crval\n\nTutorial: Examples using SVM models\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# Generate some data\nPTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.1);\n\n# Fit a SVC SVM model and find the best model by cross-validation:\nm = fit(SVM(), PTr, yTr)\n\n# ... balancing the weights for tangent space mapping\nm = fit(SVM(), PTr, yTr; w=:b)\n\n# ... using the log-Eucidean metric for tangent space projection\nm = fit(SVM(logEuclidean), PTr, yTr)\n\n# ... using the polynomial kernel of degree 3 (default)\nm = fit(SVM(logEuclidean), PTr, yTr, kernel=Polynomial)\n\n# or\n\nm = fit(SVM(logEuclidean; kernel=Polynomial), PTr, yTr)\n\n# ... using the Nu-Support Vector Classification\nm = fit(SVM(logEuclidean), PTr, yTr, kernel=Polynomial, svmtype=NuSVC)\n\n# or\n\nm = fit(SVM(logEuclidean; kernel=Polynomial, svmtype=NuSVC), PTr, yTr)\n\n# N.B. all other keyword arguments must be passed to the fit function\n# and not to the SVM constructor.\n\n# Fit a SVC SVM model using a pre-conditioning pipeline:\np = @â†’ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)\nm = fit(SVM(PosDefManifold.Euclidean), PTr, yTr; pipeline=p)\n\n# Use a recentering pipeline and project the data\n# onto the tangent space at the identity matrix.\n# In this case the metric is irrilevant as the barycenter\n# for determining the base point is not computed.\n# Note that the previous call to 'fit' has modified `PTr`,\n# so we generate new data.\nPTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80, 0.1)\np = @â†’ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)\nm = fit(SVM(), PTr, yTr; pipeline=p, meanISR=I)\n\n\n\n\n\n","category":"function"},{"location":"cv/#StatsAPI.predict","page":"Fit, Predict, CV","title":"StatsAPI.predict","text":"function predict(model  :: MDMmodel,\n                 ðTe    :: â„Vector,\n                 what   :: Symbol = :labels;\n        pipeline    :: Union{Pipeline, Nothing} = nothing,\n        verbose     :: Bool = true,\n        â©          :: Bool = true)\n\nGiven an MDM model trained (fitted) on z classes and a testing set of k positive definite matrices ðTe of type â„Vector:\n\nif what is :labels or :l (default), return the predicted class labels for each matrix in ðTe, as an IntVector. For MDM models, the predicted class 'label' of an unlabeled matrix is the serial number of the class whose mean is the closest to the matrix (minimum distance to mean). The labels are '1' for class 1, '2' for class 2, etc;\n\nif what is :probabilities or :p, return the predicted probabilities for each matrix in ðTe to belong to all classes, as a k-vector of z vectors holding reals in 0 1. The 'probabilities' are obtained passing to a softmax function the squared distances of each unlabeled matrix to all class means with inverted sign;\n\nif what is :f or :functions, return the output function of the model as a k-vector of z vectors holding reals. The function of each element in ðTe is the ratio of the  squared distance from each class to the (scalar) geometric mean of the  squared distances from all classes.\n\nIf verbose is true (default), information is printed in the REPL.\n\nIt f â© is true (default), the computation of distances is multi-threaded.\n\nNote that if the field pipeline of the provided model is not nothing, implying that a pre-conditioning pipeline has been fitted, the pipeline is applied to the data before to carry out the prediction. If you wish to adapt the pipeline to the testing data,  just fit the pipeline to the testing data overwriting the model pipeline. This is useful in a cross-session and cross-subject setting.\n\nwarning: Adapting the Pipeline\nBe careful when adapting a pipeline; if a Recenter conditioner is included in the pipeline and dimensionality reduction was sought (parameter eVar different  from nothing), then eVar must be set to an integer so that the dimension of the training ad testing data is the same after adaptation. See the example here below.\n\nSee: notation & nomenclature, the â„Vector type\n\nSee also: fit, crval, predictErr\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# Generate some data\nPTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80)\n\n# Craete and fit an MDM model\nm = fit(MDM(Fisher), PTr, yTr)\n\n# Predict labels\nyPred = predict(m, PTe, :l)\n\n# Prediction error\npredErr = predictErr(yTe, yPred)\n\n# Predict probabilities\npredict(m, PTe, :p)\n\n# Output functions\npredict(m, PTe, :f)\n\n# Using and adapting a pipeline\n\n# get some random data and labels as an example\nPTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80)\n\n# For adaptation, we need to set `eVar` to an integer or to `nothing`.\n# We will use the dimension determined on training data.\n# Note that the adaptation does not work well if the class proportions\n# of the training data is different from the class proportions of the test data.\np = @â†’ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)\n\n# Fit the model using the pre-conditioning pipeline\nm = fit(MDM(), PTr, yTr; pipeline = p)\n\n# Define the same pipeline with fixed dimensionality reduction parameter\np = @â†’ Recenter(; eVar=dim(m.pipeline)) Compress Shrink(Fisher; radius=0.02)\n\n# Fit the pipeline to testing data (adapt):\npredict(m, PTe, :l; pipeline=p) \n\n# Suppose we want to adapt recentering, but not shrinking, which also has a \n# learnable parameter. We would then use this pipeline instead:\np = deepcopy(m.pipeline)\np[1].eVar = dim(m.pipeline)\n\n\n\n\n\nfunction predict(model   :: ENLRmodel,\n\t\tðTe         :: Union{â„Vector, Matrix{Float64}},\n\t\twhat        :: Symbol = :labels,\n\t\tfitType     :: Symbol = :best,\n\t\tonWhich     :: Int    = Int(fitType==:best);\n    pipeline    :: Union{Pipeline, Nothing} = nothing,\n    meanISR     :: Union{â„, Nothing, UniformScaling} = nothing,\n    verbose     :: Bool = true,\n    â©          :: Bool = true)\n\nGiven an ENLR model trained (fitted) on 2 classes and a testing set of k positive definite matrices ðTe of type â„Vector,\n\nif what is :labels or :l (default), return the predicted class labels for each matrix in ðTe, as an IntVector. Those labels are '1' for class 1 and '2' for class 2;\n\nif what is :probabilities or :p, return the predicted probabilities for each matrix in ðTe to belong to each classe, as a k-vector of z vectors holding reals in [0, 1] (probabilities). The 'probabilities' are obtained passing to a softmax function the output of the ENLR model and zero;\n\nif what is :f or :functions, return the output function of the model, which is the raw output of the ENLR model.\n\nIf fitType = :best (default), the best model that has been found by cross-validation is used for prediction.\n\nIf fitType = :path,\n\nif onWhich is a valid serial number for a model in the model.path,\n\nthen this model is used for prediction,\n\nif onWhich is zero, all models in the model.path will be used for predictions, thus the output will be multiplied by the number of models in model.path.\n\nArgument onWhich has no effect if fitType = :best.\n\nnote: Nota Bene\nBy default, the fit function fits only the best model. If you want to use the fitType = :path option you need to invoke the fit function with optional keyword argument fitType=:path or fitType=:all. See the fit function for details.\n\nOptional keyword argument meanISR can be used to specify the principal inverse square root (ISR) of a new mean to be used as base point for projecting the matrices in testing set ðTe onto the tangent space. By default meanISR is equal to nothing, implying that the base point will be the mean used to fit the model. This corresponds to the classical 'training-test' mode of operation.\n\nPassing with argument meanISR a new mean ISR allows the adaptation first described in Barachant et al. (2013)ðŸŽ“. Typically meanISR is the ISR of the mean of the matrices in ðTe or of a subset of them. Notice that this actually performs transfer learning by parallel transporting both the training and test data to the identity matrix as defined in Zanini et al. (2018) and later taken up in Rodrigues et al. (2019)ðŸŽ“.   You can aslo pass meanISR=I, in which case the base point is taken as the identity matrix. This is possible if the set ðTe is centered to the identity, for instance, if a recentering pre-conditioner is included in a pipeline and the pipeline  is adapted as well (see the example below).\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nIf â© = true (default) and ðTe is an â„Vector type, the projection onto the tangent space is multi-threaded.\n\nNote that if the field pipeline of the provided model is not nothing, implying that a pre-conditioning pipeline has been fitted during the fitting of the model, the pipeline is applied to the data before to carry out the prediction. If you wish to adapt the pipeline to the testing data,  just pass the same pipeline as argument pipeline in this function.\n\nwarning: Adapting the Pipeline\nBe careful when adapting a pipeline; if a Recenter conditioner is included in the pipeline and dimensionality reduction was sought (parameter eVar different  from nothing), then eVar must be set to an integer so that the dimension of the training ad testing data is the same after adaptation. See the example here below.\n\nSee: notation & nomenclature, the â„Vector type\n\nSee also: fit, crval, predictErr\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# Generate some data\nPTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80)\n\n# Fit an ENLR lasso model and find the best model by cv\nm = fit(ENLR(Fisher), PTr, yTr)\n\n# Predict labels from the best model\nyPred = predict(m, PTe, :l)\n\n# Prediction error\npredErr = predictErr(yTe, yPred)\n\n# Predict probabilities from the best model\npredict(m, PTe, :p)\n\n# Output functions from the best model\npredict(m, PTe, :f)\n\n# Fit a regularization path for an ENLR lasso model\nm = fit(ENLR(Fisher), PTr, yTr; fitType=:path)\n\n# Predict labels using a specific model\nyPred = predict(m, PTe, :l, :path, 10)\n\n# Predict labels for all models\nyPred = predict(m, PTe, :l, :path, 0)\n\n# Prediction error for all models\npredErr = [predictErr(yTe, yPred[:, i]) for i=1:size(yPred, 2)]\n\n# Predict probabilities from a specific model\npredict(m, PTe, :p, :path, 12)\n\n# Predict probabilities from all models\npredict(m, PTe, :p, :path, 0)\n\n# Output functions from specific model\npredict(m, PTe, :f, :path, 3)\n\n# Output functions for all models\npredict(m, PTe, :f, :path, 0)\n\n## Adapting the base point\nPTr, PTe, yTr, yTe = gen2ClassData(10, 30, 40, 60, 80)\nm = fit(ENLR(Fisher), PTr, yTr)\npredict(m, PTe, :l; meanISR=invsqrt(mean(Fisher, PTe)))\n\n# Also using and adapting a pre-conditioning pipeline\n# For adaptation, we need to set `eVar` to an integer or to `nothing`.\n# We will use the dimension determined on training data.\n# Note that the adaptation does not work well if the class proportions\n# of the training data is different from the class proportions of the test data.\np = @â†’ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)\n\n# Fit the model using the pre-conditioning pipeline\nm = fit(ENLR(), PTr, yTr; pipeline = p)\n\n# Define the same pipeline with fixed dimensionality reduction parameter\np = @â†’ Recenter(; eVar=dim(m.pipeline)) Compress Shrink(Fisher; radius=0.02)\n\n# Fit the pipeline to testing data (adapt) and use the identity matrix as base point:\npredict(m, PTe, :l; pipeline=p, meanISR=I) \n\n# Suppose we want to adapt recentering, but not shrinking, which also has a \n# learnable parameter. We would then use this pipeline instead:\np = deepcopy(m.pipeline)\np[1].eVar = dim(m.pipeline)\n\n\n\n\n\nfunction predict(model :: SVMmodel,\n\t\t\tðTe\t:: Union{â„Vector, Matrix{Float64}},\n\t\t\twhat\t:: Symbol = :labels;\n\t\tmeanISR\t:: Union{â„, Nothing, UniformScaling} = nothing,\n\t\tpipeline:: Union{Pipeline, Nothing} = nothing,\n\t\tverbose\t:: Bool = true,\n\t\tâ©\t:: Bool = true)\n\nCompute predictions given an SVM model trained (fitted) on 2 classes and a testing set of k positive definite matrices ðTe of type â„Vector.\n\nFor the meaning of arguments what, meanISR, pipeline and verbose, see the documentation of the predict function for the ENLR model.\n\nIf â© = true (default) and ðTe is an â„Vector type, the projection onto the tangent space will be multi-threaded. Also, the prediction of the LIBSVM.jl prediction function will be multi-threaded.\n\nSee: notation & nomenclature, the â„Vector type\n\nSee also: fit, crval, predictErr\n\nExamples \n\nsee the examples for the predict function for the ENLR model; the syntax is identical, only the model used there has to be changed with a SVMmodel.\n\n\n\n\n\n","category":"function"},{"location":"cv/#PosDefManifoldML.crval","page":"Fit, Predict, CV","title":"PosDefManifoldML.crval","text":"function crval(model    :: MLmodel,\n               ð        :: â„Vector,\n               y        :: IntVector;\n            # Conditioners (Data transformation)\n            pipeline        :: Union{Pipeline, Nothing} = nothing,\n            # Cross-validation parameters\n            nFolds          :: Int      = 8,           \n            seed            :: Int      = 0,\n            # Default performance metric and statistical test\n            scoring         :: Symbol   = :b,\n            hypTest         :: Union{Symbol, Nothing} = :Bayle,\n            # arguments for this function\n            verbose         :: Bool     = true,\n            outModels       :: Bool     = false,\n            â©              :: Bool     = true,\n            BLAS_threads    :: Int = max(1, Threads.nthreads()-nFolds),\n            # Optional arguments for the fit function of the `model`\n            fitArgs...)\n\nStratified cross-validation accuracy for a machine learning model given an â„Vector ð holding k Hermitian matrices and an IntVector y holding the k labels for these matrices. Return a CVres structure.\n\nFor each fold, a machine learning model is fitted on training data and labels are predicted on testing data. Summary classification performance statistics are stored in the output structure.\n\nOptional keyword arguments\n\nIf a pipeline, of type Pipeline is provided,  the pipeline is fitted on training data and applied for predicting the testing data.\n\nnFolds, the number of folds, 8 by default.\n\nIf scoring=:b (default) the balanced accuracy is computed. Any other value will make the function returning the regular accuracy. Balanced accuracy is to be preferred for unbalanced classes. For balanced classes the balanced accuracy reduces to the regular accuracy, therefore there is no point in using regular accuracy, if not to avoid a few unnecessary computations when the classes are balanced.\n\nhypTest can be nothing or a symbol specifying the kind of statistical test to be carried out. At the moment, only :Bayle is a possible symbol and this test is performed by default. Bayle's procedure tests whether the average observed binary error loss is inferior to what is to be  expected by the hypothesis of random chance, which is set to 1-frac1z, where z is the number of classes (see testCV).\n\ntip: Error loss and Bayle's test\nNote that this function computes the error loss for each fold (see CVres). The average error loss is the complement of accuracy,  not of balanced accuracy. If the classes are balanced and you use scoring=:a (accuracy),  the average error loss within each fold is equal to 1 minus the average accuracy,  which is also computed by this function. However, this is not true if the classes are unbalanced and you use scoring=:b (default). In this case the returned error loss and accuracy may appear incoherent. For the same reason, Bayle's test, which is based on the error loss, is a test on accuracy, not on balanced accuracy, regardless the value of scoring.\n\nFor the meaning of the seed argument (0 by default), see function cvSetup, to which this argument is passed internally.\n\nIf verbose is true (default), information is printed in the REPL.\n\nIf outModels is true, return a 2-tuple holding a CVres structure and a nFolds-vector of the model fitted for each fold, otherwise (default), return only a CVres structure.\n\nIf â©, the computations are multi-threaded across folds. It is true by default. Set it to false if there are problems in running this function and for debugging.\n\nnote: Multi-threading\nIf you run the cross-validation with independent threads per fold setting â©=true(default), the fit! and predict function that will be called within each fold will we run in single-threaded mode. Vice versa, if you pass â©=false, these two functions will be run in multi-threaded mode. This is done to avoid overshooting the number of threads to be activated. Because of this behavior,  do not call this function in a multi-threaded loop.\n\nfitArgs are optional keyword arguments that are passed to the fit function called for each fold of the cross-validation. For each machine learning model, all optional keyword arguments of their fit method are elegible to be passed here, however, the arguments listed in the following table for each model should not be passed. Note that if they are passed, they will be disabled:\n\nMDM/MDMF ENLR SVM\nverbose verbose verbose\nâ© â© â©\nmeanInit meanInit meanInit\nmeanISR fitType \n offsets \n lambda \n folds \n\nIf you pass the meanISR argument, this must be nothing (default)  or I (the identity matrix). If you pass meanISR=I for a tangent space model, parallel transport of the points to the identity before projecting the points onto the tangent space will not be carried out. This can be used if a recentering conditioner is passed in the pipeline (see the fit method for the ENLR and SVM model).\n\nAlso, if you pass a w argument (weights for barycenter estimations), do not pass a vector of weights, just pass a symbol, e.g., w=:b for balancing weights.\n\nSee: notation & nomenclature, the â„Vector type\n\nSee also: fit, predict\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold, LinearAlgebra\n\n# Generate some data\nP, _dummyP, y, _dummyy = gen2ClassData(10, 60, 80, 30, 40, 0.2)\n\n# Perform 10-fold cross-validation using the minimum distance to mean classifier\n# adopting the Fisher-Rao (affine-invariant) metric (default)\ncv = crval(MDM(), P, y)\n\n# Adopting the log-Euclidean metric\ncv = crval(MDM(logEuclidean), P, y)\n\n# Apply a pre-conditioning pipeline to adopt the pseudo affine-invariant metric\np = @â†’ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)\ncv = crval(MDM(Euclidean), P, y; pipeline = p)\n\n# Apply a pre-conditioning pipeline and project the data \n# onto the tangent space at I without recentering the matrices.\n# Note that this makes sense only for tangent space ML models.\np = @â†’ Recenter(; eVar=0.999) Compress Shrink(Fisher; radius=0.02)\ncv = crval(ENLR(), P, y; pipeline = p, meanISR=I)\n\n# Perform 10-fold cross-validation using the lasso logistic regression classifier\ncv = crval(ENLR(Fisher), P, y)\n\n# ...using the support-vector machine classifier\ncv = crval(SVM(Fisher), P, y)\n\n# ...with a Polynomial kernel of order 3 (default)\ncv = crval(SVM(Fisher), P, y; kernel=Polynomial)\n\n# Perform 8-fold cross-validation instead\ncv = crval(SVM(Fisher), P, y; nFolds=8)\n\n# ...balance the weights for tangent space projection\ncv = crval(ENLR(Fisher), P, y; nFolds=8, w=:b)\n\n# perform another cross-validation shuffling the folds\ncv = crval(ENLR(Fisher), P, y; shuffle=true, nFolds=8, w=:b)\n\n\n\n\n\n\n","category":"function"},{"location":"cv/#PosDefManifoldML.cvSetup","page":"Fit, Predict, CV","title":"PosDefManifoldML.cvSetup","text":"function cvSetup(y          :: Vector{Int64},  \n                 nFolds     :: Int64;           \n                 seed       :: Int = 0)\n\nGiven a vector of labels y and the number of folds nFolds, this function generates indices for the cross-validation sets, organized by class.\n\nThe function performs a stratified cross-validation by maintaining the same class proportion across all folds as in y. \n\nEach element is used exactly once as a test sample across all folds.\n\nIf seed=0 (default), the original sequence of indices before creating  the cross-validation folds is preserved. If it is a positive number, the seed for shuffling the indices will be initialized to that number. Passing identical y and seed ensures the reproducibility of the generated folds across calls of this function.\n\nThis function is used in crval. It constitutes the fundamental basis to implement customized cross-validation procedures.\n\nReturn the 2-tuple (indTr, indTe) where:\n\nindTr is an array of arrays where indTr[i][f] contains the training indices for class i in fold f\nindTe is an array of arrays where indTe[i][f] contains the test indices for class i in fold f.\n\nEach array is organized by class and then by fold, ensuring stratified sampling across the cross-validation sets.\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\ny = [1,1,1,1,2,2,2,2,2,2]\n\ncvSetup(y, 2)\n# returns:\n# Training Arrays:\n#   Class 1: Array{Int64}[[3, 4], [1, 2]]\n#   Class 2: Array{Int64}[[4, 5, 6], [1, 2, 3]]\n# Testing Arrays:\n#   Class 1: Array{Int64}[[1, 2], [3, 4]]\n#   Class 2: Array{Int64}[[1, 2, 3], [4, 5, 6]]\n\ncvSetup(y, 2; seed=1234)\n# returns:\n# Training Arrays:\n#   Class 1: Array{Int64}[[1, 4], [2, 3]]\n#   Class 2: Array{Int64}[[4, 5, 6], [1, 2, 3]]]\n# Testing Arrays:\n#   Class 1: Array{Int64}[[2, 3], [1, 4]]\n#   Class 2: Array{Int64}[[1, 2, 3], [4, 5, 6]]\n\ncvSetup(y, 3)\n# returns:\n# Training Arrays:\n#   Class 1: Array{Int64}[[2, 3], [1, 3, 4], [1, 2, 4]]\n#   Class 2: Array{Int64}[[3, 4, 5, 6], [1, 2, 5, 6], [1, 2, 3, 4]]\n# Testing Arrays:\n#   Class 1: Array{Int64}[[1, 4], [2], [3]]\n#   Class 2: Array{Int64}[[1, 2], [3, 4], [5, 6]]\n\n\n\n\n\n\n","category":"function"},{"location":"#PosDefManifoldML-Documentation","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"","category":"section"},{"location":"#Requirements-and-Installation","page":"PosDefManifoldML Documentation","title":"Requirements & Installation","text":"","category":"section"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Julia: version â‰¥ 1.1","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Packages: see the dependencies of the main module.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"To install the package, execute the following commands in Julia's REPL:","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"]add PosDefManifold PoSDefManifoldML","category":"page"},{"location":"#Reviewers-and-Contributors","page":"PosDefManifoldML Documentation","title":"Reviewers & Contributors","text":"","category":"section"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Independent reviewers for both the code and the documentation are welcome. To contribute, please check the secion How to Contribute.","category":"page"},{"location":"#TroubleShoothing","page":"PosDefManifoldML Documentation","title":"TroubleShoothing","text":"","category":"section"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Problem Solution\nIn Windows, an error may appear at execution time the first time an SVM model is fitted or is passed to the crval function Discourse\nUndefVarErrors, looking like metric not defined, Fisher not defined, etc. install the PosDefManifold package","category":"page"},{"location":"#About-the-Authors","page":"PosDefManifoldML Documentation","title":"About the Authors","text":"","category":"section"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Marco Congedo, corresponding author, is a Research Director of CNRS (Centre National de la Recherche Scientifique), working at UGA (University of Grenoble Alpes). Contact: first name dot last name at gmail dot com","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Anton Andreev is a research engineer working at the same institution.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Fahim Doumi at the time of writing was a research ingeneer at the same institution.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Saloni Jain at the time of writing the package was a student at the Indian Institute of Technology, Kharagpur, India.","category":"page"},{"location":"#Overview","page":"PosDefManifoldML Documentation","title":"Overview","text":"","category":"section"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Riemannian geometry studies smooth manifolds, multi-dimensional curved spaces with peculiar geometries endowed with non-Euclidean metrics. In these spaces Riemannian geometry allows the definition of angles, geodesics (shortest path between two points), distances between points, centers of mass of several points, etc.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"In several fields of research such as computer vision and brain-computer interface, treating data in the manifold of positive definite matrices has allowed the introduction of machine learning approaches with remarkable characteristics, such as simplicity of use, excellent classification accuracy, as demonstrated by the winning score obtained in six international data classification competitions, and the ability to operate transfer learning (Congedo et al., 2017a, Barachant et al., 2012)ðŸŽ“.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"In this package we are concerned with making use of Riemannian Geometry for classifying data in the form of positive definite matrices (e.g., covariance matrices, Fourier cross-spectral matrices, etc.). This can be done in two ways: either directly in the manifold of positive definite matrices using Riemannian machine learning methods or in the tangent space, where traditional (Euclidean) machine learning methods apply (i.e., linear discriminant analysis, support-vector machine, logistic regression, random forest, etc.).","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"(Image: Figure 1) Figure 1","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Schematic representation of Riemannian classification. Data points are either natively positive definite matrices or are converted into this form. The classification can be performed by Riemannian methods in the manifold of positive definite matrices or by Euclidean methods after projection onto the tangent space.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Currently implemented models are:","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Acting on the manifold of PD matrices","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"the Riemannian minimum-distance to mean (MDM).","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Acting on the tangent space","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"elastic-net logistic regression (ENLR), including the pure Lasso and pure Ridge logistic regression;\nsupport-Vector machine (SVM), including C-Support Vector Classification (C-SVC), nu-SVC, one-class SVC, *Epsilon Support-Vector Regression** (SVR) and *nu SVR**.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"For a formal introduction to the manifold of positive definite matrices the reader is referred to the monography written by Bhatia(2007)ðŸŽ“.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"For an introduction to Riemannian geometry and an overview of mathematical tools implemented in the PostDefManifold package, which is used here, see Intro to Riemannian Geometry.","category":"page"},{"location":"#Code-units","page":"PosDefManifoldML Documentation","title":"Code units","text":"","category":"section"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"PosDefManifoldML includes six code units (.jl files):","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Unit Description\nMainModule Main module, declaring constants and types\nmdm.jl Unit implementing the MDM( Minimum Distance to Mean) machine learning model\nenlr.jl Unit implementing the ENLR( Elastic Net Logistic Regression) model, including the LASSO and RIDGE LR\nsvm.jl Unit implementing the SVM (Support-Vector Machine) models\ncv.jl Unit implementing cross-validation procedures\nconditioners.jl Unit implementing pre-conditioning pipelines\nstats_descriptive.jl Unit implementing descrptive statistics for cross-validation\nstats_inferential.jl Unit implementing inferential statistics (tests) for cross-validation\ntools.jl Unit containing general tools useful for machine learning\nprivate.jl Unit containing internal functions","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"ðŸŽ“","text":"","category":"section"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"References","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2012) Multi-class Brain Computer Interface Classification by Riemannian Geometry, IEEE Transactions on Biomedical Engineering, 59(4), 920-928.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2013) Classification of covariance matrices using a Riemannian-based kernel for BCI applications, Neurocomputing, 112, 172-178.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"P. Bayle, A. Bayle, L. Janson, L. Mackey (2020) Cross-validation confidence intervals for test error, Proc. of the 34th Int. Conf. on Neural Information Processing Systems (NIPS), Vancouver, canada, 1371, 16339-16350.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"R. Bhatia (2007) Positive Definite Matrices, Princeton University press.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"M. Congedo, A. Barachant, R. Bhatia R (2017a) Riemannian Geometry for EEG-based Brain-Computer Interfaces; a Primer and a Review, Brain-Computer Interfaces, 4(3), 155-174.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"M. Congedo, A. Barachant, E. Kharati Koopaei (2017b) Fixed Point Algorithms for Estimating Power Means of Positive Definite Matrices, IEEE Transactions on Signal Processing, 65(9), 2211-2220.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Rodrigues PLC, Jutten C, Congedo M (2019) Riemannian Procrustes Analysis : Transfer Learning for Brain-Computer Interfaces, IEEE Transactions on Biomedical Engineering, 66(8), 2390-2401.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"P. Zanini P, M. Congedo, C. Jutten, S. Said, Y. Berthoumieu (2018) Transfer Learning: a Riemannian geometry framework with applications to Brain-Computer Interfaces, IEEE Transactions on Biomedical Engineering, 65(5), 1107-1116.","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Resources on GLMNet","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"webinar by Trevor Hastie","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Glmnet vignette","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Glmnet in R, documentation","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Julia wrapper for GLMNet","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"A more advanced wrapper for GLMNet","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Resources on LIBSVM","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"official page","category":"page"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"github","category":"page"},{"location":"#Contents","page":"PosDefManifoldML Documentation","title":"Contents","text":"","category":"section"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Pages = [       \"index.md\",\n\t\t\t\t\"tutorial.md\",\n                \"MainModule.md\",\n                \"mdm.md\",\n                \"enlr.md\",\n\t\t\t\t\"svm.jl\",\n\t\t\t\t\"cv.md\",\n\t\t\t\t\"conditioners.md\",\n\t\t\t\t\"stats_descriptive.md\",\n\t\t\t\t\"stats_inferential.md\",\n\t\t\t\t\"contribute.md\",\n\t\t\t\t\"tools.md\",\n\t\t]\nDepth = 1","category":"page"},{"location":"#Index","page":"PosDefManifoldML Documentation","title":"Index","text":"","category":"section"},{"location":"","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If you didn't, please read first the Overview.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"PosDefManifoldML features two bacic machine learning modes of operation:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"train-test: a machine learning (ML) model is first fitted (trained), then it can be used to predict (test) the labels of testing data or the probability of the data to belong to each class. The raw prediction function of the models is available as well.\na k-fold cross-validation procedure allows to estimate the accuracy of ML models and compare them.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The train-test mode is useful in cross-subject and cross-session settings,  while cross-validation is the standard for within-session settings.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"What PosDefManifoldML does for you is to allow an homogeneous syntax to operate in these two modes for all implemented ML models, it does not matter if they act directly on the manifold of positive definite matrices or on the tangent space. It also features ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Pre-conditining pipelines, which can drastically reduce the execution time\nAdaptation techniques, which, besides being very useful in cross-session and cross-subject settings, are instrumental for implementing on-line modes of operation.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Note that models acting on the tangent space can take as input Euclidean feature vectors instead of positive definite matrices, thus they can be used in many more situations.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Get data","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let us create simulated data for a 2-class example. First, let us create symmetric positive definite matrices (real positive definite matrices):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using PosDefManifoldML, PosDefManifold\n\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"PTr is the simulated training set, holding 30 matrices for class 1 and 40 matrices for class 2\nPTe is the testing set, holding 60 matrices for class 1 and 80 matrices for class 2.\nyTr is a vector of 70 labels for the training set\nyTe is a vector of 140 labels for the testing set","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"All matrices are of size 10x10.","category":"page"},{"location":"tutorial/#Examples-using-the-MDM-model","page":"Tutorial","title":"Examples using the MDM model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The minimum distance to mean (MDM) classifier is an example of classifier acting directly on the manifold. It is deterministic and no hyperparameter tuning is needed.","category":"page"},{"location":"tutorial/#MDM-train-test","page":"Tutorial","title":"MDM train-test","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Craete and fit an MDM model","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"An MDM model is created and fitted with training data such as","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m = fit(MDM(Fisher), PTr, yTr)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where Fisher (affine-invariant) is the usual choice of a Metric as declared in the parent package PosDefManifold.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Since the Fisher metric is the default (for all ML models), the above is equivalent to:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m = fit(MDM(), PTr, yTr)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In order to adopt another metric:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m1 = fit(MDM(logEuclidean), PTr, yTr)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Predict (classify data)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In order to predict the labels of unlabeled data (which we have stored in PTe), we invoke","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m, PTe, :l)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The prediction error in percent can be retrived with","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"predictErr(yTe, yPred)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"the predicton accuracy as","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"predictAcc(yTe, yPred)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"and the confusion matrix as","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"confusionMat(yTe, yPred)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where in yTe we have stored the true labels for the matrices in PTe.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If instead we wish to estimate the probabilities for the matrices in PTe of belonging to each class:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"predict(m, PTe, :p)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Finally, the output functions of the MDM are obtaine by (see predict)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"predict(m, PTe, :f)","category":"page"},{"location":"tutorial/#MDM-cross-validation","page":"Tutorial","title":"MDM cross-validation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The balanced accuracy estimated by a k-fold cross-validation is obtained such as (10-fold by default)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"cv = crval(MDM(), PTr, yTr)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As for all functions in the Julia language, the first time you run a function it is compiled, so it is slow. To appreciate the speed, run it again ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"cv = crval(MDM(), PTr, yTr)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Struct cv has been created and therein you have access to average accuracy and confusion matrix as well as accuracies and confusion matrices for all folds. For example, print the average confusion matrix (expressed in proportions):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"cv.avgCnf","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"See CVres for details on the fields of cross-validation objects.","category":"page"},{"location":"tutorial/#MDM-adaptation","page":"Tutorial","title":"MDM adaptation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's see how to adapt a pre-conditioning pipeline. Suppose you have data from two  sessions or two subjects, s1 and s2. We want to use s1 to train a machine learning model on the tangent space and s2 to test it. A pipeline is fitted on s1 and we want this pipeline to adapt to s2 for testing. If the pipeline includes a recentering pre-conditioner, we need to make sure that the dimensionality reduction determined on s2 is the same as in s1.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Get data","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let us get some simulated data. We generate random data and labels for session (or subject) 1 and 2.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Ps1, Ps2, ys1, ys2 = gen2ClassData(10, 30, 40, 60, 80);","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Define the pre-conditioning pipeline for s1","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"p = @â†’ Recenter(; eVar=0.999) â†’ Compress â†’ Shrink(Fisher; radius=0.02)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Fit an MDM model on s1 using the pipeline","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m = fit(MDM(), Ps1, ys1; pipeline = p)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The fitted pipeline with all learnt parameters is stored in model m. Instead of transforming the data in s2 using this pipeline, which is the default behavior of the predict function, let us define the same pipeline with a dimensionality  reduction parameter fixed as it has been learnt on s1.  This way this paramater cannot change and the transformed matrices in s1 and s2 will have equal size. That is, we allow adaptation of all parameters, but force the same dimension.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"p = @â†’ Recenter(; eVar=dim(m.pipeline)) â†’ Compress â†’ Shrink(Fisher; radius=0.02)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Fit the pipeline to s2 and predict","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"predict(m, Ps2, :l; pipeline=p)","category":"page"},{"location":"tutorial/#Examples-using-the-ENLR-model","page":"Tutorial","title":"Examples using the ENLR model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The elastic net logistic regression (ENLR) classifier is an example of classifier acting on the tangent space. Besides the metric used to compute a base-point for projecting the data onto the tangent space, it has a parameter alpha and an hyperparameter lambda. The alpha parameter allows to trade off between a pure ridge LR model (Î±=0) and a pure lasso LR model (Î±=1), which is the default. Given an alpha value, the model is fitted with a number of values for the Î» (regularization) hyperparameter. Thus, differently from the previous example, tuning the Î» hyperparameter is necessary.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Also, keep in mind that the fit and predict methods for ENLR models accept optional keyword arguments that are specific to this model.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Get data","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let us get some simulated data (see the previous example for explanations).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using PosDefManifoldML, PosDefManifold\n\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);","category":"page"},{"location":"tutorial/#ENLR-train-test","page":"Tutorial","title":"ENLR train-test","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Craete and fit ENLR models","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By default, the Fisher metric ic adopted and a lasso model is fitted. The best value for the lambda hyperparameter is found by cross-validation:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m1 = fit(ENLR(), PTr, yTr; w=:balanced)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Notice that since the class are unbalanced, with the w=:balanced argument (we may as well just use w=:b) we have requested to compute a balanced mean for projecting the matrices in PTr onto the tangent space.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The optimal value of lambda for this training data is","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m1.best.lambda","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As in GLMNet.jl, the intercept and beta terms are retrived by","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m1.best.a0\nm1.best.betas","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The number of non-zero beta coefficients can be found, for example, by","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"length(unique(m1.best.betas))-1","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In order to fit a ridge LR model:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m2 = fit(ENLR(), PTr, yTr; w=:b, alpha=0)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Values of alpha in range (0 1) fit instead an elastic net LR model. In the following we also request not to normalize predictors (by default they norm is fixed):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m3 = fit(ENLR(Fisher), PTr, yTr; w=:b, alpha=0.9, normalize=nothing)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Instead we could standardize predictors:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m4 = fit(ENLR(Fisher), PTr, yTr; w=:b, alpha=0.9, normalize=standardize!)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"or rescale them within custom limits:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m5 = fit(ENLR(Fisher), PTr, yTr; w=:b, alpha=0.9, normalize=(-1.0, 1.0))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In order to find the regularization path we use the fitType keyword argument:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m1 = fit(ENLR(Fisher), PTr, yTr; w=:b, fitType=:path)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The values of lambda along the path are given by","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m1.path.lambda","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can also find the best value of the lambda hyperparameter and the regularization path at once, calling:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m1 = fit(ENLR(Fisher), PTr, yTr; w=:b, fitType=:all)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For changing the metric see MDM train-test.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"See the documentation of the fit ENLR method for details on all available optional arguments.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Classify data (predict)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For prediction, we can request to use the best model (optimal lambda), to use a specific model of the regularization path or to use all the models in the regalurization path. Note that with the last call we have done here above both the .best and .path field of the m1 structure have been created.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By default, prediction is obtained from the best model and we request to predict the labels:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe)\n\n# prediction accuracy (in proportion)\npredictAcc(yPred, yTe)\n\n# confusion matrix\nconfusionMat(yPred, yTe)\n\n# predict probabilities of matrices in `PTe` to belong to each class\npredict(m1, PTe, :p)\n\n# output function of the model for each class\npredict(m1, PTe, :f)\n","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In order to request the predition of labels for all models in the regularization path:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe, :l, :path, 0)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"while for a specific model in the path (e.g., model #10):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe, :l, :path, 10)","category":"page"},{"location":"tutorial/#ENLR-cross-validation","page":"Tutorial","title":"ENLR cross-validation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The balanced accuracy estimated by a k-fold cross-validation is obtained with the exact same basic syntax for all models, with some specific optional keyword arguments for models acting in the tangent space, for example:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"cv = crval(ENLR(), PTr, yTr; w=:b)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In order to perform another cross-validation arranging the training data differently in the folds:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"cv = crval(ENLR(), PTr, yTr; w=:b, shuffle=true)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This last command can be invoked repeatedly.","category":"page"},{"location":"tutorial/#ENLR-adaptation","page":"Tutorial","title":"ENLR adaptation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"First, let's see how to adapt the base point for projecting the data onto the tangent space. Suppose you have data from two sessions or two subjects, s1 and s2. We want to use s1 to train a machine learning model on the tangent space and s2 to test it, however, the barycenter s1 cannot be assumed equal to the barycenter of s2. The barycenter determines the base point, therefore, we adapt it.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Get data","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let us get some simulated data. We generate random data and labels for session (or subject) 1 and 2.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Ps1, Ps2, ys1, ys2 = gen2ClassData(10, 30, 40, 60, 80);","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Craete and fit an ENLR model on s1","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m = fit(ENLR(Fisher), Ps1, ys1)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Classify (predict) data of s2 adapting the base point","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"predict(m, PTe, :l; meanISR=invsqrt(mean(Fisher, PTe)))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Second, let's see how to adapt a pre-conditioning pipeline like we have done  here above for the base point. Since the pipeline we will employ recenter the data  around the identity, we can skip altogether the computation of the barycenter  for s2, using the identity matrix as the base point.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The pipeline we will define comprises a recentering pre-conditioner  with dimensionality reduction. While adapting the pipeline to s2,  we need to make sure that the matrices in s2 are reduced to the same  dimension as the matrices in s1, otherwise the  machine learning model we fit on s1 cannot operate  on s2. For this, we need to set the eVar argmument of the Recenter pre-conditioner to a integer matching the reduced dimension of s1. Note that the adaptation may not work well if the class proportions is different in s1 and s2.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Define the pre-conditioning pipeline for s1","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"p = @â†’ Recenter(; eVar=0.999) â†’ Compress â†’ Shrink(Fisher; radius=0.02)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Fit the model on s1 using the pipeline","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m = fit(ENLR(), Ps1, ys1; pipeline = p)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Define the same pipeline with fixed dimensionality reduction parameter","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"p = @â†’ Recenter(; eVar=dim(m.pipeline)) â†’ Compress â†’ Shrink(Fisher; radius=0.02)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Fit the pipeline to s2 (adapt) and use the identity matrix as base point","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"predict(m, Ps2, :l; pipeline=p, meanISR=I) ","category":"page"},{"location":"tutorial/#Examples-using-SVM-models","page":"Tutorial","title":"Examples using SVM models","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The SVM ML model actually encapsulates several support-vector classification and support-vector regression models. Here we are concerned with the former, which include the C-Support Vector Classification (SVC), the Nu-Support Vector Classification (NuSVC), similar to SVC but using a parameter to control the number of support vectors, and the One-Class SVM (OneClassSVM), which is used in general for unsupervised outlier detection. They all act in the tangent space like ENLR models. Besides the metric (see MDM train-test) used to compute a base-point for projecting the data onto the tangent space and the type of SVM model (the svmType, = SVC (default), NuSVC or OneClassSVM), the main parameter is the kernel. Avaiable kernels are:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Linear      (default)\nRadialBasis \nPolynomial\nSigmoid","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Several parameters are available for building all these kernels besides the linear one, which has no parameter. Like for ENLR, for SVM models also an hyperparameter is to be found by cross-validation.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Get data","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let us get some simulated data as in the previous examples.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using PosDefManifoldML, PosDefManifold\n\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);","category":"page"},{"location":"tutorial/#SVM-train-test","page":"Tutorial","title":"SVM train-test","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Craete and fit SVM models","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By default, a C-Support Vector Classification model is fitted:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m1 = fit(SVM(), PTr, yTr; w=:b)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Notice that, as for the example above with for ENLR model, we have requested to compute a balanced mean for projecting the matrices in PTr onto the tangent space.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In order to fit a Nu-Support Vector Classification model:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m2 = fit(SVM(), PTr, yTr; w=:b, svmType=NuSVC)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For using other kernels, e.g.:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m3 = fit(SVM(), PTr, yTr; w=:b, svmType=NuSVC, kernel=Polynomial)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In the following we request not to normalize predictors (by default they norm is fixed):","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m4 = fit(SVM(), PTr, yTr; w=:b, normalize=nothing)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Instead we could standardize predictors:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m5 = fit(SVM(), PTr, yTr; w=:b, normalize=standardize!)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"or rescale them within custom limits:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m6 = fit(SVM(), PTr, yTr; w=:b, normalize=(-1.0, 1.0))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By default the Fisher metric is used. For changing it, see MDM train-test.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"See the documentation of the fit SVM method for details on all available optional arguments.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Classify data (predict)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Just the same as for the other models:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe)\n\n# prediction accuracy (in proportion)\npredictAcc(yPred, yTe)\n\n# confusion matrix\nconfusionMat(yPred, yTe)\n\n# predict probabilities of matrices in `PTe` to belong to each class\npredict(m1, PTe, :p)\n\n# output function of the model for each class\npredict(m1, PTe, :f)","category":"page"},{"location":"tutorial/#SVM-cross-validation","page":"Tutorial","title":"SVM cross-validation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Again, the balanced accuracy estimated by a k-fold cross-validation is obtained with the exact same basic syntax for all models, with some specific optional keyword arguments for models acting in the tangent space, for example:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"cv = crval(SVM(), PTr, yTr; w=:b)","category":"page"},{"location":"tutorial/#SVM-adaptation","page":"Tutorial","title":"SVM adaptation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"See the tutirial on ENLR adaptation; the code needed is exactly the same changing the machine learning model from ENLR to SVM.","category":"page"},{"location":"conditioners/#conditioners.jl","page":"Conditioners & Pipelines","title":"conditioners.jl","text":"","category":"section"},{"location":"conditioners/","page":"Conditioners & Pipelines","title":"Conditioners & Pipelines","text":"This unit implements conditioners, also called pre-conditioners and pipelines, which are specified sequences of conditioners.","category":"page"},{"location":"conditioners/","page":"Conditioners & Pipelines","title":"Conditioners & Pipelines","text":"Pipelines are applied to the data (symmetric positive-definite matrices) in order to increase the classification performance and/or to reduce the computational complexity of classifiers.","category":"page"},{"location":"conditioners/#Conditioners","page":"Conditioners & Pipelines","title":"Conditioners","text":"","category":"section"},{"location":"conditioners/","page":"Conditioners & Pipelines","title":"Conditioners & Pipelines","text":"The available conditioners are","category":"page"},{"location":"conditioners/","page":"Conditioners & Pipelines","title":"Conditioners & Pipelines","text":"Conditioner type description\nTikhonov Tikhonov regularization\nRecenter Recentering aroung the identity matrix w/o dimensionality reduction\nCompress Global scaling reducing the norm\nEqualize Individual scaling (normalization) to reduce the norm\nShrink Move along the geodesic to reduce the norm","category":"page"},{"location":"conditioners/","page":"Conditioners & Pipelines","title":"Conditioners & Pipelines","text":"In the table above, as elsewhere in this documentation, by norm of a matrix it is meant the distance of the matrix to the identity.","category":"page"},{"location":"conditioners/#Pipelines","page":"Conditioners & Pipelines","title":"Pipelines","text":"","category":"section"},{"location":"conditioners/","page":"Conditioners & Pipelines","title":"Conditioners & Pipelines","text":"Pipelines are stored as a dedicated tuple type - see Pipeline.","category":"page"},{"location":"conditioners/#Content","page":"Conditioners & Pipelines","title":"Content","text":"","category":"section"},{"location":"conditioners/","page":"Conditioners & Pipelines","title":"Conditioners & Pipelines","text":"function description\n@pipeline Macro to create a pipeline\nfit! Fit a pipeline and transform the data\ntransform! Transform the data using a fitted pipeline\npickfirst Return a copy of a specific conditioner in a pipeline\nincludes Check whether a conditioner type is in a pipeline\ndim Dimension determined by a recentering pre-conditioner in a pipeline","category":"page"},{"location":"conditioners/#PosDefManifoldML.Tikhonov","page":"Conditioners & Pipelines","title":"PosDefManifoldML.Tikhonov","text":"mutable struct Tikhonov <: Conditioner\n    Î±\n    threaded \n\nMutable structure for the Tikhonov regularization conditioner. \n\nGiven a set of points ð in the manifold of positive-definite matrices, transform the set such as \n\nP_j+Î±I  j=1k,\n\nwhere I is the identity matrix and Î± is a non-negative number.\n\nThis conditoner structure has two fields: \n\n.Î±, which is written in the structure when it is fitted to some data.\n.threaded, to determine if the transformation is done in multi-threading mode (true by default).\n\nFor constructing an instance, Î± is an argument, while threaded is a optional keyword argument. \n\nwarning: This is not a data-driven conditioner\nThe Î± parameter must be given explicitly upon construction (it is zero by default).\n\nExamples:\n\nusing PosDefManifoldML, PosDefManifold\n\n# Create a conditioner\nT = Tikhonov(0.001)\nT = Tikhonov(0.001; threaded=false)\n\nSee also: fit!, transform!, crval\n\n\n\n\n\n","category":"type"},{"location":"conditioners/#PosDefManifoldML.Recenter","page":"Conditioners & Pipelines","title":"PosDefManifoldML.Recenter","text":"mutable struct Recenter <: Conditioner\n    metric \n    eVar \n    w \n    âœ“w \n    init \n    tol \n    verbose \n    forcediag \n    threaded \n    ## Fitted parameters\n    Z \n    iZ \n\nMutable structure for the recentering conditioner. \n\nGiven a set of nÂ·n points ð in the manifold of positive-definite matrices, transform the set such as \n\nZP_jZ^T  j=1k,\n\nwhere Z is the whitening matrix of the barycenter of ð as specified by the conditioner, i.e., if G is the barycenter of ð, then ZGZ^T=I.\n\nAfter recentering the barycenter becomes the identity matrix and the mean of the eigenvalues of the whitened matrices is 1. In the manifold of positive-definite matrices, recentering is equivalent to parallel transport of all points to the identity barycenter, according to a given metric.\n\nDepending on the eVar value used to define the Recenter conditioner, matrices Z may determine a dimensionality reduction of the input points as well. In this case Z is not square, but a wide matrix of dimension pn, with pn.\n\nThis conditoner may behave in a supervised way; providing the class labels  when it is fitted (see fit!), the classes are equally weighted to compute the barycenter G, like tsWeights`` does for computing the barycenter used for tangent space mapping. If the classes are balanced, the weighting has no effect.\n\nThis conditoner structure has the following fields:\n\n.metric, of type  Metric, is to be specified by the user. It is the metric that will be adopted to compute the class means and the distances to the mean. default: PosDefManifold.Euclidean.\n.eVar, the desired explained variance for the whitening. It can be a Real, Int or nothing. See the documentation of method whitening in Diagonalizations.jl. It is 0.9999 by default.\nFields .w, .âœ“w, .init and .tol are passed to the mean method of PosDefManifold.jl for computing the barycenter G. Refer to the documentation therein for details.\n.verbose is a boolean determining if information is to be printed to the REPL when using fit! and transform! with this conditioner. It is false by default.\n.forcediag is a boolean for forcing diagonalization. It is true by default. If false, whitening is carried out only if a dimensionality reduction is needed, as determined by eVar.\nIf .threaded is true (default), all operations are multi-threaded.\n\nFor constructing an instance, metric is an argument, while eVar, w, âœ“w,  init, tol, verbose, forcediag and threaded are optional keyword arguments.\n\nFitted parameters\n\nWhen the conditioner is fitted, the following fields are written:\n\n.Z, the whitening matrix of the fitted set P_j  j=1k, such that ZP_jZ^T is whitened;\n.iZ, the left inverse Z^* of Z, such that Z^*Z=I (identity matrix) if no dimensionality reduction is operated.\n\nIf dimensionality reduction is operated, Z^*ZI has rank p.\n\nExamples:\n\nusing PosDefManifoldML, PosDefManifold\n\n# Create a default conditioner\nR = Recenter(PosDefManifold.Euclidean)\n\n# Since the Euclidean metric is the default metric,\n# this is equivalent to\nR = Recenter()\n\n# Do not perform dimensionality reduction\nR = Recenter(PosDefManifold.Fisher; eVar=nothing)\n\n# Reduce the dimension to 10\nR = Recenter(PosDefManifold.Fisher; eVar=10)\n\n# Determine the dimension so as to explain at least 90% of the variance\nR = Recenter(PosDefManifold.Fisher; eVar=0.9)\n\n# Use class labels to balance the weights across classes\n# (let `y` be a vector of int holding the class labels)\nR = Recenter(PosDefManifold.Fisher; labels=y)\n\n\nSee also: fit!, transform!, crval\n\n\n\n\n\n","category":"type"},{"location":"conditioners/#PosDefManifoldML.Compress","page":"Conditioners & Pipelines","title":"PosDefManifoldML.Compress","text":"mutable struct Compress <: Conditioner\n    threaded\n    Î² \nend\n\nMutable structure for the compressing conditioner.\n\nGiven a set of points ð in the manifold of positive-definite matrices, transform the set such as \n\nÎ²P_j  j=1k,\n\nwhere Î² is chosen to minimize the average Euclidean norm of the transformed set, i.e., the average distance to the identity matrix according to the specifies metric.\n\nSince the Euclidean norm is the Euclidean distance to the identity, compressing  a recentered set of points minimizes the average dispersion of the set around  the identity, thus it should be performed after conditioner Recenter. \n\nThe structure has one field only:\n\n.threaded, determining whether the computations are multi-threaded (true by default).\n\nFor constructing an instance, only the threaded optional keyword argument can be used.\n\nFitted parameters\n\nWhen the conditioner is fitted, the following field is written:\n\n.Î², a positive scalar minimizing the average Euclidean norm of the fitted set.\n\nExamples:\n\nusing PosDefManifoldML, PosDefManifold\n\n# Create the conditioner\nC = Compress()\n\nSee also: fit!, transform!, crval\n\n\n\n\n\n","category":"type"},{"location":"conditioners/#PosDefManifoldML.Equalize","page":"Conditioners & Pipelines","title":"PosDefManifoldML.Equalize","text":"mutable struct Equalize <: Conditioner\n    threaded\n    Î² \nend\n\nMutable structure for the equalizing conditioner.\n\nGiven a set of points ð in the manifold of positive-definite matrices, transform the set such as \n\nÎ²_jP_j  j=1k,\n\nwhere the elements Î²_j are chosen so as to minimize the Euclidean norm  of the transformed matrices individually.\n\nSince the Euclidean norm is the Euclidean distance to the identity, equalizing  a recentered set of points minimizes the average dispersion of the set around  the identity, thus it should be performed after conditioner Recenter. \n\nAs compared to compression, equalization is more effective for reducing the distance  to the identity, however it is not an isometry.\n\nAlso, in contrast to compression, the transformation of the  matrices in set ð is individual, so fitting equalization does not imply  a learning process - see fit!.\n\nThe structure has one field only:\n\n.threaded, determining whether the computations are multi-threaded (true by default).\n\nFor constructing an instance, only the threaded optional keyword argument can be used.\n\nFitted parameters\n\nWhen the conditioner is fitted, the following field is written:\n\n.Î², a vector of positive scalars minimizing the Euclidean norm individually for each matrix in the fitted set. \n\nExamples:\n\nusing PosDefManifoldML, PosDefManifold\n\n# Create the conditioner\nE = Equalize()\n\nSee also: fit!, transform!, crval\n\n\n\n\n\n","category":"type"},{"location":"conditioners/#PosDefManifoldML.Shrink","page":"Conditioners & Pipelines","title":"PosDefManifoldML.Shrink","text":"mutable struct Shrink <: Conditioner\n        metric \n        radius \n        refpoint \n        reshape \n        epsilon \n        verbose \n        threaded \n        ## Fitted parameters\n        Î³ \n        m \n        sd \n\nMutable structure of the geodesic shrinking conditioner. \n\nGiven a set of points ð in the manifold of positive-definite matrices, this conditioner moves all points towards the identity matrix I along geodesics on the manifold defined in accordance to the specified metric. This effectively defines a ball centered at I.\n\nThe step-size Î³ of the geodesics from I to each point P in ð is given by\n\ngamma=fracrsqrtnÎ´(P I) + Ïµ\n\nwhere r is the radius argument, n is the dimension of P,  Î´(P I) is the norm of P according to the specified metric and Ïµ is an optional  small positive number given as argument epsilon.\n\nThe conditioner has the following fields, which are also keyword arguments that can be passed upon construction:\n\n.metric, of type Metric, with default PosDefManifold.Euclidean.\n\nAfter shrinking, the set of points ð acquires a sought .radius,  which is given as optional keyword argument to the constructor (default: 0.02). This is a measure of their acquired distances from the identity (norms),  specifically, the maximum distance if .refpoint=:max or the mean eccentricity  if .refpoint=:mean (default).  In the first case the argument radius defines a ball confining all points, with radius equal to the maximum distance from the identity of the transformed points + Ïµ.  In the second case, the actual radius of the ball is equal to \n\nsqrtfrac1nsum_j=1^kÎ´(P_j I) + Ïµ.\n\n.reshape, a boolean for reshaping the eigenvalues of the set ð after shrinking.  It applies only to the Fisher (affine-invariant) metric. Default: false. See below.\n\n.epsilon, a non-negative real number, the Ïµ above. Default: 0.0.\n\n.verbose, a boolean. If true, information is printed in the REPL. Default: false\n\n.threaded, a boolean for using multi-threading. Default: true\n\nFor constructing an instance, metric is an argument, while radius, refpoint, reshape,  epsilon, verbose and threaded are optional keyword arguments.\n\nFitted parameters\n\nWhen the conditioner is fitted, the following fields are written:\n\n.Î³, the step-size for geodesics (according to metric) from I to the each matrix in ð.\n\n.m and .sd, the mean and standard deviation of the eigenvalues of the set after shrinking. This is used for reshaping, which applies only if the Fisher metric is adopted. Reshaping is meaningful only if the input set has been recentered (see Recenter). It recenters again the eigenvalues of the set after shrinking (mean = 1),  and normalize them so as to have standard deviation equal to .radius.\n\nExamples:\n\nusing PosDefManifoldML, PosDefManifold\n\n# Create a conditioner adopting the Fisher Metric and use reshaping\nS = Shrink(PosDefManifold.Fisher; reshape = true)\n\nSee also: fit!, transform!, crval\n\n\n\n\n\n","category":"type"},{"location":"conditioners/#PosDefManifoldML.Pipeline","page":"Conditioners & Pipelines","title":"PosDefManifoldML.Pipeline","text":"Pipeline is a type for tuples holding conditioners.\n\nA pipeline holds a sequence of conditioners learned and (optionally) applied using fit!. It can be  subsequently applied on other data as it has been learnt  using the transform! function. All fit! methods return a pipeline.\n\nPipelines comprising a single conditioner are allowed.\n\nPipelines can be saved to a file using the saveas function and loaded from a file using the load function.\n\nNote that in Julia tuples are immutable, thus it is not possible to modify a pipeline. However it is possible to change the fields of the conditioners  it holds.\n\nIn order to create a pipeline use the @pipeline macro.\n\nSee also: fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"conditioners/#PosDefManifoldML.@pipeline","page":"Conditioners & Pipelines","title":"PosDefManifoldML.@pipeline","text":"macro pipeline(args...)\n\nCreate a Pipeline chaining the provided expressions.\n\nAs an example, the sintax is:\n\np = @pipeline Recenter() â†’ Compress â†’ Shrink(Fisher; threaded=false)\n\nNote that:\n\nThe symbol â†’ (escape \"\\to\") separating the conditioners is optional.\nThis macro has alias @â†’. \nAs in the example above, expressions may be instantiated conditioners, like Recenter(), or their type, like Compress, in which case the default conditioner of that type is created.\n\nThe example above is thus equivalent to\n\n    p = @â†’ Recenter() Compress() Shrink(Fisher; threaded=false)\n\nConditioners are not callable by the macro. Thus if you want to pass a variable, do not write\n\n    R = Recenter()\n    p = @â†’ R\n\nbut \n\n    R = Recenter()\n    p = @â†’ eval(R)\n\nAvailable conditioners to form pipelines\n\nTikhonov, Recenter, Compress, Equalize, Shrink\n\nSee also: fit!, transform!\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\nP=randP(3, 5)\npipeline = fit!(P, @â†’ Recenter â†’ Compress)\n\n\n\n\n\n","category":"macro"},{"location":"conditioners/#StatsAPI.fit!","page":"Conditioners & Pipelines","title":"StatsAPI.fit!","text":"    function fit!(ð :: â„Vector, pipeline :: Union{Pipeline, Conditioner}; \n        transform = true,\n        labels = nothing)\n\nFit the given Pipeline (or a single Conditioner) to ð and return  a fitted Pipeline object. ð must be of the â„Vector type.\n\nA single Conditioner can be given as argument instead of a pipeline; a fitted pipeline with a single element will be returned. The type of the conditioner can be gives as well, in which case the default conditioner will be used - see examples below.\n\nIf pipeline in an empty tuple, return an empty pipeline without doing anything.\n\nif transform is true (default), ð is transformed (in-place), otherwise the pipeline is fitted but ð is not transformed.\n\nIf labels is a vector of integers holding the class labels of the points  in ð, the conditioners are supervised (i.e., labels-aware),  otherwise, if it is nothing (default), they are unsupervised.  Currently the only conditioners that can behave in a supervised manner is Recenter. When supervised, the barycenter for recentering is computed given balanced weights to each class, like tsWeights does for computing the barycenter used for tangent space mapping. If the classes are balanced, the weighting has no effect.\n\nThe returned pipeline can be used as argument for the  transform! function, ensuring that the fitted parameters are properly applied. It can also be saved to a file using the saveas function and loaded from a file using the load function.\n\nNote that the pipeline given as argument is not modified.\n\nLearning parameters during fit\n\nFor some of the conditioners there is no parameter to be learnt during training.  For those, a call to the fit! function is equivalent to a call to the transform! function, with the exception that when the fit! function is called the parameters used for the tranformation are stored in the returned pipeline.\n\nSee also: transform!\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n## Example 1 (single conditioner): \n\n# Generate some data\nP=randP(3, 5) # 5 random 3x3 Hermitian matrices\nQ=copy(P)\n\n# Fit the default recentering conditioner (whitening)\npipeline = fit!(P, Recenter) \n\n# This is equivalent to\npipeline = fit!(Q, Recenter())\n\npipeline[1].Z # a learnt parameter (whitening matrix)\n\n## Example 2 (pipeline): \n\n# Fit a pipeline comprising Tikhonov regularization, \n# recentering, compressing and shrinking according to the Fisher metric.\n# The matrices in P will be first regularized, then recentered, \n# then compressed and finally shrunk.\n\nP=randP(3, 5)  \nQ=copy(P)\n\npipeline = fit!(P, \n        @â†’ Tikhonov(0.0001) â†’ Recenter â†’ Compress â†’ Shrink(Fisher; radius=0.01))\n\n# or \npipeline = fit!(Q, @â†’ Recenter Compress Shrink(Fisher; radius=0.01))\n\n# The whitening matrices of the the recentering conditioner,\npipeline[1].Z\n\n# The scaling factors of the compressing conditioner,\npipeline[2].Î²\n\n# and the step-size of the shrinking conditioner\npipeline[3]\n\n## Example 3 (pipeline with a single conditioner):\nP=randP(3, 5)  \npipeline = fit!(P, @â†’ Recenter)\n\n\n\n\n\n","category":"function"},{"location":"conditioners/#DataFrames.transform!","page":"Conditioners & Pipelines","title":"DataFrames.transform!","text":"function transform!(ð :: Union{â„Vector, â„}, pipeline :: Union{Pipeline, Conditioner})\n\n\nGiven a fitted Pipeline (or a single Conditioner), transform all matrices  in ð using the parameters learnt during the fitting process. Return ð.\n\nIn a training-test setting, a fitted conditioner or pipeline is given as argument  to this function to make sure that the testing data is transformed according to  the parameters learnt during the fitting of training data. More in general, this function can be used to transform in whatever way the data in ð.\n\nIf pipeline in an empty tuple, return ð without doing anything.\n\nð can be a single Hermitian matrix or a vector of the â„Vector type. It is transformed in-place.\n\nwarning: Dimension\nThe dimension of matrix(ces) in ð must be the same of the dimension of the matrices  used to fit the conditioner or pipeline.\n\nIn contrast to the fit! function, only instantiated conditioner can be used. For general use, this is transparent to the user as the fit! function always returns pipelines with instantiated conditioners.\n\nSee: fit!\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n## Example 1 (single conditioner)\n\n# Generate some 'training' and 'testing' data\nPTr=randP(3, 20) # 20 random 3x3 Hermitian matrices\nPTe=randP(3, 5) # 5 random 3x3 Hermitian matrices\n\n# Fit the default recentering conditioner (whitening)\n# Matrices in PTr will be transformed (recentered)\nR = fit!(PTr, Recenter()) \n\n# Transform PTe using recentering as above\n# using the parameters for recentering learnt\n# on PTr during the fitting process. \ntransform!(PTe, R)\n\nmean(PTr)-I # Should be close to the zero matrix.\nmean(PTe)-I # Should not be close to the zero matrix\n# as the recentering parameter is learnt on PTr, not on PTe.\n\n## Example 2 (pipeline)\n\n# Generate some 'training' and 'testing' data\nPTr=randP(3, 20) # 20 random 3x3 Hermitian matrices\nPTe=randP(3, 5) # 5 random 3x3 Hermitian matrices\nQTr=copy(PTr)\nQTe=copy(PTe)\n\np = @â†’ Tikhonov(0.0002) Recenter(; eVar=0.99) Compress Shrink(Fisher; radius=0.01)\npipeline = fit!(QTr, p)\ntransform!(QTe, pipeline)\n\n## Example 3 (pipeline with a single conditioner):\nP=randP(3, 5)  \n# For the Equalize conditioner there is no need to fit some data\ntransform!(P, @â†’ Equalize)\n# This gives an error as Recenter needs to learn parameters (use fit! instead):\ntransform!(P, @â†’ Recenter)\n\n\n\n\n\n","category":"function"},{"location":"conditioners/#PosDefManifoldML.pickfirst","page":"Conditioners & Pipelines","title":"PosDefManifoldML.pickfirst","text":"function pickfirst(pipeline, conditioner)\n\nReturn a copy of the first conditioner of the pipeline which is of  the same type as conditioner. If no such conditioner is found, return nothing. Note that a copy is returned, not the conditioner in the pipeline itself.\n\nThe provided conditioner can be a type or an instance of a conditioner. The returned element will always be an instance, as pipelines holds instances only.\n\nSee: includes\n\nExamples\n\nusing PosDefManifoldML\n\npipeline = @â†’ Recenter() Shrink()\nS = pickfirst(pipeline, Shrink) \nS isa Conditioner # true\nS isa Shrink # true\n\n# retrive a parameter of the conditioner\npickfirst(pipeline, Shrink).radius\n\n\n\n\n\n\n","category":"function"},{"location":"conditioners/#PosDefManifoldML.includes","page":"Conditioners & Pipelines","title":"PosDefManifoldML.includes","text":"function includes(pipeline, conditioner)\n\nReturn true if the given Pipeline includes a conditioner  of the same type as conditioner.\n\nThe provided conditioner can be a type or an instance of a conditioner.\n\nSee: pickfirst, @pipeline\n\nExamples\n\nusing PosDefManifoldML\n\npipeline= @â†’ Recenter() â†’ Shrink()\n\nincludes(pipeline, Shrink) # true\n\nincludes(pipeline, Shrink()) # true\n\n# same type, althoug a different instance\nincludes(pipeline, Shrink(Fisher; radius=0.1)) # true\n\nincludes(pipeline, Compress) # false\n\nLearn the package: check out saveas\n\n\n\n\n\n","category":"function"},{"location":"conditioners/#PosDefManifold.dim","page":"Conditioners & Pipelines","title":"PosDefManifold.dim","text":"function dim(pipeline::Pipeline)\n\nReturn the dimension determined by a fitted Recenter pre-conditioner  if the pipeline comprises such a pre-conditioner, nothing otherwise.  This is used to adapt pipelines - see the documentation of the fit! function for ENLR machine learning models for an example.\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\npipeline = @â†’ Recenter(; eVar=0.9) â†’ Shrink()\ndim(pipeline) # return false, as it is not fitted\n\nP = randP(10, 5)\np = fit!(P, pipeline)\ndim(p) # return an integer â‰¤ 10\n\nLearn the package: check out @pipeline\n\n\n\n\n\n","category":"function"}]
}
