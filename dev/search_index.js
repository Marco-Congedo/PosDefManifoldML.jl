var documenterSearchIndex = {"docs":
[{"location":"mdm/#mdm.jl-1","page":"Minimum Distance to Mean","title":"mdm.jl","text":"","category":"section"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"This unit implements the Riemannian MDM (Minimum Distance to Mean) classifier for the manifold of positive definite (PD) matrices, both real (symmetric PD) or complex (Hermitian PD) matrices. The MDM is a simple, yet efficient, deterministic and paramater-free classifier acting directly on the manifold of positive definite matrices (Barachat el al., 2012; Congedo et al., 2017a ðŸŽ“): given a number of PD matrices representing class means, the MDM classify an unknown datum (also a PD matrix) as belonging to the class whose mean is the closest to the datum. The process is illustrated in the upper part of this figure.","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"The MDM classifier involves only the concepts of a distance function for two PD matrices and a mean (center of mass) for a number of them. Those are defined for any given metric, a Metric enumerated type declared in PosDefManifold.","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"Currently supported metrics are:","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"metric (distance) mean estimation known also as\nEuclidean arithmetic \ninvEuclidean harmonic \nChoEuclidean Cholesky Euclidean \nlogEuclidean log-Euclidean \nlogCholesky log-Cholesky \nFisher Fisher Cartan, Karcher, Pusz-Woronowicz, Affine-Invariant, ...\nlogdet0 logDet S, Î±, Bhattacharyya, Jensen, ...\nJeffrey Jeffrey symmetrized Kullback-Leibler\nWasserstein Wasserstein Bures, Hellinger, optimal transport, ...","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"Do not use the Von Neumann metric, which is also supported in PosDefManifold, since it does not allow a definition of mean. See here for details on the metrics.","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"The fit and predict functions for the MDM models are reported in the cv.jl unit. Here it is reported the MDMmodel abstract type, the MDM structure and the following functions, which typically you will not need to access directly, but are nonetheless provided to facilitate low-level operations with MDM classifiers:","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"function description\ngetMean compute the mean of positive definite matrices for fitting the MDM model\ngetDistances compute the distances of a matrix set to a set of means","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"MDMmodel\nMDM\ngetMean\ngetDistances","category":"page"},{"location":"mdm/#PosDefManifoldML.MDMmodel","page":"Minimum Distance to Mean","title":"PosDefManifoldML.MDMmodel","text":"Abstract type for MDM (Minimum Distance to Mean) machine learning models\n\n\n\n\n\n","category":"type"},{"location":"mdm/#PosDefManifoldML.MDM","page":"Minimum Distance to Mean","title":"PosDefManifoldML.MDM","text":"mutable struct MDM <: MDMmodel\n    metric  :: Metric = Fisher;\n    featDim :: Int\n    means   :: â„Vector\nend\n\nMDM machine learning models are incapsulated in this mutable structure. MDM models have three fields: .metric, .featDim and .means.\n\nThe field metric, of type Metric, is to be specified by the user. It is the metric that will be adopted to compute the class means and the distances to the mean.\n\nThe field featDim is the dimension of the manifold in which the model acts. This is given by n(n+1)2, where n is the dimension of the PD matrices. This field is not to be specified by the user, instead, it is computed when the MDM model is fit using the fit function and is accessible only thereafter.\n\nThe field means is an â„Vector holding the class means, i.e., one mean for each class. This field is not to be specified by the user, instead, the means are computed when the MDM model is fit using the fit function and are accessible only thereafter.\n\nExamples:\n\nusing PosDefManifoldML\n\n# create an empty model\nm = MDM(Fisher)\n\n# since the Fisher metric is the default metric,\n# this is equivalent to\nm = MDM()\n\nNote that in general you need to invoke these constructors only when an MDM model is needed as an argument to a function, otherwise you can more simply create and fit an MDM model using the fit function.\n\n\n\n\n\n","category":"type"},{"location":"mdm/#PosDefManifoldML.getMean","page":"Minimum Distance to Mean","title":"PosDefManifoldML.getMean","text":"function getMean(metric :: Metric,\n                 ð      :: â„Vector;\n              tol :: Real   = 0.,\n              w   :: Vector = [],\n              âœ“w :: Bool    = true,\n              â© :: Bool    = true)\n\nTypically, you will not need this function as it is called by the fit function.\n\nGiven a metric of type Metric, an â„Vector of Hermitian matrices ð and an optional non-negative real weights vector w, return the (weighted) mean of the matrices in ð. This is used to fit MDM models.\n\nThis function calls the appropriate mean functions of package PostDefManifold, depending on the chosen metric, and check that, if the mean is found by an iterative algorithm, then the iterative algorithm converges.\n\nSee method (3) of the mean function for the meaning of the optional keyword arguments w, âœ“w and â©, to which they are passed.\n\nThe returned mean is flagged by Julia as an Hermitian matrix (see LinearAlgebra).\n\n\n\n\n\n","category":"function"},{"location":"mdm/#PosDefManifoldML.getDistances","page":"Minimum Distance to Mean","title":"PosDefManifoldML.getDistances","text":"function getDistances(metric :: Metric,\n                      means  :: â„Vector,\n                      ð      :: â„Vector;\n                  â© :: Bool = true)\n\nTypically, you will not need this function as it is called by the predict function.\n\nGiven an â„Vector ð holding k Hermitian matrices and an â„Vector means holding z matrix means, return the square of the distance of each matrix in ð to the means in means.\n\nThe squared distance is computed according to the chosen metric, of type Metric. See metrics for details on the supported distance functions.\n\nIf â© is true, the distances are computed using multi-threading, unless the number of threads Julia is instructed to use is <2 or <3k.\n\nThe result is a zxk matrix of squared distances.\n\n\n\n\n\n","category":"function"},{"location":"cv/#cv.jl-1","page":"fit, predict, cv","title":"cv.jl","text":"","category":"section"},{"location":"cv/#","page":"fit, predict, cv","title":"fit, predict, cv","text":"This unit implements cross-validation procedures for estimating the accuracy and balanced accuracy of machine learning models. It also reports the documentation of the fit and predict functions, as they are common to all models.","category":"page"},{"location":"cv/#","page":"fit, predict, cv","title":"fit, predict, cv","text":"Content","category":"page"},{"location":"cv/#","page":"fit, predict, cv","title":"fit, predict, cv","text":"struct description\nCVacc encapsulate the result of cross-validation procedures for estimating accuracy","category":"page"},{"location":"cv/#","page":"fit, predict, cv","title":"fit, predict, cv","text":"function description\nfit fit a model with training data, or create and fit it\npredict preidct labels, probabilities or scoring functions on test data\ncvAcc estimate accuracy of a model by cross-validation\ncvSetup generate indexes for performing cross-validtions","category":"page"},{"location":"cv/#","page":"fit, predict, cv","title":"fit, predict, cv","text":"CVacc\nfit\npredict\ncvAcc\ncvSetup","category":"page"},{"location":"cv/#PosDefManifoldML.CVacc","page":"fit, predict, cv","title":"PosDefManifoldML.CVacc","text":"struct CVacc\n    cvType  :: String\n    scoring :: Union{String, Nothing}\n    model   :: Union{MLmodel, Nothing}\n    cnfs    :: Union{Vector{Matrix{T}}, Nothing} where T<:Real\n    avgCnf  :: Union{Matrix{T}, Nothing} where T<:Real\n    accs    :: Union{Vector{T}, Nothing} where T<:Real\n    avgAcc  :: Union{Real, Nothing}\n    stdAcc  :: Union{Real, Nothing}\nend\n\nA call to cvAcc results in an instance of this structure. Fields:\n\n.cvTpe is the type of cross-validation technique, given as a string (e.g., \"10-kfold\")\n\n.scoring is the type of accuracy that is computed, given as a string. This has been passed as argument to cvAcc. Currently accuracy and balanced accuracy are supported.\n\n.model is the machine learning model that has been passed as argument to cvAcc, e.g., an MDMmodel or an ENLRmodel.\n\n.cnfs is a vector of matrices holding the confusion matrices obtained at each fold of the cross-validation.\n\n.avgCnf is the average confusion matrix across the folds of the cross-validation.\n\n.accs is a vector of real numbers holding the accuracies obtained at each fold of the cross-validation.\n\n.avgAcc is the average accuracy across the folds of the cross-validation.\n\n.stdAcc is the standard deviation of the accuracy across the folds of the cross-validation.\n\n\n\n\n\n","category":"type"},{"location":"cv/#StatsBase.fit","page":"fit, predict, cv","title":"StatsBase.fit","text":"function fit(model :: MDMmodel,\n              ðTr   :: â„Vector,\n              yTr   :: Vector;\n           w       :: Vector = [],\n           âœ“w      :: Bool  = true,\n           verbose :: Bool  = true,\n           â©      :: Bool  = true)\n\nFit an MDM machine learning model, with training data ðTr, of type â„Vector, and corresponding labels yTr, of type IntVector. Return the fitted model.\n\nFitting an MDM model involves only computing a mean of all the matrices in each class. Those class means are computed according to the metric specified by the MDM constructor.\n\nSee method (3) of the mean function for the meaning of the optional keyword arguments w, âœ“w and â©, to which they are passed. Keep in mind that here the weights should sum up to 1 separatedly for each class, which is what is ensured if âœ“w is true.\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: predict, cvAcc.\n\nExamples\n\nusing PosDefManifoldML\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.25)\n\n# create and fit a model:\nm=fit(MDM(Fisher), PTr, yTr)\n\n\n\n\n\nfunction fit(model :: ENLRmodel,\n               ðTr :: Union{â„Vector, Matrix{Float64}},\n               yTr :: Vector;\n           w       :: Vector            = [],\n           meanISR :: Union{â„, Nothing} = nothing,\n           fitType :: Symbol            = :best,\n           verbose :: Bool              = true,\n           â©     :: Bool               = true,\n                # arguments for `GLMNet.glmnet` function\n           alpha            :: Real = model.alpha < 1.0 ? model.alpha : 1.0,\n           intercept        :: Bool = true,\n           standardize      :: Bool = alphaâ‰ˆ1.0 ? true : false,\n           penalty_factor   :: Vector{Float64} = ones(_triNum(ðTr[1])),\n           constraints      :: Matrix{Float64} = [x for x in (-Inf, Inf), y in 1:_triNum(ðTr[1])],\n           offsets          :: Union{Vector{Float64}, Nothing} = nothing,\n           dfmax            :: Int = _triNum(ðTr[1]),\n           pmax             :: Int = min(dfmax*2+20, _triNum(ðTr[1])),\n           nlambda          :: Int = 100,\n           lambda_min_ratio :: Real = (length(yTr) < _triNum(ðTr[1]) ? 1e-2 : 1e-4),\n           lambda           :: Vector{Float64} = Float64[],\n           tol              :: Real = 1e-7,\n           maxit            :: Int = 1000000,\n           algorithm        :: Symbol = :newtonraphson,\n                # selection method\n           Î»SelMeth :: Symbol = :sd1,\n                # arguments for `GLMNet.glmnetcv` function\n           nfolds   :: Int = min(10, div(size(yTr, 1), 3)),\n           folds    :: Vector{Int} =\n           begin\n               n, r = divrem(size(yTr, 1), nfolds)\n               shuffle!([repeat(1:nfolds, outer=n); 1:r])\n           end,\n           parallel ::Bool=false)\n\n\nCreate and fit an ENLR machine learning model, with training data ðTr, of type â„Vector, and corresponding labels yTr, of type IntVector. Return the fitted model.\n\nAs for all ML models acting in the tangent space, fitting an ENLR model involves computing a mean of all the matrices in ðTr, mapping all matrices onto the tangent space after parallel transporting them at the identity matrix and vectorizing them using the vecP operation. Once this is done, the elastic net logistic regression is fitted.\n\nThe mean is computed according to the .metric field of the model, with optional weights w. If weights are used, they should be inversely proportional to the number of examples for each class, in such a way that each class contributes equally to the computation of the mean. The .metric field of the model is passed to the tsMap function. By default the metric is the Fisher metric. See the examples here below to see how to change metric. See mdm.jl for the available metrics.\n\nIf meanISR is passed as argument, the mean is not computed, instead this matrix is the inverse square root (ISR) of the mean used for projecting the matrices in the tangent space (see tsMap).\n\nIf fitType = :best (default), a cross-validation procedure is run to find the best lambda hyperparameter for the training data. This finds a single model that is written into the .best field of the model that will be created.\n\nIf fitType = :path, the regularization path for several values of the lambda hyperparameter if found for the training data. This creates several models, which are written into the .path field of the model that will be created, none of which is optimal, in the cross-validation sense, for the training data.\n\nIf fitType = :all, both the above fits are performed and all fields of the model that will be created will be filled in.\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nThe â© argument (true by default) is passed to the tsMap function for projecting the matrices in ðTr onto the tangent space using multi-threading.\n\nThe remaining optional keyword arguments, are\n\nthe arguments passed to the GLMNet.glmnet function for fitting the models. Those are always used.\nthe Î»SelMeth argument and the arguments passed to the GLMNet.glmnetcv function for finding the best lambda hyperparamater by cross-validation. Those are used only if fitType = :path or = :all.\n\nOptional Keyword arguments for fitting the model(s)\n\nweights: a vector of weights for each matrix of the same size as yTr. Argument w is passed, defaulting to 1 for all matrices.\n\nalpha: the hyperparameter in 0 1 to trade-off an elestic-net model. Î±=0 requests a pure ridge model and Î±=1 a pure lasso model. This defaults to 1.0, which specifies a lasso model.\n\nintercept: whether to fit an intercept term. The intercept is always unpenalized. Defaults to true.\n\nstandardize: whether to standardize predictors so that they are in the same units. Differently from GLMNet.jl, by default this is true for lasso models (Î±=1), false otherwise (0Î±1).\n\npenalty_factor: a vector of length n(n+1)2, where n is the dimension of the original PD matrices on which the model is applied, of penalties for each predictor in the tangent vectors. This defaults to all ones, which weights each predictor equally. To specify that a predictor should be unpenalized, set the corresponding entry to zero.\n\nconstraints: an n(n+1)2 x 2 matrix specifying lower bounds (first column) and upper bounds (second column) on each predictor. By default, this is [-Inf Inf] for each predictor (each element of tangent vectors).\n\ndfmax: The maximum number of predictors in the largest model.\n\npmax: The maximum number of predictors in any model.\n\nnlambda: The number of values of Î» along the path to consider.\n\nlambda_min_ratio: The smallest Î» value to consider, as a ratio of the value of Î» that gives the null model (i.e., the model with only an intercept). If the number of observations exceeds the number of variables, this defaults to 0.0001, otherwise 0.01.\n\nlambda: The Î» values to consider. By default, this is determined from nlambda and lambda_min_ratio.\n\ntol: Convergence criterion. Defaults to 1e-7.\n\nmaxit: The maximum number of iterations of the cyclic coordinate descent algorithm. If convergence is not achieved, a warning is returned.\n\nalgorithm: the algorithm used to find the regularization path. Possible values are :newtonraphson (default) and :modifiednewtonraphson.\n\nFor further informations on those arguments, refer to the resources on the GLMNet package ðŸŽ“.\n\nOptional Keyword arguments for finding the best model by cv\n\nÎ»SelMeth = :sd1 (default), the best model is defined as the one allowing the highest cvÎ».meanloss within one standard deviation of the minimum, otherwise it is defined as the one allowing the minimum cvÎ».meanloss. Note that in selecting a model, the model with only the intercept term, if it exists, is ignored. See ENLRmodel for a description of the .cvÎ» field of the model structure.\n\nArguments nfolds, folds and parallel are passed to the GLMNet.glmnetcv function. Please refer to the resources on GLMNet for details ðŸŽ“.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: predict, cvAcc.\n\nExamples\n\nusing PosDefManifoldML\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1)\n\n# Fit an ENLR lasso model and find the best model by cross-validation:\nm=fit(ENLR(), PTr, yTr)\n\n# ... using the log-Eucidean metric for tangent space projection\nm=fit(ENLR(logEuclidean), PTr, yTr)\n\n# Fit an ENLR ridge model and find the best model by cv:\nm=fit(ENLR(Fisher), PTr, yTr; alpha=0)\n\n# Fit an ENLR elastic-net model (Î±=0.9) and find the best model by cv:\nm=fit(ENLR(Fisher), PTr, yTr; alpha=0.9)\n\n# Fit an ENLR lasso model and its regularization path:\nm=fit(ENLR(), PTr, yTr; fitType=:path)\n\n# Fit an ENLR lasso model, its regularization path\n# and the best model found by cv:\nm=fit(ENLR(), PTr, yTr; fitType=:all)\n\n\n\n\n\n\n","category":"function"},{"location":"cv/#GLMNet.predict","page":"fit, predict, cv","title":"GLMNet.predict","text":"function predict(model  :: MDMmodel,\n                 ðTe    :: â„Vector,\n                 what   :: Symbol = :labels;\n               verbose :: Bool = true,\n               â©     :: Bool = true)\n\nGiven an MDM model trained (fitted) on z classes and a testing set of k positive definite matrices ðTe of type â„Vector,\n\nif what is :labels or :l (default), return the predicted class labels for each matrix in ðTe, as an IntVector. For MDM models, the predicted class 'label' of an unlabeled matrix is the serial number of the class whose mean is the closest to the matrix (minimum distance to mean). The labels are '1' for class 1, '2' for class 2, etc;\n\nif what is :probabilities or :p, return the predicted probabilities for each matrix in ðTe to belong to a all classes, as a k-vector of z vectors holding reals in 0 1 (probabilities). The 'probabilities' are obtained passing to a softmax function minus the squared distances of each unlabeled matrix to all class means;\n\nif what is :f or :functions, return the output function of the model. The ratio of the squared distance to all classes to their geometric mean gives the 'functions'.\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nIt f â© is true (default), the computation of distances is multi-threaded.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: fit, cvAcc, predictErr.\n\nExamples\n\nusing PosDefManifoldML\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)\n\n# craete and fit an MDM model\nm=fit(MDM(Fisher), PTr, yTr)\n\n# predict labels\nyPred=predict(m, PTe, :l)\n\n# prediction error\npredErr=predictErr(yTe, yPred)\n\n# predict probabilities\npredict(m, PTe, :p)\n\n# output functions\npredict(m, PTe, :f)\n\n\n\n\n\n\nfunction predict(model   :: ENLRmodel,\n                 ðTe     :: Union{â„Vector, Matrix{Float64}},\n                 what    :: Symbol = :labels,\n                 fitType :: Symbol = :best,\n                 onWhich :: Int    = Int(fitType==:best);\n            checks  :: Bool = true,\n            verbose :: Bool = true,\n            â©     :: Bool = true)\n\nGiven an ENLR model trained (fitted) on 2 classes and a testing set of k positive definite matrices ðTe of type â„Vector,\n\nif what is :labels or :l (default), return the predicted class labels for each matrix in ðTe, as an IntVector. Those labels are '1' for class 1 and '2' for class 2;\n\nif what is :probabilities or :p, return the predicted probabilities for each matrix in ðTe to belong to a all classes, as a k-vector of z vectors holding reals in 0 1 (probabilities). The 'probabilities' are obtained passing to a softmax function the output of the ENLR model and zero;\n\nif what is :f or :functions, return the output function of the model, which is the raw output of the ENLR model.\n\nIf fitType = :best (default), the best model that has been found by cross-validation is used for prediction.\n\nIf fitType = :path (default),\n\nif onWhich is a valid serial number for a model in the model.path, this model is used for prediction,\nif onWhich is zero, all model in the model.path will be used for predictions, thus the output will be multiplied by the number of models in model.path.\n\nArgumet onWhich has no effect if fitType = :best.\n\nnote: Nota Bene\nBy default, the fit function fits only the best model. If you want to use the fitType = :path option you need to invoke the fit function with optional keyword argument fitType=:path or fitType=:all. See the fit function for details.\n\nIf checks is true (default), checks on the validity of the arguments are performed. This can be set to false to spped up computations.\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nIf â© = true (default) and ðTe is an â„Vector type, the projection onto the tangent space is multi-threaded.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: fit, cvAcc, predictErr.\n\nExamples\n\nusing PosDefManifoldML\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)\n\n# fit an ENLR lasso model and find the best model by cv\nm=fit(ENLR(Fisher), PTr, yTr)\n\n# predict labels from the best model\nyPred=predict(m, PTe, :l)\n# prediction error\npredErr=predictErr(yTe, yPred)\n\n# predict probabilities from the best model\npredict(m, PTe, :p)\n\n# output functions from the best model\npredict(m, PTe, :f)\n\n# fit a regularization path for an ENLR lasso model\nm=fit(ENLR(Fisher), PTr, yTr; fitType=:path)\n\n# predict labels using a specific model\nyPred=predict(m, PTe, :l, :path, 10)\n\n# predict labels for all models\nyPred=predict(m, PTe, :l, :path, 0)\n# prediction error for all models\npredErr=[predictErr(yTe, yPred[:, i]) for i=1:size(yPred, 2)]\n\n# predict probabilities from a specific model\npredict(m, PTe, :p, :path, 12)\n\n# predict probabilities from all models\npredict(m, PTe, :p, :path, 0)\n\n# output functions from specific model\npredict(m, PTe, :f, :path, 3)\n\n# output functions for all models\npredict(m, PTe, :f, :path, 0)\n\n\n\n\n\n\n","category":"function"},{"location":"cv/#PosDefManifoldML.cvAcc","page":"fit, predict, cv","title":"PosDefManifoldML.cvAcc","text":"function cvAcc(model   :: MLmodel,\n               ðTr     :: â„Vector,\n               yTr     :: IntVector,\n               nCV     :: Int;\n           scoring   :: Symbol = :b,\n           shuffle   :: Bool   = false,\n           verbose   :: Bool   = true,\n           fitArgs...)\n\nCross-validation accuracy for a machine learning model: given an â„Vector ðTr holding k Hermitian matrices, an IntVector yTr holding the k labels for these matrices and the number of cross-validations nCV, return a CVacc structure.\n\nIf scoring=:b (default) the balanced accuracy is computed. Any other value will make the function returning the regular accuracy. Balanced accuracy is to be preferred for unbalanced classes. In any case, for balanced classes the balanced accuracy reduces to the regular accuracy, therefore there is no point in using regular accuracy.\n\nFor the meaning of the shuffle argument (false by default), see function cvSetup, to which this argument is passed.\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nfitArgs are optional keyword arguents that are passed to the fit function within this function.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: fit, predict.\n\nExamples\n\nusing PosDefManifoldML\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)\n\n# perform cross-validation using the minimum distance to mean classifier\ncv=cvAcc(MDM(Fisher), PTr, yTr, 10)\n\n# ...using the lasso logistic regression classifier\ncv=cvAcc(ENLR(Fisher), PTr, yTr, 10)\n\n# ...using the elastic-net logistic regression (Î±=0.9) classifier\ncv=cvAcc(ENLR(Fisher), PTr, yTr, 10; alpha=0.9)\n\n# perform another cross-validation\ncv=cvAcc(MDM(Fisher), PTr, yTr, 10; shuffle=true)\n\n\n\n\n\n\n\n","category":"function"},{"location":"cv/#PosDefManifoldML.cvSetup","page":"fit, predict, cv","title":"PosDefManifoldML.cvSetup","text":"function cvSetup(k       :: Int,\n                 nCV     :: Int;\n                 shuffle :: Bool = false)\n\nGiven k elements and a parameter nCV, a nCV-fold cross-validation is obtained defining nCV permutations of k elements in nTest=knCV (integer division) elements for the test and k-nTest elements for the training, in such a way that each element is represented in only one permutation.\n\nSaid differently, given a length k and the number of desired cross-validations nCV, this function generates indices from the sequence of natural numbers 1k to obtain all nCV-fold cross-validation sets. Specifically, it generates nCV vectors of indices for generating test sets and nCV vectors of indices for geerating training sets.\n\nIf optional keyword argument shuffle is true, the sequence of natural numbers 1k is shuffled before running the function, thus in this case two successive runs of this function will give different cross-validation sets, hence different accuracy scores. By default shuffle is false, so as to allow exactly the same result in successive runs. Note that no random initialization for the shuffling is provided, so as to allow the replication of the same random sequences starting again the random generation from scratch.\n\nThis function is used in cvAcc. It constitutes the fundamental basis to implement customized cross-validation procedures.\n\nReturn the 2-tuple with:\n\nA vector of nCV vectors holding the indices for the training sets,\nA vector of nCV vectors holding the indices for the corresponding test sets.\n\nExamples\n\nusing PosDefManifoldML\n\ncvSetup(10, 2)\n# return:\n# (Array{Int64,1}[[6, 7, 8, 9, 10], [1, 2, 3, 4, 5]],\n#  Array{Int64,1}[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n\ncvSetup(10, 2, shuffle=true)\n# return:\n# (Array{Int64,1}[[5, 4, 6, 1, 9], [3, 7, 8, 2, 10]],\n#  Array{Int64,1}[[3, 7, 8, 2, 10], [5, 4, 6, 1, 9]])\n\ncvSetup(10, 3)\n# return:\n# (Array{Int64,1}[[4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6]],\n#  Array{Int64,1}[[1, 2, 3], [4, 5, 6], [7, 8, 9, 10]])\n\n\n\n\n\n\n","category":"function"},{"location":"enlr/#enlr.jl-1","page":"Elastic-Net Logistic Regression","title":"enlr.jl","text":"","category":"section"},{"location":"enlr/#","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"This unit implements the elastic net logistic regression (ENLR) machine learning model on the tangent space for symmetric positive definite (SDP) matrices, i.e., real PD matrices. This model features two hyperparameters: a user-defined alpha hyperparameter, in range 0 1, where Î±=0 allows a pure Ridge LR model and Î±=1 a pure lasso LR model and the lambda (regularization) hyperparameter. When the model is fitted, we can request to find the optimal lambda hyperparameter for the given training data using cross-validation and/or to find the regularization path.","category":"page"},{"location":"enlr/#","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"The lasso model (default) has enjoyed popularity in the field of brain-computer interaces due to the winning score obtained in six international data classification competitions.","category":"page"},{"location":"enlr/#","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"The ENLR model is implemented using the Julia GLMNet.jl package. See ðŸŽ“ for resources on GLMNet.jl and learn how to use purposefully this model.","category":"page"},{"location":"enlr/#","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"The fit and predict functions for the ENRL models are reported in the cv.jl unit, since those are shared by all machine learning models. Here it is reported the ENLRmodel abstract type and the ENLR structure.","category":"page"},{"location":"enlr/#","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"ENLRmodel\nENLR","category":"page"},{"location":"enlr/#PosDefManifoldML.ENLRmodel","page":"Elastic-Net Logistic Regression","title":"PosDefManifoldML.ENLRmodel","text":"abstract type ENLRmodel<:TSmodel end\n\nAbstract type for Elastic Net Logistic Rgression (ENLR) machine learning models. See MLmodel.\n\n\n\n\n\n","category":"type"},{"location":"enlr/#PosDefManifoldML.ENLR","page":"Elastic-Net Logistic Regression","title":"PosDefManifoldML.ENLR","text":"mutable struct ENLR <: ENLRmodel\n    metric      :: Metric = Fisher;\n    alpha       :: Real\n    standardize :: Bool\n    intercept   :: Bool\n    meanISR     :: Union{â„Vector, Nothing}\n    featDim     :: Int\n    path        :: GLMNet.GLMNetPath\n    cvÎ»         :: GLMNet.GLMNetCrossValidation\n    best        :: GLMNet.GLMNetPath\nend\n\nENLR machine learning models are incapsulated in this mutable structure. Fields:\n\n.metric, of type Metric, is to be specified by the user. It is the metric that will be adopted to compute the mean used as base-point for tangent space projection.\n\nAll other fields do not correspond to arguments passed upon creation of the model. Instead, they are filled later by the fit function:\n\n.alpha is the hyperparameter in 0 1 trading-off the elestic-net model. Î±=0 requests a pure ridge model and Î±=1 a pure lasso model. This is passed as parameter to the fit function, defaulting therein to Î±=1.\n\n.standardize. If true, predictors are standardized so that they are in the same units. By default is true for lasso models (Î±=1), false otherwise (0Î±1).\n\n.intercept is true (default) if the logistic regression model has an intercept term.\n\n.meanISR is optionally passed to the fit function. By default it is computed thereby.\n\n.featDim is the length of the vectorized tangent vectors. This is given by n(n+1)2, where n is the dimension of the original PD matrices on which the model is applied once they are mapped onto the tangent space.\n\n.path is an instance of the following GLMNetPath structure of the GLMNet.jl package. It holds the regularization path that is created when the fit function is invoked with optional keyword parameter fitType = :path or = :all:\n\nstruct GLMNetPath{F<:Distribution}\n    family::F                        # Binomial()\n    a0::Vector{Float64}              # intercept values for each solution\n    betas::CompressedPredictorMatrix # coefficient values for each solution\n    null_dev::Float64                # Null deviance of the model\n    dev_ratio::Vector{Float64}       # R^2 values for each solution\n    lambda::Vector{Float64}          # lambda values for each solution\n    npasses::Int                     # actual number of passes over the\n                                     # data for all lamda values\nend\n\n.cvÎ» is an instance of the following GLMNetCrossValidation structure of the GLMNet.jl package. It holds information about the cross-validation used for estimating the optimal lambda hyperparameter by the fit function when this is invoked with optional keyword parameter fitType = :best (default) or = :all:\n\nstruct GLMNetCrossValidation\n    path::GLMNetPath            # the cv path\n    nfolds::Int                 # the number of folds for the cv\n    lambda::Vector{Float64}     # lambda values for each solution\n    meanloss::Vector{Float64}   # mean loss for each solution\n    stdloss::Vector{Float64}    # standard deviation of the mean losses\nend\n\n.best is an instance of the GLMNetPath structure (see above). It holds the model with the optimal lambda parameter found by cross-validation that is created by default when the fit function is invoked.\n\nExamples:\n\nusing PosDefManifoldML\n\n# create an empty model\nm = ENLR(Fisher)\n\n# since the Fisher metric is the default metric,\n# this is equivalent to\nm = ENLR()\n\nNote that in general you need to invoke these constructors only when an ENLR model is needed as an argument to a function, otherwise you will create and fit an ENLR model using the fit function.\n\n\n\n\n\n","category":"type"},{"location":"MainModule/#MainModule-1","page":"Main Module","title":"MainModule","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"This is the main unit containing the PosDefManifoldML module.","category":"page"},{"location":"MainModule/#dependencies-1","page":"Main Module","title":"dependencies","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"standard Julia packages external packages\nLinearAlgebra PosDefManifold\nStatistics GLMNet\nRandom Distributions\nDates ","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"The main module does not contains functions.","category":"page"},{"location":"MainModule/#types-1","page":"Main Module","title":"types","text":"","category":"section"},{"location":"MainModule/#MLmodel-1","page":"Main Module","title":"MLmodel","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"As typical in machine learning packages, a type is created (a struct in Julia) to specify a ML model. Supertype","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"abstract type MLmodel end","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"is the abstract type for all machine learning models. Supertype","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"abstract type PDmodel<:MLmodel end","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"is the abstract type for all machine learning models acting on the positive definite (PD) manifold (for example, see MDM). Supertype","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"abstract type TSmodel<:MLmodel end","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"is the abstract type for all machine learning models acting on the tangent space (for example, see ENLR).","category":"page"},{"location":"MainModule/#IntVector-1","page":"Main Module","title":"IntVector","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"In all concerned functions class labels are given as a vector of integers, of type","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"IntVector=Vector{Int}.","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"Class labels are natural numbers in 1z, where z is the number of classes.","category":"page"},{"location":"MainModule/#Tips-and-Tricks-1","page":"Main Module","title":"Tips & Tricks","text":"","category":"section"},{"location":"MainModule/#the-â„Vector-type-1","page":"Main Module","title":"the â„Vector type","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"Check this documentation on typecasting matrices.","category":"page"},{"location":"MainModule/#notation-and-nomenclature-1","page":"Main Module","title":"notation & nomenclature","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"Throughout the code of this package the following notation is followed:","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"scalars and vectors are denoted using lower-case letters, e.g., y,\nmatrices using upper case letters, e.g., X\nsets (vectors) of matrices using bold upper-case letters, e.g., ð—.","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"The following nomenclature is used consistently:","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"ðTr: a training set of positive definite matrices\nðTe: a testing set of positive definite matrices\nð: a generic set of positive definite matrices.\nw: a weights vector of non-negative real numbers\nyTr: a training set class labels vector of positive integer numbers (1, 2,...)\nyTe: a testing set class labels vector of positive integer numbers\ny: a generic class labels vector of positive integer numbers.\nz: number of classes of a ML model\nk: number of matrices in a set","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"In the examples, bold upper-case letters are replaced by upper case letters in order to allow reading in the REPL.","category":"page"},{"location":"MainModule/#acronyms-1","page":"Main Module","title":"acronyms","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"MDM: minimum distance to mean\nENLR: Elastic-Net Logistic Regression\ncv: cross-validation","category":"page"},{"location":"#PosDefManifoldML-Documentation-1","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"","category":"section"},{"location":"#Requirements-and-Installation-1","page":"PosDefManifoldML Documentation","title":"Requirements & Installation","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Julia: version â‰¥ 1.1.1","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Packages: see the dependencies of the main module.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"The package is still not registered. To install it, execute the following command in Julia's REPL:","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"]add https://github.com/Marco-Congedo/PosDefManifoldML","category":"page"},{"location":"#Disclaimer-1","page":"PosDefManifoldML Documentation","title":"Disclaimer","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"This package is still in a pre-release stage. Independent reviewers for both the code and the documentation is welcome.","category":"page"},{"location":"#About-the-Authors-1","page":"PosDefManifoldML Documentation","title":"About the Authors","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Marco Congedo, corresponding author, is a research scientist of CNRS (Centre National de la Recherche Scientifique), working in UGA (University of Grenoble Alpes).","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Contact: first name dot last name at gmail dot com","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Saloni Jain is a student at the Indian Institute of Technology, Kharagpur, India.","category":"page"},{"location":"#Overview-1","page":"PosDefManifoldML Documentation","title":"Overview","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Riemannian geometry studies smooth manifolds, multi-dimensional curved spaces with peculiar geometries endowed with non-Euclidean metrics. In these spaces Riemannian geometry allows the definition of angles, geodesics (shortest path between two points), distances between points, centers of mass of several points, etc.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"In several fields of research such as computer vision and brain-computer interface, treating data in the manifold of positive definite matrices has allowed the introduction of machine learning approaches with remarkable characteristics, such as simplicity of use, excellent classification accuracy, as demonstrated by the winning score obtained in six international data classification competitions, and the ability to operate transfer learning (Congedo et al., 2017a, Brachant et al., 2012)ðŸŽ“.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"In this package we are concerned with making use of Riemannian Geometry for classifying data in the form of positive definite matrices (e.g., covariance matrices, Fourier cross-spectral matrices, etc.). This can be done in two ways: either directly in the manifold of positive definite matrices using Riemannian machine learning methods or in the tangent space, where traditional (Euclidean) machine learning methods apply (i.e., linear discriminant analysis, support-vector machine, logistic regression, random forest, etc.).","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"(Image: Figure 1) Figure 1","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Schematic representation of Riemannian classification. Data points are either natively positive definite matrices or are converted into this form. The classification can be performed by Riemannian methods in the manifold of positive definite matrices or by Euclidean methods after projection onto the tangent space.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"For a formal introduction to the manifold of positive definite matrices the reader is referred to the monography written by Bhatia(2007)ðŸŽ“.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"For an introduction to Riemannian geometry and an overview of mathematical tools implemented in the PostDefManifold package, which is heavily used here, see Intro to Riemannian Geometry.","category":"page"},{"location":"#Code-units-1","page":"PosDefManifoldML Documentation","title":"Code units","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"PosDefManifoldML is light-weight. It includes five code units (.jl files):","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Unit Description\nMainModule Main module, declaring constants and types\ntools.jl Unit containing general tools useful for machine learning and internal functions\nmdm.jl Unit implementing the MDM( Minimum Distance to Mean) machine learning model\nenlr.jl Unit implementing the ENLR( Elastic Net Logistic Regression) model, including the LASSO and RIDGE LR\ncv.jl Unit implementing cross-validation procedures","category":"page"},{"location":"#-1","page":"PosDefManifoldML Documentation","title":"ðŸŽ“","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"References","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2012) Multi-class Brain Computer Interface Classification by Riemannian Geometry, IEEE Transactions on Biomedical Engineering, 59(4), 920-928.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2013) Classification of covariance matrices using a Riemannian-based kernel for BCI applications, Neurocomputing, 112, 172-178.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"R. Bhatia (2007) Positive Definite Matrices, Princeton University press.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"M. Congedo, A. Barachant, R. Bhatia R (2017a) Riemannian Geometry for EEG-based Brain-Computer Interfaces; a Primer and a Review, Brain-Computer Interfaces, 4(3), 155-174.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"M. Congedo, A. Barachant, E. Kharati Koopaei (2017b) Fixed Point Algorithms for Estimating Power Means of Positive Definite Matrices, IEEE Transactions on Signal Processing, 65(9), 2211-2220.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Resources on GLMNet","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"webinar by Trevor Hastie","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Glmnet vignette","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Glmnet in R, documentation","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Julia wrapper for GLMNet","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"A more advanced wrapper for GLMNet","category":"page"},{"location":"#Contents-1","page":"PosDefManifoldML Documentation","title":"Contents","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Pages = [       \"index.md\",\n\t\t\t\t\t\t\t\t\"tutorials.md\",\n                \"MainModule.md\",\n                \"mdm.md\",\n                \"enlr.md\",\n\t\t\t\t\t\t\t\t\"cv.md\",\n\t\t\t\t\t\t\t\t\"tools.md\",\n\t\t]\nDepth = 1","category":"page"},{"location":"#Index-1","page":"PosDefManifoldML Documentation","title":"Index","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"","category":"page"},{"location":"tools/#tools.jl-1","page":"Tools","title":"tools.jl","text":"","category":"section"},{"location":"tools/#","page":"Tools","title":"Tools","text":"This unit implements tools that are useful for building Riemannian and Euclidean machine learning classifiers.","category":"page"},{"location":"tools/#Content-1","page":"Tools","title":"Content","text":"","category":"section"},{"location":"tools/#","page":"Tools","title":"Tools","text":"function description\ntsMap project data on a tangent space to apply Euclidean ML models therein\ngen2ClassData generate 2-class positive definite matrix data for testing Riemannian ML models\npredictErr prediction error given a vector of true labels and a vector of predicted labels","category":"page"},{"location":"tools/#","page":"Tools","title":"Tools","text":"tsMap\ngen2ClassData\npredictErr","category":"page"},{"location":"tools/#PosDefManifoldML.tsMap","page":"Tools","title":"PosDefManifoldML.tsMap","text":"function tsMap(\tmetric :: Metric,\n\t\tð :: â„Vector;\n\t\tw :: Vector = [],\n\t\tâœ“w :: Bool = true,\n\t\tâ© :: Bool = true,\n\t\tmeanISR :: Union{â„, Nothing} = nothing,\n\t\ttranspose :: Bool = true)\n\nThe tangent space mapping of matrices P_i, i=1k with geometric mean G, once those points have been parallel transported to the identity matrix, is given by:\n\nS_i=textrmlog(G^-12 P_i G^-12).\n\nGiven a vector of k Hermitian matrices ð, return a matrix X with such tangent vectors of the matrices in ð vectorized as per the vecP operation.\n\nThe mean G of the matrices in ð is found according to the specified metric, of type Metric. A natural choice is the Fisher metric.\n\nA set of k optional non-negative weights w can be provided for computing instead the weighted mean G. If w is non-empty and optional keyword argument âœ“w is true (default), the weights are normalized so as to sum up to 1, otherwise they are used as they are passed and should be already normalized. This option is provided to allow calling this function repeatedly without normalizing the same weights vector each time.\n\nIf an Hermitian matrix is provided as optional keyword argument meanISR, then the mean G is not computed, intead this matrix is used directly in the formula as the inverse square root (ISR) G^-12.\n\nIf meanISR is not provided, return the 2-tuple (X G^-12), otherwise return only matrix X.\n\nIf optional keyword argument transpose is true (default), X holds the k vectorized tangent vectors in its rows, otherwise they are arranged in its columns. The dimension of the rows in the former case and of the columns is the latter case is n(n+1)2, where n is the size of the matrices in ð (see vecP ).\n\nif optional keyword argument â© if true (default), the computation of the mean (if this is obtained with an iterative algorithm, e.g., using the Fisher metric) and the projection on the tangent space are multi-threaded. Multi-threading is automatically disabled if the number of threads Julia is instructed to use is 2 or 3k.\n\nExamples:\n\nusing PosDefManifoldML\n\n# generate four random symmetric positive definite 3x3 matrices\nPset = randP(3, 4)\n\n# project and vectorize in the tangent space\nX, Gâ»Â½ = tsMap(Fisher, Pset)\n\n# X is a 4x6 matrix, where 6 is the size of the\n# vectorized tangent vectors (n=3, n*(n+1)/2=6)\n\n# If repeated calls have to be done, faster computations are obtained\n# providing the inverse square root of the matrices in Pset, e.g.,\nX1 = tsMap(Fisher, â„Vector(Pset[1:2]); meanISR =Gâ»Â½)\nX2 = tsMap(Fisher, â„Vector(Pset[3:4]); meanISR =Gâ»Â½)\n\n\nSee: the â„Vector type.\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.gen2ClassData","page":"Tools","title":"PosDefManifoldML.gen2ClassData","text":"function gen2ClassData(n        ::  Int,\n                       k1train  ::  Int,\n                       k2train  ::  Int,\n                       k1test   ::  Int,\n                       k2test   ::  Int,\n                       separation :: Real = 0.1)\n\nGenerate a training set of k1train+k2train and a test set of k1test+k2test symmetric positive definite matrices. All matrices have size nxn.\n\nThe training and test sets can be used to train and test any MLmodel.\n\nseparation is a coefficient determining how well the two classs are separable; the higher it is, the more separable the two classes are. It must be in [0, 1] and typically a value of 0.5 already determines complete separation.\n\nReturn a 4-tuple with\n\nan â„Vector holding the k1train+k2train matrices in the training set,\nan â„Vector holding the k1test+k2test matrices in the test set,\na vector holding the k1train+k2train labels (integers) corresponding to the matrices of the training set,\na vector holding the k1test+k2test labels corresponding to the matrices of the test set (1 for class 1 and 2 for class 2).\n\nExamples\n\nusing PosDefManifoldML\n\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.25)\n\n# PTr=training set: 30 matrices for class 1 and 40 matrices for class 2\n# PTe=testing set: 60 matrices for class 1 and 80 matrices for class 2\n# all matrices are 10x10\n# yTr=a vector of 70 labels for the training set\n# yTe=a vector of 140 labels for the testing set\n\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.predictErr","page":"Tools","title":"PosDefManifoldML.predictErr","text":"function predictErr(yTrue::IntVector, yPred::IntVector;\n\t          digits::Int=3))\n\nReturn the percent prediction error given a vector of true labels and a vector of predicted labels.\n\nThe order of arguments does not matter.\n\nThe error is rounded to the number of optional keyword argument digits, 3 by default.\n\nSee predict\n\nExamples\n\nusing PosDefManifoldML\npredictErr([1, 1, 2, 2], [1, 1, 1, 2])\n# return: 25.0\n\n\n\n\n\n","category":"function"},{"location":"tutorial/#Tutorial-1","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"PosDefManifoldML features two bacic pipelines:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"1) a machine learning (ML) model is first fit (trained), then it can be used to predict the labels of testing data or the probability of the data to belong to each class. The raw prediction function of the models is available as well.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"2) a k-fold cross-validation procedure allows to estimate directly the accuracy of ML models and compare them.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"What PosDefManifoldML does for you is to allow an homogeneous syntax to run these two pipelines for all implemented ML models, it does not matter if they act directly on the manifold of positive definite matrices or on the tangent space.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"get data","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"A real data example will be added soon. For now, let us create simulated data for a 2-class example. First, let us create symmetric positive definite matrices (real positive definite matrices):","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using PosDefManifoldML\n\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"PTr is the simulated training set, holding 30 matrices for class 1 and 40 matrices for class 2\nPTe is the testing set, holding 60 matrices for class 1 and 80 matrices for class 2.\nyTr is a vector of 70 labels for the training set\nyTe is a vector of 140 labels for the testing set","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"All matrices are of size 10x10.","category":"page"},{"location":"tutorial/#Example-using-the-MDM-model-1","page":"Tutorial","title":"Example using the MDM model","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The minimum distance to mean (MDM) classifier is an example of classifier acting directly on the manifold. It is deterministic and no hyperparameter tuning is requested.","category":"page"},{"location":"tutorial/#MDM-Pipeline-1.-(fit-and-predict)-1","page":"Tutorial","title":"MDM Pipeline 1. (fit and predict)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Craete and fit an MDM model","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"An MDM model is created and fit with training data such as","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m = fit(MDM(Fisher), PTr, yTr)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"where Fisher is the usual choice of a Metric as declared in the parent package PosDefManifold.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The model can also be just created by","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"empty_model = MDM(Fisher)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"and fitted later using the fit function.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Predict (classify data)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to predict the labels of unlabeled data, we invoke","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m, PTe, :l)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The prediction error in percent can be retrived with","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"predictErr(yTe, yPred)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"If instead we wish to estimate the probabilities for the matrices in PTe of belonging to each class,","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"predict(m, PTe, :p)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Finally, the output functions of the MDM are obtaine by (see predict)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"predict(m, PTe, :f)","category":"page"},{"location":"tutorial/#MDM-Pipeline-2.-(cross-validation)-1","page":"Tutorial","title":"MDM Pipeline 2. (cross-validation)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The balanced accuracy estimated by a k-fold cross-validation is obtained such as","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cv = cvAcc(MDM(Fisher), PTe, yTe, 10)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"where 10 is the number of folds. This implies that at each cross-validation, 1/5th of the matrices is used for training and the remaining for testing.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Struct cv has been created and therein you have access to average accuracy and confusion matrix as well as accuracies and confusion matrices for all folds. For example, print the average confusion matrix:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cv.avgCnf","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"See CVacc for details on the fields of cross-validation objects.","category":"page"},{"location":"tutorial/#Example-using-the-ENLR-model-1","page":"Tutorial","title":"Example using the ENLR model","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The elastic net logistic regression (ENLR) classifier is an example of classifier acting on the tangent space. It has two hyperparameters. The alpha hyperparameter is supplied by the user and allows to trade off between a pure ridge LR model (Î±=0) and a pure lasso LR model (Î±=1). Given an alpha value, the model is fitted with a number of values for the lambda (regularization) hyperparameter. Thus, differently from the previous example, tuning this hyperparameter is necessary. Also, keep in mind that the fit and predict methods for ENLR models accept optional keyword arguments that are specific to this model.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"get data","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Let us get some simulated data as in the previous example.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)","category":"page"},{"location":"tutorial/#ENLR-Pipeline-1.-(fit-and-predict)-1","page":"Tutorial","title":"ENLR Pipeline 1. (fit and predict)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Craete and fit ENLR models","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"By default, a lasso model is fitted and the best value for the lambda hyperparameter is found:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1 = fit(ENLR(Fisher), PTr, yTr)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The optimal value of lambda for this training data is","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1.best.lambda","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The intercept and beta terms are retrived by","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1.best.a0\nm1.best.betas","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The number of non-zero beta coefficients can be found for example by","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"length(unique(m1.best.betas))-1","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to fit a ridge LR model:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m2 = fit(ENLR(Fisher), PTr, yTr; alpha=0)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Values of alpha in range (0 1) fit instead an elastic net LR model. In the following we also request to standardize predictors:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m3 = fit(ENLR(Fisher), PTr, yTr; alpha=0.9, standardize=true)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to find the regularization path we use the fitType keyword argument:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1 = fit(ENLR(Fisher), PTr, yTr; fitType=:path)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The values of lambda along the path are given by","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1.path.lambda","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to find the best value of the lambda hyperparameter and the regularization path at once:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1 = fit(ENLR(Fisher), PTr, yTr; fitType=:all)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"See the documentation of the fit ENLR method for details on all available optional arguments.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Classify data (predict)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"For prediction, we can request to use the best model (optimal lambda) to use a specific model of the regularization path or all of them. Note that with the last call both the .best and .path field of the m1 structure have been created.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"By default, prediction is obtained from the best model and we request to predict the labels:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe)\n\n# prediction error in percent\npredictErr(yPred, yTe)\n\n# predict probabilities of matrices in `PTe` to belong to each class\npredict(m1, PTe, :p)\n\n# output function of the model for each class\npredict(m1, PTe, :f)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to request the predition of labels for all models in the regularization path:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe, :l, :path, 0)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"while for a specific model in the path:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe, :l, :path, 10)","category":"page"},{"location":"tutorial/#ENLR-Pipeline-2.-(cross-validation)-1","page":"Tutorial","title":"ENLR Pipeline 2. (cross-validation)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The balanced accuracy estimated by a k-fold cross-validation is obtained with the exact same syntax for all models, thus, for example:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cv = cvAcc(ENLR(Fisher), PTe, yTe, 10)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to perform another 10-fold cross-validation arranging the training data differently in the folds:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cv = cvAcc(ENLR(Fisher), PTe, yTe, 10; shuffle=true)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"This last command can be invoked repeatedly. ","category":"page"}]
}
