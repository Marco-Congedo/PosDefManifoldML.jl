var documenterSearchIndex = {"docs":
[{"location":"mdm/#mdm.jl-1","page":"Minimum Distance to Mean","title":"mdm.jl","text":"","category":"section"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"This unit implements the Riemannian MDM (Minimum Distance to Mean) classifier for the manifold of positive definite (PD) matrices, both real (symmetric PD) or complex (Hermitian PD) matrices. The MDM is a simple, yet efficient, deterministic and paramater-free classifier acting directly on the manifold of positive definite matrices (Barachat el al., 2012; Congedo et al., 2017a ðŸŽ“): given a number of PD matrices representing class means, the MDM classify an unknown datum (also a PD matrix) as belonging to the class whose mean is the closest to the datum. The process is illustrated in the upper part of this figure.","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"The MDM classifier involves only the concepts of a distance function for two PD matrices and a mean (center of mass) for a number of them. Those are defined for any given metric, a Metric enumerated type declared in PosDefManifold.","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"Currently supported metrics are:","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"metric (distance) mean estimation known also as\nEuclidean arithmetic \ninvEuclidean harmonic \nChoEuclidean Cholesky Euclidean \nlogEuclidean log-Euclidean \nlogCholesky log-Cholesky \nFisher Fisher Cartan, Karcher, Pusz-Woronowicz, Affine-Invariant, ...\nlogdet0 logDet S, Î±, Bhattacharyya, Jensen, ...\nJeffrey Jeffrey symmetrized Kullback-Leibler\nWasserstein Wasserstein Bures, Hellinger, optimal transport, ...","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"Do not use the Von Neumann metric, which is also supported in PosDefManifold, since it does not allow a definition of mean. See here for details on the metrics.","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"The fit, predict and cvAcc functions for the MDM model are  reported in the cv.jl unit, since those are homogeneous across all machine learning models. Here it is reported the MDMmodel abstract type, the MDM structure and the following functions, which typically you will not need to access directly, but are nonetheless provided to facilitate low-level operations with MDM classifiers:","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"function description\ngetMean compute the mean of positive definite matrices for fitting the MDM model\ngetDistances compute the distances of a matrix set to a set of means","category":"page"},{"location":"mdm/#","page":"Minimum Distance to Mean","title":"Minimum Distance to Mean","text":"MDMmodel\nMDM\ngetMean\ngetDistances","category":"page"},{"location":"mdm/#PosDefManifoldML.MDMmodel","page":"Minimum Distance to Mean","title":"PosDefManifoldML.MDMmodel","text":"Abstract type for MDM (Minimum Distance to Mean) machine learning models\n\n\n\n\n\n","category":"type"},{"location":"mdm/#PosDefManifoldML.MDM","page":"Minimum Distance to Mean","title":"PosDefManifoldML.MDM","text":"mutable struct MDM <: MDMmodel\n    metric  :: Metric = Fisher;\n    featDim :: Int\n    means   :: â„Vector\n    imeans  :: â„Vector\nend\n\nMDM machine learning models are incapsulated in this mutable structure. MDM models have three fields: .metric, .featDim and .means.\n\nThe field metric, of type Metric, is to be specified by the user. It is the metric that will be adopted to compute the class means and the distances to the mean.\n\nThe field featDim is the dimension of the manifold in which the model acts. This is given by n(n+1)2, where n is the dimension of the PD matrices. This field is not to be specified by the user, instead, it is computed when the MDM model is fit using the fit function and is accessible only thereafter.\n\nThe field means is an â„Vector holding the class means, i.e., one mean for each class. This field is not to be specified by the user, instead, the means are computed when the MDM model is fitted using the fit function and are accessible only thereafter.\n\nThe field imeans is an â„Vector holding the inverse of the matrices in means. This also is not to be specified by the user, is computed when the model is fitted and is accessible only thereafter. It is used to optimize the computation of distances if the model is fitted useing the Fisher metric (default).\n\nExamples:\n\nusing PosDefManifoldML, PosDefManifold\n\n# create an empty model\nm = MDM(Fisher)\n\n# since the Fisher metric is the default metric,\n# this is equivalent to\nm = MDM()\n\nNote that in general you need to invoke these constructors only when an MDM model is needed as an argument to a function, otherwise you can more simply create and fit an MDM model using the fit function.\n\n\n\n\n\n","category":"type"},{"location":"mdm/#PosDefManifoldML.getMean","page":"Minimum Distance to Mean","title":"PosDefManifoldML.getMean","text":"function getMean(metric :: Metric,\n                 ð      :: â„Vector;\n              w        :: Vector = [],\n              âœ“w       :: Bool    = true,\n              meanInit :: Union{â„, Nothing} = nothing,\n              tol      :: Real   = 0.,\n              â©      :: Bool    = true)\n\nTypically, you will not need this function as it is called by the fit function.\n\nGiven a metric of type Metric, an â„Vector of Hermitian matrices ð and an optional non-negative real weights vector w, return the (weighted) mean of the matrices in ð. This is used to fit MDM models.\n\nThis function calls the appropriate mean functions of package PostDefManifold, depending on the chosen metric, and check that, if the mean is found by an iterative algorithm, then the iterative algorithm converges.\n\nSee method (3) of the mean function for the meaning of the optional keyword arguments w, âœ“w, meanInit, tol and â©, to which they are passed.\n\nThe returned mean is flagged by Julia as an Hermitian matrix (see LinearAlgebra).\n\n\n\n\n\n","category":"function"},{"location":"mdm/#PosDefManifoldML.getDistances","page":"Minimum Distance to Mean","title":"PosDefManifoldML.getDistances","text":"function getDistances(metric :: Metric,\n                      means  :: â„Vector,\n                      ð      :: â„Vector;\n                imeans  :: Union{â„Vector, Nothing} = false,\n                scale   :: Bool = false,\n                â©      :: Bool = true)\n\nTypically, you will not need this function as it is called by the predict function.\n\nGiven an â„Vector ð holding k Hermitian matrices and an â„Vector means holding z matrix means, return the square of the distance of each matrix in ð to the means in means.\n\nThe squared distance is computed according to the chosen metric, of type Metric. See metrics for details on the supported distance functions.\n\nThe computation of distances is optimized for the Fisher metric if an â„Vector holding the inverse of the means in means is passed as optional keyword argument imeans.\n\nIf scale is true, the distances are divided by the size of the matrices in ð. This is used to compare disctances computed on manifolds with different dimensions.\n\nIf â© is true, the distances are computed using multi-threading, unless the number of threads Julia is instructed to use is <2 or <3k.\n\nThe result is a zxk matrix of squared distances.\n\n\n\n\n\n","category":"function"},{"location":"contribute/#How-to-Contribute-1","page":"How to contribute","title":"How to Contribute","text":"","category":"section"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"You can easily contribute a new ML model to PosDefManifoldML.jl package following these steps:","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"Let's say you want to contribute an ML model named ABC.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"If the model act on the manifold of positive definite matrices (PSD), use as template the mdm.jl unit. If it acts on the tangent space, use as template the svm.jl unit (you can also check the enlr.jl unit).","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"Save the template unit as unit abc.jl in the same directory where the template file is.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"Implementing your ABC model entails the following five steps:","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"1) Declare an abstract type for the model","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"if your model acts on the manifold of PSD matrices, this will be","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"abstract type ABCmodel<:PDmodel end","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"if your model act on the tangent space, this will be","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"abstract type ABCmodel<:TSmodel end","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"2) Declare the struct to hold the model and its default creator","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"This will look like:","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"mutable struct ABC <: ABCmodel\n\t\tmetric        :: Metric\n\t\tdefaultkwarg1 :: Type_of_defaultkwarg1\n\t\tdefaultkwarg2 :: Type_of_defaultkwarg2\n\t\t...\n\t\tvoidkwarg1\n\t\tvoidkwarg2\n\t\t...\n\t\tfunction ABC(metric :: Metric=Fisher;\n\t\t\t\tdefaultkwarg1 :: Type_of_defaultkwarg1 = default_value,\n\t\t\t\tdefaultkwarg2 :: Type_of_defaultkwarg2 = default_value,\n\t\t\t\t...\n\t\t\t\tvoidkwarg1 = nothing,\n\t\t\t\tvoidkwarg2 = nothing,\n\t\t\t\t...)\n\t\t\tnew(metric, defaultkwarg1, defaultkwarg1,...,\n\t\t\t\tvoidkwarg1, voidkwarg2,...)\n\t\tend\nend","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"In the above example defaultkwarg are essential parameters that should be set by default upon creation. Use as few as those as needed to obtain a working ML model when the user does not pass any argument.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"The voidkwarg arguments are arguments that you wish be accessible to the user in the structure once the model has been fitted. Include here as few of them as possible.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"3) write the fit function","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"This function will fit the model. Its default behavior should fit the model and tune hyperparameters in order to find the best model by cross-validation if the ABC model has hyperparameters.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"Your fit function declaration will look like:","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"function fit(   model :: ABCmodel,\n\t\t\t    ðTr :: â„Vector,\n\t\t\t\t# if the model acts on the tangent space use:\n\t\t\t\t# ðTr :: Union{â„Vector, Matrix{Float64}}\n\t\t\t\tyTr :: Vector;\n\t\t\t\tverbose :: Bool = true\n\t\t\t\tkwarg1  :: type-of-kwarg1 = default-value,\n\t\t\t\tkwarg2  :: type-of-kwarg2 = default-value,\n\t\t\t\t...)\nend","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"Here you can use as many kwarg arguments as you wish. Currently, all ML models have a verbose argument. Your fit function should starts with:","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"âŒš=now() # time in milliseconds\nâ„³=deepcopy(model) # output model","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"and ends with","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"verbose && println(defaultFont, \"Done in \", now()-âŒš,\".\")\nreturn â„³","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"In between these two blocks you will fit the model and write into â„³ the voidkwarg arguments you have declared in the ABC struct (see above).","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"Keep in mind that if the model acts in the tangent space you will need to project the data therein. For doing so you can use the _getFeat_fit! internal function (declared in unit tools.jl), as it is done for the ENLR and SVM models. This entails using some standard arguments for tangent space projection, which should be given as options to the user, as done for these models. See how this is done in the fit function of the ENLR and SVM models (unit enlr.jl and svm.jl, respectively).","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"Once you have finished, a call such as","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"m1 = fit(ABC(), PTr, yTr)","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"for some data PTr(a matrix or â„Vector type) and labels yTr (see fit) should fit the model in such a way that it is ready to allow a call to the predict function and return the model.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"4) write the predict function","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"Your predict function declaration will look like:","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"function predict(model   :: ABCmodel,\n\t\t\t\tðTe :: Union{â„Vector, Matrix{Float64}},\n\t\t\t\t# if the model acts on the tangent space use:\n\t\t\t\t# ðTr :: Union{â„Vector, Matrix{Float64}}\n\t\t\t\twhat :: Symbol = :labels;\n\t\t\t\tkwarg1 :: type_of_kwarg1 = default_value\n\t\t\t\tkwarg2 :: type_of_kwarg2 = default_value\n\t\t\t\t...\n\t\t\t\tverbose  :: Bool = true)","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"where in general here you will not need kwarg arguments.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"Your predict function should starts with:","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"âŒš=now() # time in milliseconds","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"and ends with","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"verbose && println(defaultFont, \"Done in \", now()-âŒš,\".\")\nverbose && println(titleFont, \"\\nPredicted \",_what2Str(what),\":\", defaultFont)\nreturn ðŸƒ","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"where in between these two blocks variable ðŸƒ has been filled with the prediction.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"To use this template code, in the declaration of the _what2Str function (declared in unit tools.jl), add a line to allow returning the full name of your model as a string.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"As for the fit function, if the model acts in the tangent space you will need to project the data therein. For doing so you can use here the _getFeat_Predict! internal function (declared in unit tools.jl), as it is done for the ENLR and SVM models. This entails using some standard arguments for tangent space projection, which should be given as options to the user, as done for these models. Note that _getFeat_Predict! is similar, but not the same as _getFeat_fit! function.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"All ML models implemented so far allow three types of prediction, depending on the symbol passed by the user with argument what. See the documentation of the predict function for the ENLR model to see what these three types of predictions are and the code of the function in unit enlr.jl for an example on how to compute them. Note that the returned type of the predict function, the variable ðŸƒ, depends on what is predicted.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"5) Allow the cvAcc function to support your model properly","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"If you have been following these guidelines so far, the cvAcc function (declared in unit cv.jl) will be able to perform k-fold cross-validation on data using your ABC model.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"This function allows the user to pass an arbitrary number of optional keyword arguments, so the user will be able to pass here the kwarg arguments you have declared in your fit function for the ABC model and those will be passed to the fit function when fitting the model at each fold.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"This will be done automatically, however it is necessary to prevent the user from passing here optional keyword arguments that should not be used in a k-fold cross-validation setting (if in your fit function you have declared such arguments).","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"Also, it is necessary here to overwrite into the ABC model that is passed by the user as argument to the cvAcc function the defaultkwarg1 fields of the ABC struct you have declared, if the user can request different values for those fields using optional keyword arguments passed to the cvAcc function. For example, consider the SVM model; suppose the user pass to the cvAcc function a default SVM model. For such a model the kernel is a radial basis kernel. Suppose the user has passed to the cvAcc function argument kernel=linear. If you don't overwrite the kernel into the model, the radianl basis kernel will be used instead of the desired linear kernel.","category":"page"},{"location":"contribute/#","page":"How to contribute","title":"How to contribute","text":"You can do so easily using the _rmArgs and _getArgValue internal functions (declared in unit tools.jl), as done for the SVM and ENLR model (see the code of the cvAcc function).","category":"page"},{"location":"cv/#cv.jl-1","page":"fit, predict, cv","title":"cv.jl","text":"","category":"section"},{"location":"cv/#","page":"fit, predict, cv","title":"fit, predict, cv","text":"This unit implements cross-validation procedures for estimating the accuracy and balanced accuracy of machine learning models. It also reports the documentation of the fit and predict functions, as they are common to all models.","category":"page"},{"location":"cv/#","page":"fit, predict, cv","title":"fit, predict, cv","text":"Content","category":"page"},{"location":"cv/#","page":"fit, predict, cv","title":"fit, predict, cv","text":"struct description\nCVacc encapsulate the result of cross-validation procedures for estimating accuracy","category":"page"},{"location":"cv/#","page":"fit, predict, cv","title":"fit, predict, cv","text":"function description\nfit fit a model with training data, or create and fit it\npredict preidct labels, probabilities or scoring functions on test data\ncvAcc estimate accuracy of a model by cross-validation\ncvSetup generate indexes for performing cross-validtions","category":"page"},{"location":"cv/#","page":"fit, predict, cv","title":"fit, predict, cv","text":"CVacc\nfit\npredict\ncvAcc\ncvSetup","category":"page"},{"location":"cv/#PosDefManifoldML.CVacc","page":"fit, predict, cv","title":"PosDefManifoldML.CVacc","text":"struct CVacc\n    cvType    :: String\n    scoring   :: Union{String, Nothing}\n    modelType :: Union{String, Nothing}\n    cnfs      :: Union{Vector{Matrix{T}}, Nothing} where T<:Real\n    avgCnf    :: Union{Matrix{T}, Nothing} where T<:Real\n    accs      :: Union{Vector{T}, Nothing} where T<:Real\n    avgAcc    :: Union{Real, Nothing}\n    stdAcc    :: Union{Real, Nothing}\nend\n\nA call to cvAcc results in an instance of this structure. Fields:\n\n.cvTpe is the type of cross-validation technique, given as a string (e.g., \"10-kfold\")\n\n.scoring is the type of accuracy that is computed, given as a string. This has been passed as argument to cvAcc. Currently accuracy and balanced accuracy are supported.\n\n.modelType is type of the machine learning used for performing the cross-validation, given as a string.\n\n.cnfs is a vector of matrices holding the confusion matrices obtained at each fold of the cross-validation.\n\n.avgCnf is the average confusion matrix across the folds of the cross-validation.\n\n.accs is a vector of real numbers holding the accuracies obtained at each fold of the cross-validation.\n\n.avgAcc is the average accuracy across the folds of the cross-validation.\n\n.stdAcc is the standard deviation of the accuracy across the folds of the cross-validation.\n\n\n\n\n\n","category":"type"},{"location":"cv/#StatsBase.fit","page":"fit, predict, cv","title":"StatsBase.fit","text":"function fit(model :: MDMmodel,\n              ðTr   :: â„Vector,\n              yTr   :: IntVector;\n       w        :: Vector = [],\n       âœ“w       :: Bool  = true,\n       meanInit :: Union{â„Vector, Nothing} = nothing,\n       tol      :: Real  = 1e-5,\n       verbose  :: Bool  = true,\n       â©       :: Bool  = true)\n\nFit an MDM machine learning model, with training data ðTr, of type â„Vector, and corresponding labels yTr, of type IntVector. Return the fitted model.\n\nLabels must be provided using the natural numbers, i.e., 1 for the first class, 2 for the second class, etc.\n\nFitting an MDM model involves only computing a mean of all the matrices in each class. Those class means are computed according to the metric specified by the MDM constructor.\n\nOptional keyword argument w is a vector of non-negative weights associated with the matrices in ðTr. This weights are used to compute the mean for each class. See method (3) of the mean function for the meaning of the arguments w, âœ“w and â©, to which they are passed. Keep in mind that here the weights should sum up to 1 separatedly for each class, which is what is ensured by this function if âœ“w is true.\n\nOptional keyword argument tol is the tolerance required for those algorithms that compute the mean iteratively (they are those adopting the Fisher, logde0 or Wasserstein metric). It defaults to 1e-5. For details on this argument see the functions that are called for computing the means:\n\nFisher metric: gmean\nlogdet0 metric: ld0mean\nWasserstein metric: Wasmean.\n\nFor those algorithm an initialization can be provided with optional keyword argument meanInit. If provided, this must be a vector of Hermitian matrices of the â„Vector type and must contain as many initializations as classes, in the natural order corresponding to the class labels (see above).\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: predict, cvAcc.\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.25)\n\n# create and fit a model:\nm=fit(MDM(Fisher), PTr, yTr)\n\n\n\n\n\nfunction fit(model\t:: ENLRmodel,\n               ðTr\t :: Union{â„Vector, Matrix{Float64}},\n               yTr\t:: IntVector;\n\t\t# parameters for projection onto the tangent space\n\t\tw\t\t:: Union{Symbol, Tuple, Vector} = [],\n\t\tmeanISR\t:: Union{â„, Nothing} = nothing,\n\t\tmeanInit:: Union{â„, Nothing} = nothing,\n\t\tvecRange:: UnitRange = ðTr isa â„Vector ? (1:size(ðTr[1], 2)) : (1:size(ðTr, 2)),\n\t\tfitType\t:: Symbol = :best,\n\t\tverbose\t:: Bool = true,\n\t\tâ©\t   :: Bool = true,\n\t\t# arguments for `GLMNet.glmnet` function\n\t\talpha\t\t\t:: Real = model.alpha,\n\t\tweights\t\t\t:: Vector{Float64} = ones(Float64, length(yTr)),\n\t\tintercept\t\t:: Bool = true,\n\t\tstandardize\t\t:: Bool = true,\n\t\tpenalty_factor\t:: Vector{Float64} = ones(Float64, _getDim(ðTr, vecRange)),\n\t\tconstraints\t\t:: Matrix{Float64} = [x for x in (-Inf, Inf), y in 1:_getDim(ðTr, vecRange)],\n\t\toffsets\t\t\t:: Union{Vector{Float64}, Nothing} = nothing,\n\t\tdfmax\t\t\t:: Int = _getDim(ðTr, vecRange),\n\t\tpmax\t\t\t:: Int = min(dfmax*2+20, _getDim(ðTr, vecRange)),\n\t\tnlambda\t\t\t:: Int = 100,\n\t\tlambda_min_ratio:: Real = (length(yTr) < _getDim(ðTr, vecRange) ? 1e-2 : 1e-4),\n\t\tlambda\t\t\t:: Vector{Float64} = Float64[],\n\t\ttol\t\t\t\t:: Real = 1e-5,\n\t\tmaxit\t\t\t:: Int = 1000000,\n\t\talgorithm\t\t:: Symbol = :newtonraphson,\n\t\t# selection method\n\t\tÎ»SelMeth\t:: Symbol = :sd1,\n\t\t# arguments for `GLMNet.glmnetcv` function\n\t\tnfolds\t\t:: Int = min(10, div(size(yTr, 1), 3)),\n\t\tfolds\t\t:: Vector{Int} =\n\t\tbegin\n\t\t\tn, r = divrem(size(yTr, 1), nfolds)\n\t\t\tshuffle!([repeat(1:nfolds, outer=n); 1:r])\n\t\tend,\n\t\tparallel \t:: Bool=true)\n\nCreate and fit an ENLR machine learning model, with training data ðTr, of type â„Vector, and corresponding labels yTr, of type IntVector. Return the fitted model(s) as an instance of the ENLR structure.\n\nAs for all ML models acting in the tangent space, fitting an ENLR model involves computing a mean of all the matrices in ðTr, mapping all matrices onto the tangent space after parallel transporting them at the identity matrix and vectorizing them using the vecP operation. Once this is done, the elastic net logistic regression is fitted.\n\nThe mean is computed according to the .metric field of the model, with optional weights w. The .metric field of the model is passed to the tsMap function. By default the metric is the Fisher metric. See the examples here below to see how to change metric. See mdm.jl or check out directly the documentation of PosDefManifold.jl for the available metrics.\n\nOptional keyword arguments\n\nBy default, uniform weights will be given to all observations for computing the mean to pass in the tangent space. This is equivalent to passing as argument w=:uniform (or w=:u). You can also pass as argument:\n\nw=:balanced (or simply w=:b). If the two classes are unbalanced, the weights should be inversely proportional to the number of examples for each class, in such a way that each class contributes equally to the computation of the mean. This is equivalent of passing w=tsWeights(yTr). See the tsWeights function for details.\nw=v, where v is a user defined vector of non-negative weights for the observations, thus, v must contain the same number of elements as yTr. For example, w=[1.0, 1.0, 2.0, 2.0, ...., 1.0]\nw=t, where t is a 2-tuple of real weights, one weight for each class, for example w=(0.5, 1.5). This is equivalent to passing w=tsWeights(yTr; classWeights=collect(t)), see the tsWeights function for details.\n\nIf meanISR is passed as argument, the mean is not computed, instead this matrix is the inverse square root (ISR) of the mean used for projecting the matrices in the tangent space (see tsMap). Passed or computed, the inverse square root (ISR) of the mean will be written in the .meanISR field of the created ENLR structure. If meanISRis is not provided and the .metric field of the model is Fisher, logdet0 or Wasserstein, the tolerance of the iterative algorithm used to compute the mean is set to the argument passed as tol (default 1e-5). Also, in this case a particular initialization for those iterative algorithms can be provided as an Hermitian matrix with argument meanInit.\n\nThis function also allows to fit a model passing as training data ðTr directly a matrix of feature vectors, where each feature vector is a row of the matrix. In this case the metric of the ENLR model and argument meanISR are not used. Therefore, the .meanISR field of the created ENLR structure will be set to nothing.\n\nIf a UnitRange is passed with optional keyword argument vecRange, then if ðTr is a vector of Hermitian matrices, the vectorization of those matrices once they are projected onto the tangent space concerns only the rows (or columns) given in the specified range, else if ðTr is a matrix with feature vectors arranged in its rows, then only the columns of ðTr given in the specified range will be used.\n\nIf fitType = :best (default), a cross-validation procedure is run to find the best lambda hyperparameter for the given training data. This finds a single model that is written into the .best field of the ENLR structure that will be created.\n\nIf fitType = :path, the regularization path for several values of the lambda hyperparameter is found for the given training data. This creates several models, which are written into the .path field of the ENLR structure that will be created, none of which is optimal, in the cross-validation sense, for the given training data.\n\nIf fitType = :all, both the above fits are performed and all fields of the ENLR structure that will be created will be filled in.\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nThe â© argument (true by default) is passed to the tsMap function for projecting the matrices in ðTr onto the tangent space and to the GLMNet.glmnetcv function to run inner cross-validation to find the best model using multi-threading.\n\nThe remaining optional keyword arguments, are\n\nthe arguments passed to the GLMNet.glmnet function for fitting the models. Those are always used.\nthe Î»SelMeth argument and the arguments passed to the GLMNet.glmnetcv function for finding the best lambda hyperparamater by cross-validation. Those are used only if fitType = :path or = :all.\n\nOptional keyword arguments for fitting the model(s) using GLMNet\n\nalpha: the hyperparameter in 0 1 to trade-off an elestic-net model. Î±=0 requests a pure ridge model and Î±=1 a pure lasso model. This defaults to 1.0, which specifies a lasso model, unless the input ENLR model has another value in the alpha field, in which case this value is used. If argument alpha is passed here, it will overwrite the alpha field of the input model.\n\nweights: a vector of weights for each matrix (or feature vectors) of the same size as yTr. It defaults to 1 for all matrices.\n\nintercept: whether to fit an intercept term. The intercept is always unpenalized. Defaults to true.\n\nstandardize: if true (default), GLMNet standardize the predictors (presumably this amounts to transform to unit variance) so that they are in the same units. This is a common choice for regularized regression models.\n\npenalty_factor: a vector of length n(n+1)2, where n is the dimension of the original PD matrices on which the model is applied, of penalties for each predictor in the tangent vectors. This defaults to all ones, which weights each predictor equally. To specify that a predictor should be unpenalized, set the corresponding entry to zero.\n\nconstraints: an n(n+1)2 x 2 matrix specifying lower bounds (first column) and upper bounds (second column) on each predictor. By default, this is [-Inf Inf] for each predictor (each element of tangent vectors).\n\noffset: see documentation of original GLMNet package ðŸŽ“.\n\ndfmax: The maximum number of predictors in the largest model.\n\npmax: The maximum number of predictors in any model.\n\nnlambda: The number of values of Î» along the path to consider.\n\nlambda_min_ratio: The smallest Î» value to consider, as a ratio of the value of Î» that gives the null model (i.e., the model with only an intercept). If the number of observations exceeds the number of variables, this defaults to 0.0001, otherwise 0.01.\n\nlambda: The Î» values to consider for fitting. By default, this is determined from nlambda and lambda_min_ratio.\n\ntol: Is the convergence criterion for both the computation of a mean for projecting onto the tangent space (if the metric requires an iterative algorithm) and for the GLMNet fitting algorithm. Defaults to 1e-5. In order to speed up computations, you may try to set a lower tol; The convergence will be faster but more coarse, with a possible drop of classification accuracy, depending on the signal-to-noise ratio of the input features.\n\nmaxit: The maximum number of iterations of the cyclic coordinate descent algorithm. If convergence is not achieved, a warning is returned.\n\nalgorithm: the algorithm used to find the regularization path. Possible values are :newtonraphson (default) and :modifiednewtonraphson.\n\nFor further informations on those arguments, refer to the resources on the GLMNet package ðŸŽ“.\n\nOptional Keyword arguments for finding the best model by cv\n\nÎ»SelMeth = :sd1 (default), the best model is defined as the one allowing the highest cvÎ».meanloss within one standard deviation of the minimum, otherwise it is defined as the one allowing the minimum cvÎ».meanloss. Note that in selecting a model, the model with only the intercept term, if it exists, is ignored. See ENLRmodel for a description of the .cvÎ» field of the model structure.\n\nArguments nfolds and folds are passed to the GLMNet.glmnetcv function along with the â© argument. Please refer to the resources on GLMNet for details ðŸŽ“.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: predict, cvAcc.\n\nTutorial: Example using the ENLR model.\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1)\n\n# Fit an ENLR lasso model and find the best model by cross-validation:\nm=fit(ENLR(), PTr, yTr)\n\n# ... balancing the weights for tangent space mapping\nm=fit(ENLR(), PTr, yTr; w=tsWeights(yTr))\n\n# ... using the log-Eucidean metric for tangent space projection\nm=fit(ENLR(logEuclidean), PTr, yTr)\n\n# Fit an ENLR ridge model and find the best model by cv:\nm=fit(ENLR(Fisher), PTr, yTr; alpha=0)\n\n# Fit an ENLR elastic-net model (Î±=0.9) and find the best model by cv:\nm=fit(ENLR(Fisher), PTr, yTr; alpha=0.9)\n\n# Fit an ENLR lasso model and its regularization path:\nm=fit(ENLR(), PTr, yTr; fitType=:path)\n\n# Fit an ENLR lasso model, its regularization path\n# and the best model found by cv:\nm=fit(ENLR(), PTr, yTr; fitType=:all)\n\n\n\n\n\n\nfunction fit(model     :: SVMmodel,\n               ðTr     :: Union{â„Vector, Matrix{Float64}},\n               yTr     :: IntVector=[];\n           # parameters for projection onto the tangent space\n\t   w :: Union{Symbol, Tuple, Vector} = [],\n\t   meanISR :: Union{â„, Nothing} = nothing,\n\t   meanInit :: Union{â„, Nothing} = nothing,\n\t   vecRange\t:: UnitRange = ðTr isa â„Vector ? (1:size(ðTr[1], 2)) :\n\t   \t\t\t\t\t\t\t\t\t\t\t (1:size(ðTr, 2)),\n\t   # SVM paramters\n\t   svmType :: Type = SVC,\n\t   kernel :: Kernel.KERNEL = RadialBasis,\n\t   epsilon :: Float64 = 0.1,\n\t   cost\t:: Float64 = 1.0,\n\t   gamma :: Float64\t= 1/_getDim(ðTr, vecRange),\n\t   degree :: Int64\t= 3,\n\t   coef0 :: Float64\t= 0.,\n\t   nu :: Float64 = 0.5,\n\t   shrinking :: Bool = true,\n\t   probability :: Bool = false,\n\t   weights :: Union{Dict{Int, Float64}, Nothing} = nothing,\n\t   cachesize :: Float64\t= 200.0,\n\t   # Generic and common parameters\n\t   tol :: Real = 1e-5,\n\t   rescale :: Tuple\t= (-1, 1),\n\t   verbose :: Bool = true,\n\t   â© :: Bool = true)\n\nCreate and fit an SVM machine learning model, with training data ðTr, of type â„Vector, and corresponding labels yTr, of type IntVector. The label vector can be omitted if the svmType is OneClassSVM (see SVM). Return the fitted model as an instance of the SVM structure.\n\nAs for all ML models acting in the tangent space, fitting an SVM model involves computing a mean of all the matrices in ðTr, mapping all matrices onto the tangent space after parallel transporting them at the identity matrix and vectorizing them using the vecP operation. Once this is done, the support-vector machine is fitted.\n\nArguments w, meanISR, meanInit and vecRange allow to tune the projection onto the tangent space. See the documentation of the fit function for the ENLR model here above for their meaning.\n\nsvmType and kernel allow to chose among several available SVM models. See the documentation of the SVM structure.\n\nepsilon, with default 0.1, is the epsilon in loss function of the epsilonSVR SVM model.\n\ncost, with default 1.0, is the cost parameter C of SVC, epsilonSVR, and nuSVR SVM models.\n\ngamma, defaulting to 1 divided by the length of the feature vectors, is the Î³ parameter for RadialBasis, Polynomial and Sigmoid kernels.\n\ndegree, with default 3, is the degree for Polynomial kernels\n\ncoef0, zero by default, is a parameter for the Sigmoid and Polynomial kernel.\n\nnu, with default 0.5, is the parameter ðœˆ of nuSVC, OneClassSVM, and nuSVR SVM models. It should be in the interval (0, 1].\n\nshrinking, true by default, sets whether to use the shrinking heuristics.\n\nprobability, false by default sets whether to train a SVC or SVR model allowing probability estimates.\n\nif a Dict{Int, Float64} is passed as weights argument, it will be used to give weights to the classes. By default it is equal to nothing, implying equal weights to all classes.\n\ncachesize for the kernel, 200.0 by defaut (in MB), can be increased for very large problems.\n\ntol is the convergence criterion for both the computation of a mean for projecting onto the tangent space (if the metric recquires an iterative algorithm) and for the LIBSVM fitting algorithm. Defaults to 1e-5.\n\nrescale is a 2-tuple of the lower and upper limit to rescale the feature vectors within these limits. The default is (-1, 1), since tangent vectors of PD matrices have positive and negative elements. If ðTr is a feature matrix and the features are only positive, use (0, 1) instead. In order not to rescale the feature vectors, use ().\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL. It may not work properly in a multithreaded context (see â© argument here below).\n\nThe â© argument (true by default) is passed to the tsMap function for projecting the matrices in ðTr onto the tangent space and to the LIBSVM function that perform the fit in order to run them in multi-threaded mode.\n\nFor further information on tho LIBSVM arguments, refer to the resources on the LIBSVM package ðŸŽ“.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: predict, cvAcc.\n\nTutorial: Example using SVM models.\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);\n\n# Fit an SVC SVM model and find the best model by cross-validation:\nm=fit(SVM(), PTr, yTr)\n\n# ... balancing the weights for tangent space mapping\nm=fit(SVM(), PTr, yTr; w=:b)\n\n# ... using the log-Eucidean metric for tangent space projection\nm=fit(SVM(logEuclidean), PTr, yTr)\n\n# ... using the linear kernel\nm=fit(SVM(logEuclidean), PTr, yTr, kernel=Linear)\n\n# or\n\nm=fit(SVM(logEuclidean; kernel=Linear), PTr, yTr)\n\n# ... using the Nu-Support Vector Classification\nm=fit(SVM(logEuclidean), PTr, yTr, kernel=Linear, svmtype=NuSVC)\n\n# or\n\nm=fit(SVM(logEuclidean; kernel=Linear, svmtype=NuSVC), PTr, yTr)\n\n# N.B. all other keyword arguments must be passed to the fit function\n# and not to the SVM constructor.\n\n\n\n\n\n\n\n","category":"function"},{"location":"cv/#StatsBase.predict","page":"fit, predict, cv","title":"StatsBase.predict","text":"function predict(model  :: MDMmodel,\n                 ðTe    :: â„Vector,\n                 what   :: Symbol = :labels;\n               verbose :: Bool = true,\n               â©     :: Bool = true)\n\nGiven an MDM model trained (fitted) on z classes and a testing set of k positive definite matrices ðTe of type â„Vector,\n\nif what is :labels or :l (default), return the predicted class labels for each matrix in ðTe, as an IntVector. For MDM models, the predicted class 'label' of an unlabeled matrix is the serial number of the class whose mean is the closest to the matrix (minimum distance to mean). The labels are '1' for class 1, '2' for class 2, etc;\n\nif what is :probabilities or :p, return the predicted probabilities for each matrix in ðTe to belong to a all classes, as a k-vector of z vectors holding reals in 0 1 (probabilities). The 'probabilities' are obtained passing to a softmax function minus the squared distances of each unlabeled matrix to all class means;\n\nif what is :f or :functions, return the output function of the model. The ratio of the squared distance to all classes to their geometric mean gives the 'functions'.\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nIt f â© is true (default), the computation of distances is multi-threaded.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: fit, cvAcc, predictErr.\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)\n\n# craete and fit an MDM model\nm=fit(MDM(Fisher), PTr, yTr)\n\n# predict labels\nyPred=predict(m, PTe, :l)\n\n# prediction error\npredErr=predictErr(yTe, yPred)\n\n# predict probabilities\npredict(m, PTe, :p)\n\n# output functions\npredict(m, PTe, :f)\n\n\n\n\n\n\nfunction predict(model   :: ENLRmodel,\n                 ðTe     :: Union{â„Vector, Matrix{Float64}},\n                 what    :: Symbol = :labels,\n                 fitType :: Symbol = :best,\n                 onWhich :: Int    = Int(fitType==:best);\n\t\t\ttransfer   :: Union{â„, Nothing} = nothing,\n            verbose    :: Bool = true,\n            â©        :: Bool = true)\n\nGiven an ENLR model trained (fitted) on 2 classes and a testing set of k positive definite matrices ðTe of type â„Vector,\n\nif what is :labels or :l (default), return the predicted class labels for each matrix in ðTe, as an IntVector. Those labels are '1' for class 1 and '2' for class 2;\n\nif what is :probabilities or :p, return the predicted probabilities for each matrix in ðTe to belong to each classe, as a k-vector of z vectors holding reals in 0 1 (probabilities). The 'probabilities' are obtained passing to a softmax function the output of the ENLR model and zero;\n\nif what is :f or :functions, return the output function of the model, which is the raw output of the ENLR model.\n\nIf fitType = :best (default), the best model that has been found by cross-validation is used for prediction.\n\nIf fitType = :path,\n\nif onWhich is a valid serial number for a model in the model.path,\n\nthen this model is used for prediction,\n\nif onWhich is zero, all model in the model.path will be used for\n\npredictions, thus the output will be multiplied by the number of models in model.path.\n\nArgumet onWhich has no effect if fitType = :best.\n\nnote: Nota Bene\nBy default, the fit function fits only the best model. If you want to use the fitType = :path option you need to invoke the fit function with optional keyword argument fitType=:path or fitType=:all. See the fit function for details.\n\nOptional keyword argument transfer can be used to specify the principal inverse square root (ISR) of a new mean to be used as base point for projecting the matrices in ðTe onto the tangent space. By default transfer is equal to nothing, implying that the base point will be the mean used to fit the model. Passing a new mean ISR allows the adaptation first described in Barachant et al.(2013). Typically transfer is the ISR of the mean of the matrices in ðTe or of a subset of them. Notice that this actually performs transfer learning by parallel transporting both the training and test data to the identity matrix as defined in Zanini et al.(2018) and later taken up in Rodrigues et al.(2019)ðŸŽ“.\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nIf â© = true (default) and ðTe is an â„Vector type, the projection onto the tangent space is multi-threaded.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: fit, cvAcc, predictErr.\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)\n\n# fit an ENLR lasso model and find the best model by cv\nm=fit(ENLR(Fisher), PTr, yTr)\n\n# predict labels from the best model\nyPred=predict(m, PTe, :l)\n# prediction error\npredErr=predictErr(yTe, yPred)\n\n# predict probabilities from the best model\npredict(m, PTe, :p)\n\n# output functions from the best model\npredict(m, PTe, :f)\n\n# fit a regularization path for an ENLR lasso model\nm=fit(ENLR(Fisher), PTr, yTr; fitType=:path)\n\n# predict labels using a specific model\nyPred=predict(m, PTe, :l, :path, 10)\n\n# predict labels for all models\nyPred=predict(m, PTe, :l, :path, 0)\n# prediction error for all models\npredErr=[predictErr(yTe, yPred[:, i]) for i=1:size(yPred, 2)]\n\n# predict probabilities from a specific model\npredict(m, PTe, :p, :path, 12)\n\n# predict probabilities from all models\npredict(m, PTe, :p, :path, 0)\n\n# output functions from specific model\npredict(m, PTe, :f, :path, 3)\n\n# output functions for all models\npredict(m, PTe, :f, :path, 0)\n\n\n\n\n\n\nfunction predict(model\t:: SVMmodel,\n                 ðTe\t :: Union{â„Vector, Matrix{Float64}},\n                 what\t:: Symbol = :labels;\n\t\t\ttransfer:: Union{â„, Nothing} = nothing,\n\t\t\tverbose\t:: Bool = true,\n\t\t\tâ©\t   :: Bool = true)\n\nGiven an SVM model trained (fitted) on 2 classes and a testing set of k positive definite matrices ðTe of type â„Vector,\n\nFor the meaning of arguments what, transfer and verbose, see the documentation of the predict function for the ENLR model.\n\nIf â© = true (default) and ðTe is an â„Vector type, the projection onto the tangent space will be multi-threaded. Also, the prediction of the LIBSVM function will be multi-threaded.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: fit, cvAcc, predictErr.\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)\n\n# fit an SVM model\nm=fit(SVM(Fisher), PTr, yTr)\n\n# predict labels\nyPred=predict(m, PTe, :l)\n# prediction error\npredErr=predictErr(yTe, yPred)\n\n# predict probabilities\npredict(m, PTe, :p)\n\n# output functions\npredict(m, PTe, :f)\n\n\n\n\n\n","category":"function"},{"location":"cv/#PosDefManifoldML.cvAcc","page":"fit, predict, cv","title":"PosDefManifoldML.cvAcc","text":"function cvAcc(model   :: MLmodel,\n               ðTr     :: â„Vector,\n               yTr     :: IntVector;\n           nFolds    :: Int       = min(10, length(yTr)Ã·3),\n           scoring   :: Symbol    = :b,\n           shuffle   :: Bool      = false,\n           verbose   :: Bool      = true,\n           outModels :: Bool      = false,\n           fitArgs...)\n\nCross-validation accuracy for a machine learning model: given an â„Vector ðTr holding k Hermitian matrices, an IntVector yTr holding the k labels for these matrices and the number of folds nFolds, return a CVacc structure.\n\noptional keyword arguments\n\nnFolds by default is set to the minimum between 10 and the number of observation Ã· 3 (integer division).\n\nIf scoring=:b (default) the balanced accuracy is computed. Any other value will make the function returning the regular accuracy. Balanced accuracy is to be preferred for unbalanced classes. For balanced classes the balanced accuracy reduces to the regular accuracy, therefore there is no point in using regular accuracy if not to avoid a few unnecessary computations when the class are balanced.\n\nFor the meaning of the shuffle argument (false by default), see function cvSetup, to which this argument is passed.\n\nIf verbose is true (default), information is printed in the REPL. This option is included to allow repeated calls to this function without crowding the REPL.\n\nif outModels is true return a 2-tuple holding a CVacc structure and a nFolds-vector of the model fitted for each fold, otherwise (default), return only a CVacc structure.\n\nfitArgs are optional keyword arguments that are passed to the fit function called for each fold of the cross-validation. For each machine learning model, all optional keyword arguments of their fit method are elegible to be passed here, however, the arguments listed in the following table for each model should not be passed. Note that if they are passed, they will be disabled:\n\nMDM/MDMF ENLR SVM\nverbose verbose verbose\nâ© â© â©\n meanISR meanISR\n meanInit meanInit\n fitType \n offsets \n lambda \n folds \n\nAlso, if you pass a w (weights for tangent space projection) argument, do not pass a vector of weights, just pass a symbol, e.g., w=:b for balancing weights.\n\nSee: notation & nomenclature, the â„Vector type.\n\nSee also: fit, predict.\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80)\n\n# perform 10-fold cross-validation using the minimum distance to mean classifier\ncv=cvAcc(MDM(Fisher), PTr, yTr)\n\n# ...using the lasso logistic regression classifier\ncv=cvAcc(ENLR(Fisher), PTr, yTr)\n\n# ...using the support-vector machine classifier\ncv=cvAcc(SVM(Fisher), PTr, yTr)\n\n# ...With a Polynomial kernel of order 3 (default)\ncv=cvAcc(SVM(Fisher), PTr, yTr; kernel=kernel.Polynomial)\n\n# perform 8-fold cross-validation instead\n# (and see that you can go pretty fast if your PC has 8 threads)\ncv=cvAcc(SVM(Fisher), PTr, yTr; nFolds=8)\n\n# ...balance the weights for tangent space projection\ncv=cvAcc(ENLR(Fisher), PTr, yTr; nFolds=8, w=:b)\n\n# perform another cross-validation shuffling the folds\ncv=cvAcc(ENLR(Fisher), PTr, yTr; shuffle=true, nFolds=8, w=:b)\n\n\n\n\n\n\n","category":"function"},{"location":"cv/#PosDefManifoldML.cvSetup","page":"fit, predict, cv","title":"PosDefManifoldML.cvSetup","text":"function cvSetup(k       :: Int,\n                 nCV     :: Int;\n                 shuffle :: Bool = false)\n\nGiven k elements and a parameter nCV, a nCV-fold cross-validation is obtained defining nCV permutations of k elements in nTest=knCV (integer division) elements for the test and k-nTest elements for the training, in such a way that each element is represented in only one permutation.\n\nSaid differently, given a length k and the number of desired cross-validations nCV, this function generates indices from the sequence of natural numbers 1k to obtain all nCV-fold cross-validation sets. Specifically, it generates nCV vectors of indices for generating test sets and nCV vectors of indices for geerating training sets.\n\nIf optional keyword argument shuffle is true, the sequence of natural numbers 1k is shuffled before running the function, thus in this case two successive runs of this function will give different cross-validation sets, hence different accuracy scores. By default shuffle is false, so as to allow exactly the same result in successive runs. Note that no random initialization for the shuffling is provided, so as to allow the replication of the same random sequences starting again the random generation from scratch.\n\nThis function is used in cvAcc. It constitutes the fundamental basis to implement customized cross-validation procedures.\n\nReturn the 2-tuple with:\n\nA vector of nCV vectors holding the indices for the training sets,\nA vector of nCV vectors holding the indices for the corresponding test sets.\n\nExamples\n\nusing PosDefManifoldML, PosDefManifold\n\ncvSetup(10, 2)\n# return:\n# (Array{Int64,1}[[6, 7, 8, 9, 10], [1, 2, 3, 4, 5]],\n#  Array{Int64,1}[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n\ncvSetup(10, 2, shuffle=true)\n# return:\n# (Array{Int64,1}[[5, 4, 6, 1, 9], [3, 7, 8, 2, 10]],\n#  Array{Int64,1}[[3, 7, 8, 2, 10], [5, 4, 6, 1, 9]])\n\ncvSetup(10, 3)\n# return:\n# (Array{Int64,1}[[4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6]],\n#  Array{Int64,1}[[1, 2, 3], [4, 5, 6], [7, 8, 9, 10]])\n\n\n\n\n\n\n","category":"function"},{"location":"enlr/#enlr.jl-1","page":"Elastic-Net Logistic Regression","title":"enlr.jl","text":"","category":"section"},{"location":"enlr/#","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"This unit implements the elastic net logistic regression (ENLR) machine learning model on the tangent space for symmetric positive definite (SDP) matrices, i.e., real PD matrices. This model features two hyperparameters: a user-defined alpha hyperparameter, in range 0 1, where Î±=0 allows a pure Ridge LR model and Î±=1 a pure lasso LR model and the lambda (regularization) hyperparameter. When the model is fitted, we can request to find the optimal lambda hyperparameter for the given training data using cross-validation and/or to find the regularization path.","category":"page"},{"location":"enlr/#","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"The lasso model (default) has enjoyed popularity in the field of brain-computer interaces due to the winning score obtained in six international data classification competitions.","category":"page"},{"location":"enlr/#","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"The ENLR model is implemented using the Julia GLMNet.jl package. See ðŸŽ“ for resources on GLMNet.jl and learn how to use purposefully this model.","category":"page"},{"location":"enlr/#","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"The fit, predict and cvAcc functions for the ENRL models are reported in the cv.jl unit, since those are homogeneous across all machine learning models. Here it is reported the ENLRmodel abstract type and the ENLR structure.","category":"page"},{"location":"enlr/#","page":"Elastic-Net Logistic Regression","title":"Elastic-Net Logistic Regression","text":"ENLRmodel\nENLR","category":"page"},{"location":"enlr/#PosDefManifoldML.ENLRmodel","page":"Elastic-Net Logistic Regression","title":"PosDefManifoldML.ENLRmodel","text":"abstract type ENLRmodel<:TSmodel end\n\nAbstract type for Elastic Net Logistic Rgression (ENLR) machine learning models. See MLmodel.\n\n\n\n\n\n","category":"type"},{"location":"enlr/#PosDefManifoldML.ENLR","page":"Elastic-Net Logistic Regression","title":"PosDefManifoldML.ENLR","text":"mutable struct ENLR <: ENLRmodel\n    metric      :: Metric = Fisher;\n    alpha       :: Real = 1.0\n    standardize :: Bool\n    intercept   :: Bool\n\tmeanISR     :: Union{â„Vector, Nothing}\n\tvecRange    :: UnitRange\n    featDim     :: Int\n\t# GLMNet Models\n    path        :: GLMNet.GLMNetPath\n    cvÎ»         :: GLMNet.GLMNetCrossValidation\n    best        :: GLMNet.GLMNetPath\nend\n\nENLR machine learning models are incapsulated in this mutable structure. Fields:\n\n.metric, of type Metric, is the metric that will be adopted to compute the mean used as base-point for tangent space projection. By default the Fisher metric is adopted. See mdm.jl for the available metrics. If the data used to train the model are not positive definite matrices, but Euclidean feature vectors, the .metric field has no use.\n\n.alpha is the hyperparameter in 0 1 trading-off the elestic-net model. Î±=0 requests a pure ridge model and Î±=1 a pure lasso model. By default, Î±=1 is specified (lasso model). This argument is usually passed as parameter to the fit function, defaulting therein to Î±=1 too. See the examples here below.\n\nAll other fields do not correspond to arguments passed upon creation of the model by the default creator. Instead, they are filled later when a model is created by the fit function:\n\nFor the content of fields standardize, intercept, meanISR and vecRange, please see the documentation of the fit function.\n\nif the data used to train the model are positive definite matrices, .featDim is the length of the vectorized tangent vectors. This is given by n(n+1)2 (integer division), where n is the dimension of the original PD matrices on which the model is applied once they are mapped onto the tangent space. If feature vectors are used to train the model, .featDim is the length of these vectors. If for fitting the model you have provided an optional keyword argument vecRange, .featDim will be reduced accordingly.\n\n.path is an instance of the following GLMNetPath structure of the GLMNet.jl package. It holds the regularization path that is created when the fit function is invoked with optional keyword parameter fitType = :path or = :all:\n\nstruct GLMNetPath{F<:Distribution}\n    family::F                        # Binomial()\n    a0::Vector{Float64}              # intercept values for each solution\n    betas::CompressedPredictorMatrix # coefficient values for each solution\n    null_dev::Float64                # Null deviance of the model\n    dev_ratio::Vector{Float64}       # R^2 values for each solution\n    lambda::Vector{Float64}          # lambda values for each solution\n    npasses::Int                     # actual number of passes over the\n                                     # data for all lamda values\nend\n\n.cvÎ» is an instance of the following GLMNetCrossValidation structure of the GLMNet.jl package. It holds information about the cross-validation used for estimating the optimal lambda hyperparameter by the fit function when this is invoked with optional keyword parameter fitType = :best (default) or = :all:\n\nstruct GLMNetCrossValidation\n    path::GLMNetPath            # the cv path\n    nfolds::Int                 # the number of folds for the cv\n    lambda::Vector{Float64}     # lambda values for each solution\n    meanloss::Vector{Float64}   # mean loss for each solution\n    stdloss::Vector{Float64}    # standard deviation of the mean losses\nend\n\n.best is an instance of the GLMNetPath structure (see above). It holds the model with the optimal lambda parameter found by cross-validation that is created by default when the fit function is invoked.\n\nExamples:\n\n# Note: creating models with the default creator is possible,\n# but not useful in general.\n\nusing PosDefManifoldML, PosDefManifold\n\n# create an empty lasso model\nm = ENLR(Fisher)\n\n# since the Fisher metric is the default metric,\n# this is equivalent to\nm = ENLR()\n\n# create an empty ridge model using the logEuclidean metric\nm = ENLR(logEuclidean; alpha=0)\n\n# Empty models can be passed as first argument of the `fit` function\n# to fit a model. For instance, this will fit a ridge model of the same\n# kind of `m` and put the fitted model in `m1`:\nm1=fit(m, PTr, yTr)\n\n# in general you don't need this machinery for fitting a model,\n# since you can specify a model by creating one on the fly:\nm2=fit(ENLR(logEuclidean; alpha=0), PTr, yTr)\n\n# which is equivalent to\nm2=fit(ENLR(logEuclidean), PTr, yTr; alpha=0)\n\n# note that, albeit model `m` has been created as a ridge model,\n# you have passed `m` and overwritten the `alpha` hyperparameter.\n# The metric, instead, cannot be overwritten.\n\n\n\n\n\n\n","category":"type"},{"location":"MainModule/#MainModule-1","page":"Main Module","title":"MainModule","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"This is the main unit containing the PosDefManifoldML module.","category":"page"},{"location":"MainModule/#dependencies-1","page":"Main Module","title":"dependencies","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"standard Julia packages external packages\nLinearAlgebra PosDefManifold\nStatistics GLMNet\nRandom Distributions\nDates LIBSVM\nStatsBase ","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"The main module does not contain functions.","category":"page"},{"location":"MainModule/#types-1","page":"Main Module","title":"types","text":"","category":"section"},{"location":"MainModule/#MLmodel-1","page":"Main Module","title":"MLmodel","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"As typical in machine learning packages, a type is created (a struct in Julia) to specify a ML model. Supertype","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"abstract type MLmodel end","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"is the abstract type for all machine learning models. Supertype","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"abstract type PDmodel<:MLmodel end","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"is the abstract type for all machine learning models acting on the positive definite (PD) manifold (for example, see MDM). Supertype","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"abstract type TSmodel<:MLmodel end","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"is the abstract type for all machine learning models acting on the tangent space (for example, see ENLR).","category":"page"},{"location":"MainModule/#IntVector-1","page":"Main Module","title":"IntVector","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"In all concerned functions class labels are given as a vector of integers, of type","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"IntVector=Vector{Int}","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"Class labels are natural numbers in 1z, where z is the number of classes.","category":"page"},{"location":"MainModule/#Tips-and-Tricks-1","page":"Main Module","title":"Tips & Tricks","text":"","category":"section"},{"location":"MainModule/#the-â„Vector-type-1","page":"Main Module","title":"the â„Vector type","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"Check this documentation on typecasting matrices.","category":"page"},{"location":"MainModule/#notation-and-nomenclature-1","page":"Main Module","title":"notation & nomenclature","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"Throughout the code of this package the following notation is followed:","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"scalars and vectors are denoted using lower-case letters, e.g., y,\nmatrices using upper case letters, e.g., X\nsets (vectors) of matrices using bold upper-case letters, e.g., ð—.","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"The following nomenclature is used consistently:","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"ðTr: a training set of positive definite matrices\nðTe: a testing set of positive definite matrices\nð: a generic set of positive definite matrices.\nw: a weights vector of non-negative real numbers\nyTr: a training set class labels vector of positive integer numbers (1, 2,...)\nyTe: a testing set class labels vector of positive integer numbers\ny: a generic class labels vector of positive integer numbers.\nz: number of classes of a ML model\nk: number of matrices in a set","category":"page"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"In the examples, bold upper-case letters are replaced by upper case letters in order to allow reading in the REPL.","category":"page"},{"location":"MainModule/#acronyms-1","page":"Main Module","title":"acronyms","text":"","category":"section"},{"location":"MainModule/#","page":"Main Module","title":"Main Module","text":"MDM: minimum distance to mean\nENLR: Elastic-Net Logistic Regression\nSVM: Support-Vector Machine\ncv: cross-validation","category":"page"},{"location":"svm/#svm.jl-1","page":"Support-Vector Machine","title":"svm.jl","text":"","category":"section"},{"location":"svm/#","page":"Support-Vector Machine","title":"Support-Vector Machine","text":"This unit implements several Suport-Vector Machine (SVM) machine learning models on the tangent space for symmetric positive definite (SDP) matrices, i.e., real PD matrices. Several models can be obtained with different combinations of the svmType and the kernel arguments when the model is fit. Optimal hyperparameters for the given training data are found using cross-validation.","category":"page"},{"location":"svm/#","page":"Support-Vector Machine","title":"Support-Vector Machine","text":"All SVM models are implemented using the Julia LIBSVM.jl package. See ðŸŽ“ for resources on the original LIBSVM C library and learn how to use purposefully these models.","category":"page"},{"location":"svm/#","page":"Support-Vector Machine","title":"Support-Vector Machine","text":"The fit, predict and cvAcc functions for the SVM models are reported in the cv.jl unit, since those are homogeneous across all machine learning models. Here it is reported the SVMmodel abstract type and the SVM structure.","category":"page"},{"location":"svm/#","page":"Support-Vector Machine","title":"Support-Vector Machine","text":"SVMmodel\nSVM","category":"page"},{"location":"svm/#PosDefManifoldML.SVMmodel","page":"Support-Vector Machine","title":"PosDefManifoldML.SVMmodel","text":"abstract type SVMmodel<:TSmodel end\n\nAbstract type for Support-Vector Machine (SVM) learning models. See MLmodel.\n\n\n\n\n\n","category":"type"},{"location":"svm/#PosDefManifoldML.SVM","page":"Support-Vector Machine","title":"PosDefManifoldML.SVM","text":"mutable struct SVM <: SVMmodel\n\t\tmetric\t\t:: Metric\n\t\tsvmType\t\t:: Type\n\t\tkernel\t\t:: Kernel.KERNEL\n\t\trescale\t\t:: Tuple\n\t\tmeanISR\t\t:: Union{â„Vector, Nothing}\n\t\tvecRange\t:: UnitRange\n\t\tfeatDim\t\t:: Int\n\t\tsvmModel #store the training model from the SVM library\n\nSVM machine learning models are incapsulated in this mutable structure. Fields:\n\n.metric, of type Metric, is the metric that will be adopted to compute the mean used as base-point for tangent space projection. By default the Fisher metric is adopted. See mdm.jl for the available metrics. If the data used to train the model are not positive definite matrices, but Euclidean feature vectors, the .metric field has no use.\n\nsvmType, a generic Type of SVM models used in LIBSVM. Available types are:\n\nSVC: C-Support Vector Classification. The fit time complexity is more  than quadratic with the number of observations. The multiclass support is handled  according to a one-vs-one scheme,\nNuSVC: Nu-Support Vector Classification. Similar to SVC but uses a  parameter to control the number of support vectors,\nOneClassSVM: Unsupervised outlier detection. Estimate the support of a high-dimensional distribution,\nEpsilonSVR: Epsilon-Support Vector Regression,\nNuSVR: Nu-Support Vector Regression.\n\nThe default is SVC, unless labels are not provided while fitting the model, in which case it defaults to OneClassSVM.\n\nkernel, a kernel type. Available kernels are declared as constants in the main module. They are:\n\nRadialBasis (default)\nLinear\nPolynomial\nSigmoid\nPrecomputed (not supported).\n\nAll other fields do not correspond to arguments passed upon creation of the model by the default creator. Instead, they are filled later when a model is created by the fit function:\n\nFor the content of field rescale please see the documentation of the fit function for the SVM model.\n\nFor the content of fields vecRange, please see the documentation of the fit function for the ENLR model.\n\nFor the content of the .meanISR and .featDim fields please see the documentation of the ENLR structure.\n\nsvmModel holds the model structure created by LIBSVM when the model is fitted (declared here).\n\nExamples:\n\n# Note: creating models with the default creator is possible,\n# but not useful in general.\n\nusing PosDefManifoldML, PosDefManifold\n\n# create an empty SVM model\nm = SVM(Fisher)\n\n# since the Fisher metric is the default metric,\n# this is equivalent to\nm = SVM()\n\n# create an empty SVM model using the logEuclidean metric\nm = SVM(logEuclidean)\n\n# generate some data\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);\n\n# Empty models can be passed as first argument of the `fit` function\n# to fit a model. For instance, this will fit an SVM model of the same\n# kind of `m` and put the fitted model in `m1`:\nm1=fit(m, PTr, yTr)\n\n# in general you don't need this machinery for fitting a model,\n# since you can specify a model by creating one on the fly:\nm2=fit(SVM(logEuclidean), PTr, yTr; kernel=Linear)\n\n# which is equivalent to\nm2=fit(m, PTr, yTr; kernel=Linear)\n\n# note that, albeit model `m` has been created as an SVM model\n# with the default kernel (RadialBasis),\n# you have passed `m` and overwritten the `kernel` type.\n# You can also overwrite the `svmType`.\n# The metric, instead, cannot be overwritten.\n\n\n\n\n\n\n","category":"type"},{"location":"#PosDefManifoldML-Documentation-1","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"","category":"section"},{"location":"#Requirements-and-Installation-1","page":"PosDefManifoldML Documentation","title":"Requirements & Installation","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Julia: version â‰¥ 1.3","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Packages: see the dependencies of the main module.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"To install the package, execute the following command in Julia's REPL:","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"    ]add PoSDefManifoldML","category":"page"},{"location":"#Reviewers-and-Contributors-1","page":"PosDefManifoldML Documentation","title":"Reviewers & Contributors","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Independent reviewers for both the code and the documentation are welcome. To contribute, please check the secion How to Contribute.","category":"page"},{"location":"#TroubleShoothing-1","page":"PosDefManifoldML Documentation","title":"TroubleShoothing","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Problem Solution\nThis error may appear at execution time \nthe first time an SVM model is fitted or is passed to the cvAcc function Discourse","category":"page"},{"location":"#About-the-Authors-1","page":"PosDefManifoldML Documentation","title":"About the Authors","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Marco Congedo, corresponding author, is a research scientist of CNRS (Centre National de la Recherche Scientifique), working at UGA (University of Grenoble Alpes). Contact: first name dot last name at gmail dot com","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Anton Andreev is a research engineer working at the same institution.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Saloni Jain is a student at the Indian Institute of Technology, Kharagpur, India.","category":"page"},{"location":"#Overview-1","page":"PosDefManifoldML Documentation","title":"Overview","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Riemannian geometry studies smooth manifolds, multi-dimensional curved spaces with peculiar geometries endowed with non-Euclidean metrics. In these spaces Riemannian geometry allows the definition of angles, geodesics (shortest path between two points), distances between points, centers of mass of several points, etc.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"In several fields of research such as computer vision and brain-computer interface, treating data in the manifold of positive definite matrices has allowed the introduction of machine learning approaches with remarkable characteristics, such as simplicity of use, excellent classification accuracy, as demonstrated by the winning score obtained in six international data classification competitions, and the ability to operate transfer learning (Congedo et al., 2017a, Brachant et al., 2012)ðŸŽ“.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"In this package we are concerned with making use of Riemannian Geometry for classifying data in the form of positive definite matrices (e.g., covariance matrices, Fourier cross-spectral matrices, etc.). This can be done in two ways: either directly in the manifold of positive definite matrices using Riemannian machine learning methods or in the tangent space, where traditional (Euclidean) machine learning methods apply (i.e., linear discriminant analysis, support-vector machine, logistic regression, random forest, etc.).","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"(Image: Figure 1) Figure 1","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Schematic representation of Riemannian classification. Data points are either natively positive definite matrices or are converted into this form. The classification can be performed by Riemannian methods in the manifold of positive definite matrices or by Euclidean methods after projection onto the tangent space.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Currently implemented models are:","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Acting on the manifold of PD matrices","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"the Riemannian minimum-distance to mean (MDM).","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Acting on the tangent space","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"elastic-net logistic regression (ENLR), including the pure Lasso and pure Ridge logistic regression;\nsupport-Vector machine (SVM), including C-Support Vector Classification (C-SVC), nu-SVC, one-class SVC, *Epsilon Support-Vector Regression** (SVR) and *nu SVR**.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"For a formal introduction to the manifold of positive definite matrices the reader is referred to the monography written by Bhatia(2007)ðŸŽ“.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"For an introduction to Riemannian geometry and an overview of mathematical tools implemented in the PostDefManifold package, which is used here, see Intro to Riemannian Geometry.","category":"page"},{"location":"#Code-units-1","page":"PosDefManifoldML Documentation","title":"Code units","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"PosDefManifoldML includes six code units (.jl files):","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Unit Description\nMainModule Main module, declaring constants and types\nmdm.jl Unit implementing the MDM( Minimum Distance to Mean) machine learning model\nenlr.jl Unit implementing the ENLR( Elastic Net Logistic Regression) model, including the LASSO and RIDGE LR\nsvm.jl Unit implementing the SVM (Support-Vector Machine) models\ncv.jl Unit implementing cross-validation procedures\ntools.jl Unit containing general tools useful for machine learning and internal functions","category":"page"},{"location":"#-1","page":"PosDefManifoldML Documentation","title":"ðŸŽ“","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"References","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2012) Multi-class Brain Computer Interface Classification by Riemannian Geometry, IEEE Transactions on Biomedical Engineering, 59(4), 920-928.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2013) Classification of covariance matrices using a Riemannian-based kernel for BCI applications, Neurocomputing, 112, 172-178.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"R. Bhatia (2007) Positive Definite Matrices, Princeton University press.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"M. Congedo, A. Barachant, R. Bhatia R (2017a) Riemannian Geometry for EEG-based Brain-Computer Interfaces; a Primer and a Review, Brain-Computer Interfaces, 4(3), 155-174.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"M. Congedo, A. Barachant, E. Kharati Koopaei (2017b) Fixed Point Algorithms for Estimating Power Means of Positive Definite Matrices, IEEE Transactions on Signal Processing, 65(9), 2211-2220.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Rodrigues PLC, Jutten C, Congedo M (2019) Riemannian Procrustes Analysis : Transfer Learning for Brain-Computer Interfaces, IEEE Transactions on Biomedical Engineering, 66(8), 2390-2401.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"P. Zanini P, M. Congedo, C. Jutten, S. Said, Y. Berthoumieu (2018) Transfer Learning: a Riemannian geometry framework with applications to Brain-Computer Interfaces, IEEE Transactions on Biomedical Engineering, 65(5), 1107-1116.","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Resources on GLMNet","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"webinar by Trevor Hastie","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Glmnet vignette","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Glmnet in R, documentation","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Julia wrapper for GLMNet","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"A more advanced wrapper for GLMNet","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Resources on LIBSVM","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"official page","category":"page"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"github","category":"page"},{"location":"#Contents-1","page":"PosDefManifoldML Documentation","title":"Contents","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"Pages = [       \"index.md\",\n\t\t\t\t\t\t\t\t\"tutorials.md\",\n                \"MainModule.md\",\n                \"mdm.md\",\n                \"enlr.md\",\n\t\t\t\t\t\t\t\t\"svm.jl\",\n\t\t\t\t\t\t\t\t\"cv.md\",\n\t\t\t\t\t\t\t\t\"tools.md\",\n\t\t]\nDepth = 1","category":"page"},{"location":"#Index-1","page":"PosDefManifoldML Documentation","title":"Index","text":"","category":"section"},{"location":"#","page":"PosDefManifoldML Documentation","title":"PosDefManifoldML Documentation","text":"","category":"page"},{"location":"tools/#tools.jl-1","page":"Tools","title":"tools.jl","text":"","category":"section"},{"location":"tools/#","page":"Tools","title":"Tools","text":"This unit implements tools that are useful for building Riemannian and Euclidean machine learning classifiers.","category":"page"},{"location":"tools/#Content-1","page":"Tools","title":"Content","text":"","category":"section"},{"location":"tools/#","page":"Tools","title":"Tools","text":"function description\ntsMap project data on a tangent space to apply Euclidean ML models therein\ntsWeights generator of weights for tagent space mapping\ngen2ClassData generate 2-class positive definite matrix data for testing Riemannian ML models\npredictErr prediction error given a vector of true labels and a vector of predicted labels\nrescale! Rescale the rows of a real matrix to be in range [a, b]","category":"page"},{"location":"tools/#","page":"Tools","title":"Tools","text":"tsMap\ntsWeights\ngen2ClassData\npredictErr\nrescale!","category":"page"},{"location":"tools/#PosDefManifoldML.tsMap","page":"Tools","title":"PosDefManifoldML.tsMap","text":"function tsMap(\tmetric :: Metric,\n\t\tð      :: â„Vector;\n\t\tw    \t  :: Vector\t= [],\n\t\tâœ“w   \t  :: Bool = true,\n\t\tâ©   \t :: Bool = true,\n\t\tmeanISR   :: Union{â„, Nothing}  = nothing,\n\t\tmeanInit  :: Union{â„, Nothing}  = nothing,\n\t\ttol       :: Real               = 0.,\n\t\ttranspose :: Bool   \t\t\t = true,\n\t\tvecRange  :: UnitRange          = 1:size(ð[1], 1))\n\nThe tangent space mapping of positive definite matrices P_i, i=1k with mean G, once those points have been parallel transported to the identity matrix, is given by:\n\nS_i=textrmlog(G^-12 P_i G^-12).\n\nGiven a vector of k matrices ð flagged by julia as Hermitian, return a matrix X with such tangent vectors of the matrices in ð vectorized as per the vecP operation.\n\nThe mean G of the matrices in ð is found according to the specified metric, of type Metric. A natural choice is the Fisher metric. If the metric is Fisher, logdet0 or Wasserstein the mean is found with an iterative algorithm with tolerance given by optional keyword argument tol. By default tol is set by the function mean. For those iterative algorithms a particular initialization can be provided as an Hermitian matrix by optional keyword argument meanInit.\n\nA set of k optional non-negative weights w can be provided for computing a weighted mean G, for any metrics. If w is non-empty and optional keyword argument âœ“w is true (default), the weights are normalized so as to sum up to 1, otherwise they are used as they are passed and should be already normalized. This option is provided to allow calling this function repeatedly without normalizing the same weights vector each time.\n\nIf an Hermitian matrix is provided as optional keyword argument meanISR, then the mean G is not computed, intead this matrix is used directly in the formula as the inverse square root (ISR) G^-12. If meanISR is provided, arguments tol and meanInit have no effect whatsoever.\n\nIf meanISR is not provided, return the 2-tuple (X G^-12), otherwise return only matrix X.\n\nIf an UnitRange is provided with the optional keyword argument vecRange, the vectorization concerns only the columns (or rows) of the matrices ð specified by the range.\n\nIf optional keyword argument transpose is true (default), X holds the k vectorized tangent vectors in its rows, otherwise they are arranged in its columns. The dimension of the rows in the former case and of the columns is the latter case is n(n+1)2 (integer division), where n is the size of the matrices in ð, unless a vecRange spanning a subset of the columns or rows of the matrices in ð has been provided, in which case the dimension will be smaller. (see vecP ).\n\nif optional keyword argument â© if true (default), the computation of the mean and the projection on the tangent space are multi-threaded. Multi-threading is automatically disabled if the number of threads Julia is instructed to use is 2 or 2k.\n\nExamples:\n\nusing PosDefManifoldML\n\n# generate four random symmetric positive definite 3x3 matrices\nPset = randP(3, 4)\n\n# project and vectorize in the tangent space\nX, Gâ»Â½ = tsMap(Fisher, Pset)\n\n# X is a 4x6 matrix, where 6 is the size of the\n# vectorized tangent vectors (n=3, n*(n+1)/2=6)\n\n# If repeated calls have to be done, faster computations are obtained\n# providing the inverse square root of the matrices in Pset, e.g.,\nX1 = tsMap(Fisher, â„Vector(Pset[1:2]); meanISR = Gâ»Â½)\nX2 = tsMap(Fisher, â„Vector(Pset[3:4]); meanISR = Gâ»Â½)\n\nSee: the â„Vector type.\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.tsWeights","page":"Tools","title":"PosDefManifoldML.tsWeights","text":"function tsWeights(y::Vector{Int}; classWeights=[])\n\nGiven an IntVector of labels y, return a vector of weights summing up to 1 such that the overall weight is the same for all classes (balancing). This is useful for machine learning models in the tangent space with unbalanced classes for computing the mean, that is, the base point to map PD matrices onto the tangent space. For this mapping, giving equal weights to all observations actually overweights the larger classes and downweight the smaller classes.\n\nClass labels for n classes must be the first n natural numbers, that is, 1 for class 1, 2 for class 2, etc. The labels in y can be provided in any order.\n\nif a vector of n weights is specified as optional keyword argument classWeights, the overall weights for each class will be first balanced (see here above), then weighted by the classWeights. This allow user-defined control of weighting independently from the number of observations in each class. The weights in classWeights can be any integer or real non-negative numbers. The returned weight vector will nonetheless sum up to 1.\n\nWhen you invoke the fit function for tangent space models you don't actually need this function, as you can invoke it implicitly passing symbol :balanced (or just :b) or a tuple with the class weights as optional keyword argument w.\n\nExamples\n\n# generate some data; the classes are unbalanced\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1)\n\n# Fit an ENLR lasso model and find the best model by cross-validation\n# balancing the weights for tangent space mapping\nm=fit(ENLR(), PTr, yTr; w=tsWeights(yTr))\n\n# A simpler syntax is\nm=fit(ENLR(), PTr, yTr; w=:balanced)\n\n# to balance the weights and then give overall weight 0.5 to class 1\n# and 1.5 to class 2:\nm=fit(ENLR(), PTr, yTr; w=(0.5, 1.5))\n\n# which is equivalent to\nm=fit(ENLR(), PTr, yTr; w=tsWeights(yTr; classWeights=(0.5, 1.5)))\n\n\n# This is how it works:\n\njulia> y=[1, 1, 1, 1, 2, 2]\n6-element Array{Int64,1}:\n 1\n 1\n 1\n 1\n 2\n 2\n\nWe want the four observations of class 1 to count as much\nas the two observations of class 2.\n\njulia> tsWeights(y)\n6-element Array{Float64,1}:\n 0.125\n 0.125\n 0.125\n 0.125\n 0.25\n 0.25\n\ni.e., 0.125*4 = 1.25*2\nand all weights sum up to 1\n\nNow, suppose we want to give to class 2 a weight\nfour times bigger as compared to class 1:\n\njulia> tsWeights(y, classWeights=[1, 4])\n6-element Array{Float64,1}:\n 0.05\n 0.05\n 0.05\n 0.05\n 0.4\n 0.4\n\nand, again, all weights sum up to 1\n\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.gen2ClassData","page":"Tools","title":"PosDefManifoldML.gen2ClassData","text":"function gen2ClassData(n        ::  Int,\n                       k1train  ::  Int,\n                       k2train  ::  Int,\n                       k1test   ::  Int,\n                       k2test   ::  Int,\n                       separation :: Real = 0.1)\n\nGenerate a training set of k1train+k2train and a test set of k1test+k2test symmetric positive definite matrices. All matrices have size nxn.\n\nThe training and test sets can be used to train and test any MLmodel.\n\nseparation is a coefficient determining how well the two classs are separable; the higher it is, the more separable the two classes are. It must be in [0, 1] and typically a value of 0.5 already determines complete separation.\n\nReturn a 4-tuple with\n\nan â„Vector holding the k1train+k2train matrices in the training set,\nan â„Vector holding the k1test+k2test matrices in the test set,\na vector holding the k1train+k2train labels (integers) corresponding to the matrices of the training set,\na vector holding the k1test+k2test labels corresponding to the matrices of the test set (1 for class 1 and 2 for class 2).\n\nExamples\n\nusing PosDefManifoldML\n\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.25)\n\n# PTr=training set: 30 matrices for class 1 and 40 matrices for class 2\n# PTe=testing set: 60 matrices for class 1 and 80 matrices for class 2\n# all matrices are 10x10\n# yTr=a vector of 70 labels for the training set\n# yTe=a vector of 140 labels for the testing set\n\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.predictErr","page":"Tools","title":"PosDefManifoldML.predictErr","text":"function predictErr(yTrue::IntVector, yPred::IntVector;\n\t          \t\tdigits::Int=3))\n\nReturn the percent prediction error given a vector of true labels and a vector of predicted labels.\n\nThe order of arguments does not matter.\n\nThe error is rounded to the number of optional keyword argument digits, 3 by default.\n\nSee predict\n\nExamples\n\nusing PosDefManifoldML\npredictErr([1, 1, 2, 2], [1, 1, 1, 2])\n# return: 25.0\n\n\n\n\n\n","category":"function"},{"location":"tools/#PosDefManifoldML.rescale!","page":"Tools","title":"PosDefManifoldML.rescale!","text":"function rescale!(\tX::Matrix{T},\n\t\t\t\t\tbounds::Tuple=(-1, 1);\n\t\t\t\t\tdims::Int=1) where T<:Real\n\nRescale the columns or the rows of real matrix X to be in range [a, b], where a and b are the first and seconf elements of tuple bounds.\n\nBy default rescaling apply to the columns. Use dims=2 for rescaling the rows.\n\nThis function is used, for instance, by the SVM fit and predict functions.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/#Tutorial-1","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"If you didn't, please read first the Overview.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"PosDefManifoldML features two bacic pipelines:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"1) a machine learning (ML) model is first fit (trained), then it can be used to predict the labels of testing data or the probability of the data to belong to each class. The raw prediction function of the models is available as well.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"2) a k-fold cross-validation procedure allows to estimate directly the accuracy of ML models and compare them.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"What PosDefManifoldML does for you is to allow an homogeneous syntax to run these two pipelines for all implemented ML models, it does not matter if they act directly on the manifold of positive definite matrices or on the tangent space. Furthermore, models acting on the tangent space can take as input Euclidean feature vectors instead of positive definite matrices, thus they can be used in many more situations.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"get data","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"A real data example will be added soon. For now, let us create simulated data for a 2-class example. First, let us create symmetric positive definite matrices (real positive definite matrices):","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using PosDefManifoldML, PosDefManifold\n\nPTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"PTr is the simulated training set, holding 30 matrices for class 1 and 40 matrices for class 2\nPTe is the testing set, holding 60 matrices for class 1 and 80 matrices for class 2.\nyTr is a vector of 70 labels for the training set\nyTe is a vector of 140 labels for the testing set","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"All matrices are of size 10x10.","category":"page"},{"location":"tutorial/#Example-using-the-MDM-model-1","page":"Tutorial","title":"Example using the MDM model","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The minimum distance to mean (MDM) classifier is an example of classifier acting directly on the manifold. It is deterministic and no hyperparameter tuning is requested.","category":"page"},{"location":"tutorial/#MDM-Pipeline-1.-(fit-and-predict)-1","page":"Tutorial","title":"MDM Pipeline 1. (fit and predict)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Craete and fit an MDM model","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"An MDM model is created and fit with training data such as","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m = fit(MDM(Fisher), PTr, yTr)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"where Fisher is the usual choice of a Metric as declared in the parent package PosDefManifold.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Since the Fisher metric is the default (for all ML models), the above is equivalent to:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m = fit(MDM(), PTr, yTr)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to adopt another metric:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1 = fit(MDM(logEuclidean), PTr, yTr)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Predict (classify data)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to predict the labels of unlabeled data (which we have stored in PTe), we invoke","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m, PTe, :l)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The prediction error in percent can be retrived with","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"predictErr(yTe, yPred)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"or by","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"predictErr(yPred, yTe)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"where in yTe we have stored the true labels for the matrices in PTe.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"If instead we wish to estimate the probabilities for the matrices in PTe of belonging to each class:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"predict(m, PTe, :p)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Finally, the output functions of the MDM are obtaine by (see predict)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"predict(m, PTe, :f)","category":"page"},{"location":"tutorial/#MDM-Pipeline-2.-(cross-validation)-1","page":"Tutorial","title":"MDM Pipeline 2. (cross-validation)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The balanced accuracy estimated by a k-fold cross-validation is obtained such as (10-fold by default)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cv = cvAcc(MDM(), PTr, yTr)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Struct cv has been created and therein you have access to average accuracy and confusion matrix as well as accuracies and confusion matrices for all folds. For example, print the average confusion matrix:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cv.avgCnf","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"See CVacc for details on the fields of cross-validation objects.","category":"page"},{"location":"tutorial/#Example-using-the-ENLR-model-1","page":"Tutorial","title":"Example using the ENLR model","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The elastic net logistic regression (ENLR) classifier is an example of classifier acting on the tangent space. Besides the metric (see above) used to compute a base-point for projecting the data onto the tangent space, it has a parameter alpha and an hyperparameter lambda. The alpha parameter allows to trade off between a pure ridge LR model (Î±=0) and a pure lasso LR model (Î±=1), which is the default. Given an alpha value, the model is fitted with a number of values for the Î» (regularization) hyperparameter. Thus, differently from the previous example, tuning the Î» hyperparameter is necessary.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Also, keep in mind that the fit and predict methods for ENLR models accept optional keyword arguments that are specific to this model.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"get data","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Let us get some simulated data (see the previous example for explanations).","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);","category":"page"},{"location":"tutorial/#ENLR-Pipeline-1.-(fit-and-predict)-1","page":"Tutorial","title":"ENLR Pipeline 1. (fit and predict)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Craete and fit ENLR models","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"By default, the Fisher metric ic adopted and a lasso model is fitted. The best value for the lambda hyperparameter is found by cross-validation:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1 = fit(ENLR(), PTr, yTr; w=:balanced)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Notice that since the class are unbalanced, with the w=:balanced argument (we may as well just use w=:b) we have requested to compute a balanced mean for projecting the matrices in PTr onto the tangent space.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The optimal value of lambda for this training data is","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1.best.lambda","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"As in GLMNet.jl, the intercept and beta terms are retrived by","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1.best.a0\nm1.best.betas","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The number of non-zero beta coefficients can be found for example by","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"length(unique(m1.best.betas))-1","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to fit a ridge LR model:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m2 = fit(ENLR(), PTr, yTr; w=:b, alpha=0)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Values of alpha in range (0 1) fit instead an elastic net LR model. In the following we also request not to standardize predictors:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m3 = fit(ENLR(Fisher), PTr, yTr; w=:b, alpha=0.9, standardize=false)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to find the regularization path we use the fitType keyword argument:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1 = fit(ENLR(Fisher), PTr, yTr; w=:b, fitType=:path)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The values of lambda along the path are given by","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1.path.lambda","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"We can also find the best value of the lambda hyperparameter and the regularization path at once, calling:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1 = fit(ENLR(Fisher), PTr, yTr; w=:b, fitType=:all)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"For changing the metric see MDM Pipeline 1. (fit and predict).","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"See the documentation of the fit ENLR method for details on all available optional arguments.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Classify data (predict)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"For prediction, we can request to use the best model (optimal lambda), to use a specific model of the regularization path or to use all the model in the regalurization path. Note that with the last call we have done here above both the .best and .path field of the m1 structure have been created.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"By default, prediction is obtained from the best model and we request to predict the labels:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe)\n\n# prediction error in percent\npredictErr(yPred, yTe)\n\n# predict probabilities of matrices in `PTe` to belong to each class\npredict(m1, PTe, :p)\n\n# output function of the model for each class\npredict(m1, PTe, :f)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to request the predition of labels for all models in the regularization path:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe, :l, :path, 0)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"while for a specific model in the path (e.g., the 1Oth model):","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe, :l, :path, 10)","category":"page"},{"location":"tutorial/#ENLR-Pipeline-2.-(cross-validation)-1","page":"Tutorial","title":"ENLR Pipeline 2. (cross-validation)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The balanced accuracy estimated by a k-fold cross-validation is obtained with the exact same basic syntax for all models, with some specific optional keyword arguments for models acting in the tangent space, for example:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cv = cvAcc(ENLR(), PTr, yTr; w=:b)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to perform another cross-validation arranging the training data differently in the folds:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cv = cvAcc(ENLR(), PTr, yTr; w=:b, shuffle=true)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"This last command can be invoked repeatedly.","category":"page"},{"location":"tutorial/#Example-using-SVM-models-1","page":"Tutorial","title":"Example using SVM models","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The SVM ML model actually encapsulates several support-vector classification and support-vector regression models. Here we are here concerned with the former, which include the C-Support Vector Classification (SVC), the Nu-Support Vector Classification (NuSVC), similar to SVC but using a parameter to control the number of support vectors, and the One-Class SVM (OneClassSVM), which is used in general for unsupervised outlier detection. They all act in the tangent space like ENLR models. Besides the metric (see MDM Pipeline 1. (fit and predict)) used to compute a base-point for projecting the data onto the tangent space and the type of SVM model (the svmType, = SVC (default), NuSVC or OneClassSVM), the main parameter is the kernel. Avaiable kernels are:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"RadialBasis (default)\nLinear\nPolynomial\nSigmoid","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Several parameters are available for building all these kernels besides the linear one, which has no parameter. Like for ENLR, for SVM models also an hyperparameter is to be found by cross-validation.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"get data","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Let us get some simulated data as in the previous examples.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);","category":"page"},{"location":"tutorial/#SVM-Pipeline-1.-(fit-and-predict)-1","page":"Tutorial","title":"SVM Pipeline 1. (fit and predict)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Craete and fit SVM models","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"By default, a C-Support Vector Classification model is fitted:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m1 = fit(SVM(), PTr, yTr; w=:b)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Notice that as for the example above with for ENLR model, we have requested to compute a balanced mean for projecting the matrices in PTr onto the tangent space.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In order to fit a Nu-Support Vector Classification model:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m2 = fit(SVM(), PTr, yTr; w=:b, svmType=NuSVC)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"For using other kernels, e.g.:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m3 = fit(SVM(), PTr, yTr; w=:b, svmType=NuSVC, kernel=Linear)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In the following we also request not to rescale predictors:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"m3 = fit(SVM(), PTr, yTr;\n        w=:b, svmType=NuSVC, kernel=Linear, rescale=())","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"By default the Fisher metric is used. For changing it see MDM Pipeline 1. (fit and predict).","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"See the documentation of the fit ENLR method for details on all available optional arguments.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Classify data (predict)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Just the same as for the other models:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"yPred=predict(m1, PTe)\n\n# prediction error in percent\npredictErr(yPred, yTe)\n\n# predict probabilities of matrices in `PTe` to belong to each class\npredict(m1, PTe, :p)\n\n# output function of the model for each class\npredict(m1, PTe, :f)","category":"page"},{"location":"tutorial/#SVM-Pipeline-2.-(cross-validation)-1","page":"Tutorial","title":"SVM Pipeline 2. (cross-validation)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Again, the balanced accuracy estimated by a k-fold cross-validation is obtained with the exact same basic syntax for all models, with some specific optional keyword arguments for models acting in the tangent space, for example:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cv = cvAcc(SVM(), PTr, yTr; w=:b)","category":"page"}]
}
