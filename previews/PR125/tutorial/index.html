<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · PosDefManifoldML</title><meta name="title" content="Tutorial · PosDefManifoldML"/><meta property="og:title" content="Tutorial · PosDefManifoldML"/><meta property="twitter:title" content="Tutorial · PosDefManifoldML"/><meta name="description" content="Documentation for PosDefManifoldML."/><meta property="og:description" content="Documentation for PosDefManifoldML."/><meta property="twitter:description" content="Documentation for PosDefManifoldML."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="PosDefManifoldML logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">PosDefManifoldML</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">PosDefManifoldML Documentation</a></li><li class="is-active"><a class="tocitem" href>Tutorial</a><ul class="internal"><li><a class="tocitem" href="#Examples-using-the-MDM-model"><span>Examples using the MDM model</span></a></li><li><a class="tocitem" href="#Examples-using-the-ENLR-model"><span>Examples using the ENLR model</span></a></li><li><a class="tocitem" href="#Examples-using-SVM-models"><span>Examples using SVM models</span></a></li></ul></li><li><a class="tocitem" href="../MainModule/">Main Module</a></li><li><a class="tocitem" href="../tools/">Tools</a></li><li><span class="tocitem">ML Models: PD Manifold</span><ul><li><a class="tocitem" href="../mdm/">Minimum Distance to Mean</a></li></ul></li><li><span class="tocitem">ML Models: PD Tangent Space</span><ul><li><a class="tocitem" href="../enlr/">Elastic-Net Logistic Regression</a></li><li><a class="tocitem" href="../svm/">Support-Vector Machine</a></li></ul></li><li><a class="tocitem" href="../cv/">Fit, Predict, CV</a></li><li><span class="tocitem">Statistics</span><ul><li><a class="tocitem" href="../stats_descriptive/">Descriptive</a></li><li><a class="tocitem" href="../stats_inferential/">Inferential</a></li></ul></li><li><a class="tocitem" href="../conditioners/">Conditioners &amp; Pipelines</a></li><li><a class="tocitem" href="../contribute/">How to contribute</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="..." title="View the repository"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">Repository</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Tutorial"><a class="docs-heading-anchor" href="#Tutorial">Tutorial</a><a id="Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#Tutorial" title="Permalink"></a></h1><p>If you didn&#39;t, please read first the <a href="../#Overview">Overview</a>.</p><p><em>PosDefManifoldML</em> features two bacic machine learning modes of operation:</p><ul><li><p><strong>train-test</strong>: a machine learning (ML) model is first <strong>fitted</strong> (trained), then it can be used to <strong>predict</strong> (test) the <em>labels</em> of testing data or the <em>probability</em> of the data to belong to each class. The raw prediction function of the models is available as well.</p></li><li><p>a <strong>k-fold cross-validation</strong> procedure allows to estimate the <strong>accuracy</strong> of ML models and compare them.</p></li></ul><p>The train-test mode is useful in <strong>cross-subject</strong> and <strong>cross-session</strong> settings,  while cross-validation is the standard for <strong>within-session</strong> settings.</p><p>What <em>PosDefManifoldML</em> does for you is to allow an homogeneous syntax to operate in these two modes for all implemented ML models, it does not matter if they act directly on the manifold of positive definite matrices or on the tangent space. It also features </p><ul><li><strong>Pre-conditining pipelines</strong>, which can drastically reduce the execution time</li><li><strong>Adaptation</strong> techniques, which, besides being very useful in cross-session and cross-subject settings, </li></ul><p>are instrumental for implementing on-line modes of operation.</p><p>Note that models acting on the tangent space can take as input Euclidean feature vectors instead of positive definite matrices, thus they can be used in many more situations.</p><p><strong>Get data</strong></p><p>Let us create simulated data for a <strong>2-class example</strong>. First, let us create symmetric positive definite matrices (real positive definite matrices):</p><pre><code class="language-julia hljs">using PosDefManifoldML, PosDefManifold

PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);</code></pre><ul><li><code>PTr</code> is the simulated training set, holding 30 matrices for class 1 and 40 matrices for class 2</li><li><code>PTe</code> is the testing set, holding 60 matrices for class 1 and 80 matrices for class 2.</li><li><code>yTr</code> is a vector of 70 labels for the training set</li><li><code>yTe</code> is a vector of 140 labels for the testing set</li></ul><p>All matrices are of size 10x10.</p><h2 id="Examples-using-the-MDM-model"><a class="docs-heading-anchor" href="#Examples-using-the-MDM-model">Examples using the MDM model</a><a id="Examples-using-the-MDM-model-1"></a><a class="docs-heading-anchor-permalink" href="#Examples-using-the-MDM-model" title="Permalink"></a></h2><p>The <strong>minimum distance to mean (MDM)</strong> classifier is an example of classifier acting directly on the manifold. It is deterministic and no hyperparameter tuning is needed.</p><h3 id="MDM-train-test"><a class="docs-heading-anchor" href="#MDM-train-test">MDM train-test</a><a id="MDM-train-test-1"></a><a class="docs-heading-anchor-permalink" href="#MDM-train-test" title="Permalink"></a></h3><p><strong>Craete and fit an MDM model</strong></p><p>An MDM model is created and fitted with training data such as</p><pre><code class="language-julia hljs">m = fit(MDM(Fisher), PTr, yTr)</code></pre><p>where <code>Fisher</code> (affine-invariant) is the usual choice of a <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#Metric::Enumerated-type-1">Metric</a> as declared in the parent package <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/">PosDefManifold</a>.</p><p>Since the Fisher metric is the default (for all ML models), the above is equivalent to:</p><pre><code class="language-julia hljs">m = fit(MDM(), PTr, yTr)</code></pre><p>In order to adopt another metric:</p><pre><code class="language-julia hljs">m1 = fit(MDM(logEuclidean), PTr, yTr)</code></pre><p><strong>Predict (classify data)</strong></p><p>In order to predict the labels of unlabeled data (which we have stored in <code>PTe</code>), we invoke</p><pre><code class="language-julia hljs">yPred=predict(m, PTe, :l)</code></pre><p>The prediction error in percent can be retrived with</p><pre><code class="language-julia hljs">predictErr(yTe, yPred)</code></pre><p>the predicton accuracy as</p><pre><code class="language-julia hljs">predictAcc(yTe, yPred)</code></pre><p>and the confusion matrix as</p><pre><code class="language-julia hljs">confusionMat(yTe, yPred)</code></pre><p>where in <code>yTe</code> we have stored the <em>true</em> labels for the matrices in <code>PTe</code>.</p><p>If instead we wish to estimate the probabilities for the matrices in <code>PTe</code> of belonging to each class:</p><pre><code class="language-julia hljs">predict(m, PTe, :p)</code></pre><p>Finally, the output functions of the MDM are obtaine by (see <a href="../cv/#StatsAPI.predict"><code>predict</code></a>)</p><pre><code class="language-julia hljs">predict(m, PTe, :f)</code></pre><h3 id="MDM-cross-validation"><a class="docs-heading-anchor" href="#MDM-cross-validation">MDM cross-validation</a><a id="MDM-cross-validation-1"></a><a class="docs-heading-anchor-permalink" href="#MDM-cross-validation" title="Permalink"></a></h3><p>The balanced accuracy estimated by a <em>k-fold cross-validation</em> is obtained such as (10-fold by default)</p><pre><code class="language-julia hljs">cv = crval(MDM(), PTr, yTr)</code></pre><p>As for all functions in the Julia language, the first time you run a function it is compiled, so it is slow. To appreciate the speed, run it again </p><pre><code class="language-julia hljs">cv = crval(MDM(), PTr, yTr)</code></pre><p>Struct <code>cv</code> has been created and therein you have access to average accuracy and confusion matrix as well as accuracies and confusion matrices for all folds. For example, print the average confusion matrix (expressed in <em>proportions</em>):</p><pre><code class="language-julia hljs">cv.avgCnf</code></pre><p>See <a href="../cv/#PosDefManifoldML.CVres"><code>CVres</code></a> for details on the fields of cross-validation objects.</p><h3 id="MDM-adaptation"><a class="docs-heading-anchor" href="#MDM-adaptation">MDM adaptation</a><a id="MDM-adaptation-1"></a><a class="docs-heading-anchor-permalink" href="#MDM-adaptation" title="Permalink"></a></h3><p>Let&#39;s see how to adapt a pre-conditioning pipeline. Suppose you have data from two  sessions or two subjects, <code>s1</code> and <code>s2</code>. We want to use <code>s1</code> to train a machine learning model on the tanget space and <code>s2</code> to test it. A pipeline is fitted sor <code>s1</code> and we want this pipeline to adapt to <code>s2</code> for testing. If the pipeline includes a recentering pre-conditioner, we need to make sure that the dimensionality reduction determined on <code>s2</code> is that same as in `s1.</p><p><strong>Get data</strong> Let us get some simulated data. We generate random data and labels for session (or subject) 1 and 2.</p><pre><code class="language-julia hljs">Ps1, Ps2, ys1, ys2 = gen2ClassData(10, 30, 40, 60, 80);</code></pre><p><strong>Define the pre-conditioning pipeline for s1</strong></p><pre><code class="language-julia hljs">p = @→ Recenter(; eVar=0.999) → Compress → Shrink(Fisher; radius=0.02)</code></pre><p><strong>Fit an MDM model on s1 using the pipeline</strong></p><pre><code class="language-julia hljs">m = fit(MDM(), Ps1, ys1; pipeline = p)</code></pre><p>The fitted pipeline with all learnt parameters is stored in model <code>m</code>. Instead of transforming the data in <code>s2</code> using this pipeline, which is the default behavior of the <code>predict</code> function, let us define the same pipeline with a dimensionality  reduction parameter fixed as it has been learnt on <code>s1</code>.  This way this paramater cannot change and the transformed matrices in <code>s1</code> and <code>s2</code> will have equal size. That is, we allow adaptation of all parameters, but force the same dimension.</p><pre><code class="language-julia hljs">p = @→ Recenter(; eVar=dim(m.pipeline)) → Compress → Shrink(Fisher; radius=0.02)</code></pre><p><strong>Fit the pipeline to s2 and predict</strong></p><pre><code class="language-julia hljs">predict(m, Ps2, :l; pipeline=p)</code></pre><h2 id="Examples-using-the-ENLR-model"><a class="docs-heading-anchor" href="#Examples-using-the-ENLR-model">Examples using the ENLR model</a><a id="Examples-using-the-ENLR-model-1"></a><a class="docs-heading-anchor-permalink" href="#Examples-using-the-ENLR-model" title="Permalink"></a></h2><p>The <strong>elastic net logistic regression (ENLR)</strong> classifier is an example of classifier acting on the tangent space. Besides the <strong>metric</strong> (see above) used to compute a base-point for projecting the data onto the tangent space, it has a parameter <strong>alpha</strong> and an hyperparameter <strong>lambda</strong>. The <strong>alpha</strong> parameter allows to trade off between a pure <strong>ridge</strong> LR model (<span>$α=0$</span>) and a pure <strong>lasso</strong> LR model (<span>$α=1$</span>), which is the default. Given an alpha value, the model is fitted with a number of values for the <span>$λ$</span> (regularization) hyperparameter. Thus, differently from the previous example, tuning the <span>$λ$</span> hyperparameter is necessary.</p><p>Also, keep in mind that the <a href="../cv/#StatsAPI.fit"><code>fit</code></a> and <a href="../cv/#StatsAPI.predict"><code>predict</code></a> methods for ENLR models accept optional keyword arguments that are specific to this model.</p><p><strong>Get data</strong></p><p>Let us get some simulated data (see the previous example for explanations).</p><pre><code class="language-julia hljs">using PosDefManifoldML, PosDefManifold

PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);</code></pre><h3 id="ENLR-train-test"><a class="docs-heading-anchor" href="#ENLR-train-test">ENLR train-test</a><a id="ENLR-train-test-1"></a><a class="docs-heading-anchor-permalink" href="#ENLR-train-test" title="Permalink"></a></h3><p><strong>Craete and fit ENLR models</strong></p><p>By default, the Fisher metric ic adopted and a lasso model is fitted. The best value for the lambda hyperparameter is found by cross-validation:</p><pre><code class="language-julia hljs">m1 = fit(ENLR(), PTr, yTr; w=:balanced)</code></pre><p>Notice that since the class are unbalanced, with the <code>w=:balanced</code> argument (we may as well just use <code>w=:b</code>) we have requested to compute a balanced mean for projecting the matrices in <code>PTr</code> onto the tangent space.</p><p>The optimal value of lambda for this training data is</p><pre><code class="language-julia hljs">m1.best.lambda</code></pre><p>As in <em>GLMNet.jl</em>, the intercept and beta terms are retrived by</p><pre><code class="language-julia hljs">m1.best.a0
m1.best.betas</code></pre><p>The number of non-zero beta coefficients can be found, for example, by</p><pre><code class="language-julia hljs">length(unique(m1.best.betas))-1</code></pre><p>In order to fit a ridge LR model:</p><pre><code class="language-julia hljs">m2 = fit(ENLR(), PTr, yTr; w=:b, alpha=0)</code></pre><p>Values of <code>alpha</code> in range <span>$(0, 1)$</span> fit instead an elastic net LR model. In the following we also request not to normalize predictors (by default they norm is fixed):</p><pre><code class="language-julia hljs">m3 = fit(ENLR(Fisher), PTr, yTr; w=:b, alpha=0.9, normalize=nothing)</code></pre><p>Instead we could standardize predictors:</p><pre><code class="language-julia hljs">m4 = fit(ENLR(Fisher), PTr, yTr; w=:b, alpha=0.9, normalize=standardize!)</code></pre><p>or rescale them within custom limits:</p><pre><code class="language-julia hljs">m5 = fit(ENLR(Fisher), PTr, yTr; w=:b, alpha=0.9, normalize=(-1.0, 1.0))</code></pre><p>In order to find the regularization path we use the <code>fitType</code> keyword argument:</p><pre><code class="language-julia hljs">m1 = fit(ENLR(Fisher), PTr, yTr; w=:b, fitType=:path)</code></pre><p>The values of lambda along the path are given by</p><pre><code class="language-julia hljs">m1.path.lambda</code></pre><p>We can also find the best value of the lambda hyperparameter and the regularization path at once, calling:</p><pre><code class="language-julia hljs">m1 = fit(ENLR(Fisher), PTr, yTr; w=:b, fitType=:all)</code></pre><p>For changing the metric see <a href="#MDM-train-test">MDM train-test</a>.</p><p>See the documentation of the <a href="../cv/#StatsAPI.fit"><code>fit</code></a> ENLR method for details on all available optional arguments.</p><p><strong>Classify data (predict)</strong></p><p>For prediction, we can request to use the best model (optimal lambda), to use a specific model of the regularization path or to use all the models in the regalurization path. Note that with the last call we have done here above both the <code>.best</code> and <code>.path</code> field of the <code>m1</code> structure have been created.</p><p>By default, prediction is obtained from the best model and we request to predict the labels:</p><pre><code class="language-julia hljs">yPred=predict(m1, PTe)

# prediction accuracy (in proportion)
predictAcc(yPred, yTe)

# confusion matrix
confusionMat(yPred, yTe)

# predict probabilities of matrices in `PTe` to belong to each class
predict(m1, PTe, :p)

# output function of the model for each class
predict(m1, PTe, :f)
</code></pre><p>In order to request the predition of labels for all models in the regularization path:</p><pre><code class="language-julia hljs">yPred=predict(m1, PTe, :l, :path, 0)</code></pre><p>while for a specific model in the path (e.g., model #10):</p><pre><code class="language-julia hljs">yPred=predict(m1, PTe, :l, :path, 10)</code></pre><h3 id="ENLR-cross-validation"><a class="docs-heading-anchor" href="#ENLR-cross-validation">ENLR cross-validation</a><a id="ENLR-cross-validation-1"></a><a class="docs-heading-anchor-permalink" href="#ENLR-cross-validation" title="Permalink"></a></h3><p>The balanced accuracy estimated by a <em>k-fold cross-validation</em> is obtained with the exact same basic syntax for all models, with some specific optional keyword arguments for models acting in the tangent space, for example:</p><pre><code class="language-julia hljs">cv = crval(ENLR(), PTr, yTr; w=:b)</code></pre><p>In order to perform another cross-validation arranging the training data differently in the folds:</p><pre><code class="language-julia hljs">cv = crval(ENLR(), PTr, yTr; w=:b, shuffle=true)</code></pre><p>This last command can be invoked repeatedly.</p><h3 id="ENLR-adaptation"><a class="docs-heading-anchor" href="#ENLR-adaptation">ENLR adaptation</a><a id="ENLR-adaptation-1"></a><a class="docs-heading-anchor-permalink" href="#ENLR-adaptation" title="Permalink"></a></h3><p>First, let&#39;s see how to adapt the base point for projecting the data onto the tangent space. Suppose you have data from two sessions or two subjects, <code>s1</code> and <code>s2</code>. We want to use <code>s1</code> to train a machine learning model on the tanget space and <code>s2</code> to test it, however, the barycenter <code>s1</code> cannot be assumed equal to the barycenter of <code>s2</code>. The barycenter determines the base point, therefore, we adapt it.</p><p><strong>Get data</strong> Let us get some simulated data. We generate random data and labels for session (or subject) 1 and 2.</p><pre><code class="language-julia hljs">Ps1, Ps2, ys1, ys2 = gen2ClassData(10, 30, 40, 60, 80);</code></pre><p><strong>Craete and fit an ENLR model on s1</strong></p><pre><code class="language-julia hljs">m = fit(ENLR(Fisher), Ps1, ys1)</code></pre><p><strong>Classify (predict) data of s2 adapting the base point</strong></p><pre><code class="language-julia hljs">predict(m, PTe, :l; meanISR=invsqrt(mean(Fisher, PTe)))</code></pre><p>Second, let&#39;s see how to adapt a pre-conditioning pipeline like we have done  here above for the base point. Since the pipeline we will employ recenter the data  around the identity, we can skip altogether the computation of the barycenter  for <code>s2</code>, using the identity matrix as the base point.</p><p>The pipeline we will define comprises a recentering pre-conditioner  with dimensionality reduction. While adapting the pipeline to <code>s2</code>,  we need to make sure that the matrices in <code>s2</code> are reduced to the same  dimension as the matrices in <code>s1</code>, otherwise the  machine learning model we fit on <code>s1</code> cannot operate  on <code>s2</code>. For this, we need to set the <code>eVar</code> argmument of the [<code>Recenter</code>] pre-conditioner to a integer matching the reduced dimension of <code>s1</code>. Note that the adaptation may not work well if the class proportions is different in <code>s1</code> and <code>s2</code>.</p><p><strong>Define the pre-conditioning pipeline for s1</strong></p><pre><code class="language-julia hljs">p = @→ Recenter(; eVar=0.999) → Compress → Shrink(Fisher; radius=0.02)</code></pre><p><strong>Fit the model on s1 using the pipeline</strong></p><pre><code class="language-julia hljs">m = fit(ENLR(), Ps1, ys1; pipeline = p)</code></pre><p><strong>Define the same pipeline with fixed dimensionality reduction parameter</strong></p><pre><code class="language-julia hljs">p = @→ Recenter(; eVar=dim(m.pipeline)) → Compress → Shrink(Fisher; radius=0.02)</code></pre><p><strong>Fit the pipeline to s2 (adapt) and use the identity matrix as base point</strong></p><pre><code class="language-julia hljs">predict(m, Ps2, :l; pipeline=p, meanISR=I) </code></pre><h2 id="Examples-using-SVM-models"><a class="docs-heading-anchor" href="#Examples-using-SVM-models">Examples using SVM models</a><a id="Examples-using-SVM-models-1"></a><a class="docs-heading-anchor-permalink" href="#Examples-using-SVM-models" title="Permalink"></a></h2><p>The SVM ML model actually encapsulates several <strong>support-vector classification</strong> and <strong>support-vector regression</strong> models. Here we are concerned with the former, which include the <strong>C-Support Vector Classification (SVC)</strong>, the <strong>Nu-Support Vector Classification (NuSVC)</strong>, similar to SVC but using a parameter to control the number of support vectors, and the <strong>One-Class SVM (OneClassSVM)</strong>, which is used in general for unsupervised outlier detection. They all act in the tangent space like ENLR models. Besides the <strong>metric</strong> (see <a href="#MDM-train-test">MDM train-test</a>) used to compute a base-point for projecting the data onto the tangent space and the type of SVM model (the <strong>svmType</strong>, = <code>SVC</code> (default), <code>NuSVC</code> or <code>OneClassSVM</code>), the main parameter is the <strong>kernel</strong>. Avaiable kernels are:</p><ul><li><code>Linear</code>      (default)</li><li><code>RadialBasis</code> </li><li><code>Polynomial</code></li><li><code>Sigmoid</code></li></ul><p>Several parameters are available for building all these kernels besides the linear one, which has no parameter. Like for ENLR, for SVM models also an hyperparameter is to be found by cross-validation.</p><p><strong>Get data</strong></p><p>Let us get some simulated data as in the previous examples.</p><pre><code class="language-julia hljs">using PosDefManifoldML, PosDefManifold

PTr, PTe, yTr, yTe=gen2ClassData(10, 30, 40, 60, 80, 0.1);</code></pre><h3 id="SVM-train-test"><a class="docs-heading-anchor" href="#SVM-train-test">SVM train-test</a><a id="SVM-train-test-1"></a><a class="docs-heading-anchor-permalink" href="#SVM-train-test" title="Permalink"></a></h3><p><strong>Craete and fit SVM models</strong></p><p>By default, a C-Support Vector Classification model is fitted:</p><pre><code class="language-julia hljs">m1 = fit(SVM(), PTr, yTr; w=:b)</code></pre><p>Notice that as for the example above with for ENLR model, we have requested to compute a balanced mean for projecting the matrices in <code>PTr</code> onto the tangent space.</p><p>In order to fit a Nu-Support Vector Classification model:</p><pre><code class="language-julia hljs">m2 = fit(SVM(), PTr, yTr; w=:b, svmType=NuSVC)</code></pre><p>For using other kernels, e.g.:</p><pre><code class="language-julia hljs">m3 = fit(SVM(), PTr, yTr; w=:b, svmType=NuSVC, kernel=Polynomial)</code></pre><p>In the following we request not to normalize predictors (by default they norm is fixed):</p><pre><code class="language-julia hljs">m4 = fit(SVM(), PTr, yTr; w=:b, normalize=nothing)</code></pre><p>Instead we could standardize predictors:</p><pre><code class="language-julia hljs">m5 = fit(SVM(), PTr, yTr; w=:b, normalize=standardize!)</code></pre><p>or rescale them within custom limits:</p><pre><code class="language-julia hljs">m6 = fit(SVM(), PTr, yTr; w=:b, normalize=(-1.0, 1.0))</code></pre><p>By default the Fisher metric is used. For changing it, see <a href="#MDM-train-test">MDM train-test</a>.</p><p>See the documentation of the <a href="../cv/#StatsAPI.fit"><code>fit</code></a> SVM method for details on all available optional arguments.</p><p><strong>Classify data (predict)</strong></p><p>Just the same as for the other models:</p><pre><code class="language-julia hljs">yPred=predict(m1, PTe)

# prediction accuracy (in proportion)
predictAcc(yPred, yTe)

# confusion matrix
confusionMat(yPred, yTe)

# predict probabilities of matrices in `PTe` to belong to each class
predict(m1, PTe, :p)

# output function of the model for each class
predict(m1, PTe, :f)</code></pre><h3 id="SVM-cross-validation"><a class="docs-heading-anchor" href="#SVM-cross-validation">SVM cross-validation</a><a id="SVM-cross-validation-1"></a><a class="docs-heading-anchor-permalink" href="#SVM-cross-validation" title="Permalink"></a></h3><p>Again, the balanced accuracy estimated by a <em>k-fold cross-validation</em> is obtained with the exact same basic syntax for all models, with some specific optional keyword arguments for models acting in the tangent space, for example:</p><pre><code class="language-julia hljs">cv = crval(SVM(), PTr, yTr; w=:b)</code></pre><h3 id="SVM-adaptation"><a class="docs-heading-anchor" href="#SVM-adaptation">SVM adaptation</a><a id="SVM-adaptation-1"></a><a class="docs-heading-anchor-permalink" href="#SVM-adaptation" title="Permalink"></a></h3><p>See the tutirial on <a href="#ENLR-adaptation">ENLR adaptation</a>; the code needed is exactly the same changing the machine learning model from <code>ENLR</code> to <code>SVM</code>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« PosDefManifoldML Documentation</a><a class="docs-footer-nextpage" href="../MainModule/">Main Module »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Sunday 6 July 2025 15:29">Sunday 6 July 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
